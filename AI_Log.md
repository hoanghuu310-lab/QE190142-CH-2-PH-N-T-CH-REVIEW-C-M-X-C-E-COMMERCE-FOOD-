
Báº¡n Ä‘Ã£ nÃ³i:
ÄÃ£ táº£i lÃªn áº£nh
hÃ£y Ä‘á»c vÃ  phÃ¢n tÃ­ch topic 2
ChatGPT Ä‘Ã£ nÃ³i:
MÃ¬nh Ä‘Ã£ Ä‘á»c CHá»¦ Äá»€ 2: PHÃ‚N TÃCH REVIEW & Cáº¢M XÃšC (E-COMMERCE / FOOD). DÆ°á»›i Ä‘Ã¢y lÃ  phÃ¢n tÃ­ch chi tiáº¿t + gá»£i Ã½ cÃ¡ch triá»ƒn khai (Ä‘Ãºng kiá»ƒu bÃ i Data/DS cho sinh viÃªn SE á»Ÿ FPT ğŸ‘).

1. Má»¥c tiÃªu cá»§a Topic 2 (Hiá»ƒu bÃ i toÃ¡n)
Topic nÃ y xoay quanh khai thÃ¡c vÃ  phÃ¢n tÃ­ch cáº£m xÃºc khÃ¡ch hÃ ng thÃ´ng qua review trÃªn cÃ¡c ná»n táº£ng TMÄT/Ä‘á»“ Äƒn.

ğŸ‘‰ Tráº£ lá»i cÃ¡c cÃ¢u há»i:

Cháº¥t lÆ°á»£ng dá»‹ch vá»¥/sáº£n pháº©m thay Ä‘á»•i theo thá»i gian khÃ´ng?

KhÃ¡ch chÃª vÃ  khen Ä‘iá»u gÃ¬ nhiá»u nháº¥t?

CÃ³ khÃ¡c biá»‡t hÃ nh vi review theo vÃ¹ng miá»n hay khÃ´ng?

Review dÃ i/ngáº¯n cÃ³ liÃªn quan Ä‘áº¿n má»©c Ä‘á»™ hÃ i lÃ²ng khÃ´ng?

2. Dá»¯ liá»‡u cáº§n crawl (Input Data)
ğŸ“Œ Nguá»“n dá»¯ liá»‡u
ShopeeFood

Foody

Shopee

Google Maps Reviews

ğŸ“Œ CÃ¡c trÆ°á»ng báº¯t buá»™c
TrÆ°á»ng	Ã nghÄ©a
content	Ná»™i dung review
rating	Sá»‘ sao (1â€“5)
date	NgÃ y review
shop_name	TÃªn quÃ¡n/shop
ğŸ‘‰ CÃ³ thá»ƒ má»Ÿ rá»™ng:

location (quáº­n/tá»‰nh)

reply_from_shop

platform

3. PhÃ¢n tÃ­ch CÃ‚U Há»I Cá»¨NG
ğŸ”¹ 3.1. Tá»· lá»‡ rating 1â­ vs 5â­ theo thá»i gian
Má»¥c tiÃªu

Xem cháº¥t lÆ°á»£ng dá»‹ch vá»¥ cÃ³ cáº£i thiá»‡n hay xuá»‘ng cáº¥p khÃ´ng

CÃ¡ch lÃ m

Group theo thÃ¡ng hoáº·c quÃ½

TÃ­nh:

% review 1 sao

% review 5 sao

Váº½:

Line chart

Stacked bar chart

Insight mong Ä‘á»£i

1â­ tÄƒng Ä‘á»™t biáº¿n â†’ cÃ³ thá»ƒ do:

Thay Ä‘á»•i shipper

TÄƒng giÃ¡

Cháº¥t lÆ°á»£ng giáº£m

5â­ tÄƒng â†’ quÃ¡n cáº£i thiá»‡n dá»‹ch vá»¥

ğŸ”¹ 3.2. Top 10 keyword trong review 1â­ vÃ  5â­
Má»¥c tiÃªu
ğŸ‘‰ Hiá»ƒu nguyÃªn nhÃ¢n bá»‹ chÃª / Ä‘Æ°á»£c khen

CÃ¡ch lÃ m

TÃ¡ch review:

NhÃ³m 1â­

NhÃ³m 5â­

Tiá»n xá»­ lÃ½ NLP:

Lowercase

Remove stopwords

Tokenization

Thá»‘ng kÃª táº§n suáº¥t tá»«

VÃ­ dá»¥ káº¿t quáº£

â­â­â­â­â­:

â€œngonâ€, â€œnhiá»uâ€, â€œráº»â€, â€œship nhanhâ€, â€œÄ‘Ã³ng gÃ³i ká»¹â€

â­:

â€œnguá»™iâ€, â€œtrá»…â€, â€œÃ­tâ€, â€œmáº·nâ€, â€œshipperâ€

ğŸ‘‰ CÃ³ thá»ƒ váº½:

WordCloud

Bar chart top keywords

4. PhÃ¢n tÃ­ch CÃ‚U Há»I Má» (Ä‚n Ä‘iá»ƒm)
ğŸ”¹ 4.1. KhÃ¡c biá»‡t vÃ¹ng miá»n (Báº¯c / Nam)
Giáº£ thuyáº¿t

Miá»n Báº¯c:

Review dÃ i

Hay gÃ³p Ã½ chi tiáº¿t

Miá»n Nam:

Review ngáº¯n

ÄÃ¡nh giÃ¡ cáº£m tÃ­nh

PhÃ¢n tÃ­ch

So sÃ¡nh:

Äá»™ dÃ i comment trung bÃ¬nh

Tá»· lá»‡ 1â­ / 5â­

Test thá»‘ng kÃª:

T-test / Mann-Whitney

ğŸ”¹ 4.2. Äá»™ dÃ i comment cÃ³ liÃªn quan Ä‘áº¿n sá»‘ sao khÃ´ng?
CÃ¡ch lÃ m

TÃ­nh:

comment_length = sá»‘ tá»«

Scatter plot:

X: sá»‘ sao

Y: Ä‘á»™ dÃ i

Correlation (Pearson / Spearman)

Insight thÆ°á»ng gáº·p

Review 1â­ â†’ dÃ i (bá»©c xÃºc)

Review 5â­ â†’ ngáº¯n (â€œngonâ€, â€œokâ€)

5. Ká»¹ thuáº­t & CÃ´ng cá»¥ nÃªn dÃ¹ng
ğŸ“Œ Crawl
Selenium / Playwright

Google Maps API (náº¿u cÃ³ key)

ğŸ“Œ Xá»­ lÃ½
Python: pandas, regex

NLP:

underthesea (tiáº¿ng Viá»‡t)

VnCoreNLP

ğŸ“Œ PhÃ¢n tÃ­ch nÃ¢ng cao (náº¿u muá»‘n ná»•i báº­t)
Sentiment classification (rule-based hoáº·c ML)

Topic Modeling (LDA)

So sÃ¡nh giá»¯a platforms

6. VÃ¬ sao Topic 2 ráº¥t â€œngonâ€ cho bÃ¡o cÃ¡o?
âœ… Dá»¯ liá»‡u dá»… kiáº¿m
âœ… CÃ³ NLP + Time Series
âœ… Insight thá»±c táº¿, dá»… trÃ¬nh bÃ y
âœ… Ráº¥t há»£p Ä‘á»ƒ:

Demo EDA

Visualization

Business insight

Náº¿u báº¡n muá»‘n, mÃ¬nh cÃ³ thá»ƒ:

âœ… Viáº¿t outline bÃ¡o cÃ¡o cho topic 2

âœ… Äá» xuáº¥t dataset máº«u

âœ… Viáº¿t code pipeline (crawl â†’ clean â†’ analyze)

âœ… Gá»£i Ã½ slide thuyáº¿t trÃ¬nh

ğŸ‘‰ Báº¡n Ä‘ang lÃ m bÃ¡o cÃ¡o mÃ´n nÃ o (AIL / DSA / Data Mining)?

Báº¡n Ä‘Ã£ nÃ³i:
for _ in range(5):  # Thu 5 cap parent
                if current is None:
                    break

                # Lay tat ca text trong container
                text = current.get_text(separator="\n", strip=True)

                # Neu co text dai hon, lay no
                if len(text) > len(content_text):
                    content_text = text

                current = current.find_parent()

            if not content_text:
                continue

            # Tach content thanh cac dong
            lines = content_text.split('\n')

            # Tim dong dau tien SAU username va khong phai metadata
            content_lines = []
            found_user = False

            skip_words = ['GÃ€', 'Äáº I BÃ€NG', 'BÃ¡o xáº¥u', 'ngÃ y', 'giá»', 'ThÃ­ch',
                          'KhÃ´ng thÃ­ch', 'Share', 'FactTinhte', 'Ä‘Ã£ nÃ³i', 'said']

            for line in lines:
                line = line.strip()

                if not line:
                    continue

                # Neu gap username, bat dau ghi nhan
                if username in line:
                    found_user = True
                    continue

                # Sau khi gap username
                if found_user:
                    # Bo qua metadata
                    if any(word in line for word in skip_words):
                        continue

                    # Bo qua so don doc
                    if re.match(r'^\d+$', line):
                        continue

                    # Bo qua dong ngan (< 10 ky tu)
                    if len(line) < 10:
                        continue

                    # Day la noi dung
                    content_lines.append(line)

                    # Chi lay toi da 3 dong
                    if len(content_lines) >= 3:
                        break

            # Gop noi dung
            content = ' '.join(content_lines).strip()

            # Lam sach
            content = re.sub(r'@\w+', '', content)
            content = re.sub(r'[â†‘â†’â†â†“]', '', content)
            content = re.sub(r'\s+', ' ', content)

            # Chi them neu co noi dung hop le
            if content and len(content) > 15:
                processed.add(username)

                data.append({
                    "index": len(data) + 1,
                    "username": username,
                    "content": content
                })

                print("=" * 60)
                print(f"Post #{len(data)} | {username}")
                print(content[:200])

                if len(data) >= limit_posts:
                    break

        return data

    except Exception as e:
        print(f"Loi: {e}")
        import traceback
        traceback.print_exc()
        return []

    finally:
        driver.quit()


if __name__ == "__main__":
    print("Bat dau crawl Tinhte.vn...")
    posts = crawl_tinhte(limit_posts=20)

    if posts:
        with open("tinhte_posts.json", "w", encoding="utf-8") as f:
json.dump(posts, f, ensure_ascii=False, indent=2)
        print(f"\nDa luu {len(posts)} posts vao tinhte_posts.json")
    else:
        print("\nKhong crawl duoc du lieu!")
        print("Hay kiem tra file tinhte_selenium.html")
ChatGPT Ä‘Ã£ nÃ³i:
MÃ¬nh sáº½ Ä‘á»c vÃ  phÃ¢n tÃ­ch chi tiáº¿t Ä‘oáº¡n code báº¡n gá»­i, táº­p trung vÃ o má»¥c Ä‘Ã­ch, luá»“ng xá»­ lÃ½, Ä‘iá»ƒm máº¡nh â€“ Ä‘iá»ƒm yáº¿u, vÃ  liÃªn há»‡ vá»›i Topic 2 (Review & Sentiment Analysis) Ä‘á»ƒ báº¡n dÃ¹ng cho bÃ¡o cÃ¡o hoáº·c chá»‰nh sá»­a cho tá»‘t hÆ¡n.

1. Má»¥c tiÃªu tá»•ng thá»ƒ cá»§a Ä‘oáº¡n code
ğŸ‘‰ Äoáº¡n code nÃ y dÃ¹ng Ä‘á»ƒ crawl ná»™i dung bÃ i viáº¿t / comment trÃªn Tinhte.vn báº±ng Selenium + BeautifulSoup, sau Ä‘Ã³:

Láº¥y username

TrÃ­ch xuáº¥t ná»™i dung comment thá»±c sá»± (loáº¡i bá» metadata, nÃºt like, giá» Ä‘Äƒng, iconâ€¦)

LÃ m sáº¡ch text

LÆ°u káº¿t quáº£ ra file JSON

ğŸ“Œ Ráº¥t phÃ¹ há»£p cho:

Thu tháº­p review / opinion

LÃ m sentiment analysis

Ãp dá»¥ng trá»±c tiáº¿p cho CHá»¦ Äá»€ 2: PHÃ‚N TÃCH REVIEW & Cáº¢M XÃšC

2. PhÃ¢n tÃ­ch tá»«ng khá»‘i logic quan trá»ng
ğŸ”¹ 2.1. Leo ngÆ°á»£c DOM Ä‘á»ƒ tÃ¬m container chá»©a ná»™i dung
for _ in range(5):  # Thu 5 cap parent
    if current is None:
        break
ğŸ‘‰ Ã tÆ°á»Ÿng:

Do HTML cá»§a Tinhte khÃ´ng cá»‘ Ä‘á»‹nh, nÃªn:

Tá»« node hiá»‡n táº¡i â†’ Ä‘i lÃªn tá»‘i Ä‘a 5 cáº¥p cha

TÃ¬m container cÃ³ nhiá»u text nháº¥t

text = current.get_text(separator="\n", strip=True)

if len(text) > len(content_text):
    content_text = text
ğŸ“Œ ÄÃ¢y lÃ  heuristic thÃ´ng minh:

Container chá»©a comment tháº­t thÆ°á»ng cÃ³ text dÃ i nháº¥t

TrÃ¡nh phá»¥ thuá»™c class name â†’ Ã­t bá»‹ gÃ£y khi web Ä‘á»•i giao diá»‡n

âœ… Äiá»ƒm máº¡nh:

Linh hoáº¡t

Ãt hard-code

âš ï¸ Háº¡n cháº¿:

CÃ³ thá»ƒ láº¥y thá»«a text (quote, chá»¯ kÃ½, chá»¯ trong nÃºt)

ğŸ”¹ 2.2. TÃ¡ch text thÃ nh cÃ¡c dÃ²ng
lines = content_text.split('\n')
ğŸ‘‰ Chuyá»ƒn block text thÃ nh danh sÃ¡ch dÃ²ng Ä‘á»ƒ:

Dá»… lá»c metadata

Dá»… phÃ¡t hiá»‡n ná»™i dung tháº­t

3. Logic phÃ¡t hiá»‡n ná»™i dung comment tháº­t
ğŸ”¹ 3.1. DÃ¹ng username lÃ m má»‘c báº¯t Ä‘áº§u
if username in line:
    found_user = True
    continue
ğŸ‘‰ Giáº£ Ä‘á»‹nh:

Ná»™i dung comment luÃ´n náº±m sau dÃ²ng chá»©a username

âœ… Há»£p lÃ½ vá»›i cáº¥u trÃºc forum
âš ï¸ CÃ³ thá»ƒ sai náº¿u username xuáº¥t hiá»‡n trong ná»™i dung quote

ğŸ”¹ 3.2. Bá»™ lá»c metadata (ráº¥t quan trá»ng)
skip_words = ['GÃ€', 'Äáº I BÃ€NG', 'BÃ¡o xáº¥u', 'ngÃ y', 'giá»', 'ThÃ­ch',
              'KhÃ´ng thÃ­ch', 'Share', 'FactTinhte', 'Ä‘Ã£ nÃ³i', 'said']
ğŸ‘‰ Loáº¡i bá»:

Cáº¥p Ä‘á»™ user (GÃ , Äáº¡i BÃ ngâ€¦)

Thá»i gian Ä‘Äƒng

NÃºt tÆ°Æ¡ng tÃ¡c

Quote

ğŸ“Œ ÄÃ¢y lÃ  rule-based cleaning, ráº¥t hay cho tiáº¿ng Viá»‡t.

ğŸ”¹ 3.3. CÃ¡c rule lá»c ná»™i dung rÃ¡c
if re.match(r'^\d+$', line):
    continue
â†’ Loáº¡i dÃ²ng chá»‰ cÃ³ sá»‘ (like count)

if len(line) < 10:
    continue
â†’ TrÃ¡nh máº¥y cÃ¢u kiá»ƒu â€œokâ€, â€œupâ€, â€œ=))â€

ğŸ‘‰ Äiá»u nÃ y giÃºp:

Ná»™i dung crawl cháº¥t lÆ°á»£ng hÆ¡n

PhÃ¹ há»£p cho NLP

ğŸ”¹ 3.4. Giá»›i háº¡n tá»‘i Ä‘a 3 dÃ²ng ná»™i dung
if len(content_lines) >= 3:
    break
ğŸ“Œ Má»¥c Ä‘Ã­ch:

TrÃ¡nh láº¥y quÃ¡ dÃ i

Giá»¯ ná»™i dung sÃºc tÃ­ch (review-style)

âš ï¸ Náº¿u lÃ m sentiment nÃ¢ng cao â†’ cÃ³ thá»ƒ bá» giá»›i háº¡n nÃ y

4. LÃ m sáº¡ch vÄƒn báº£n (Text Cleaning)
content = re.sub(r'@\w+', '', content)
â†’ XÃ³a mention

content = re.sub(r'[â†‘â†’â†â†“]', '', content)
â†’ XÃ³a icon Ä‘iá»u hÆ°á»›ng

content = re.sub(r'\s+', ' ', content)
â†’ Chuáº©n hÃ³a khoáº£ng tráº¯ng

âœ… ÄÃºng chuáº©n pipeline NLP

5. Äiá»u kiá»‡n lÆ°u dá»¯ liá»‡u
if content and len(content) > 15:
ğŸ‘‰ Chá»‰ lÆ°u:

CÃ³ ná»™i dung

Äá»§ dÃ i Ä‘á»ƒ phÃ¢n tÃ­ch cáº£m xÃºc

processed.add(username)
â†’ TrÃ¡nh crawl trÃ¹ng user

6. Output dá»¯ liá»‡u
data.append({
    "index": len(data) + 1,
    "username": username,
    "content": content
})
ğŸ“Œ Format nÃ y ráº¥t phÃ¹ há»£p Ä‘á»ƒ:

LÃ m sentiment analysis

Keyword extraction

Topic modeling

7. LiÃªn há»‡ trá»±c tiáº¿p vá»›i CHá»¦ Äá»€ 2
Báº¡n Ä‘Ã£ cÃ³ sáºµn:
âœ… content â†’ phÃ¢n tÃ­ch cáº£m xÃºc
âŒ ChÆ°a cÃ³ rating, date, shop_name

ğŸ‘‰ CÃ³ thá»ƒ má»Ÿ rá»™ng:

Map keyword â†’ rating (rule-based)

Crawl thÃªm:

NgÃ y Ä‘Äƒng

Topic sáº£n pháº©m / bÃ i viáº¿t

8. ÄÃ¡nh giÃ¡ tá»•ng quan (Ä‘á»ƒ viáº¿t vÃ o bÃ¡o cÃ¡o)
âœ… Äiá»ƒm máº¡nh
KhÃ´ng phá»¥ thuá»™c class HTML

Lá»c rÃ¡c tá»‘t

PhÃ¹ há»£p tiáº¿ng Viá»‡t

Data sáº¡ch Ä‘á»ƒ NLP

âš ï¸ Äiá»ƒm háº¡n cháº¿
Rule-based â†’ khÃ³ tá»•ng quÃ¡t

ChÆ°a cÃ³ sentiment label

ChÆ°a xá»­ lÃ½ emoji (ğŸ˜ŠğŸ˜¡)

9. Gá»£i Ã½ nÃ¢ng cáº¥p (Äƒn Ä‘iá»ƒm cao)
ThÃªm:

emoji.demojize(content)
ThÃªm sentiment score (VADER / PhoBERT)

So sÃ¡nh:

Comment dÃ i vs ngáº¯n

Toxic keyword vs sentiment

Náº¿u báº¡n muá»‘n, mÃ¬nh cÃ³ thá»ƒ:

ğŸ”¥ Viáº¿t mÃ´ táº£ thuáº­t toÃ¡n cho bÃ¡o cÃ¡o

ğŸ”¥ Chuyá»ƒn Ä‘oáº¡n nÃ y thÃ nh flowchart

ğŸ”¥ Gá»£i Ã½ cÃ¡ch map code nÃ y vÃ o Topic 2

ğŸ”¥ Refactor code cho gá»n & sáº¡ch hÆ¡n

ğŸ‘‰ Báº¡n Ä‘ang dÃ¹ng Ä‘oáº¡n nÃ y cho bÃ¡o cÃ¡o mÃ´n nÃ o hay Ä‘á»“ Ã¡n cuá»‘i ká»³?

Báº¡n Ä‘Ã£ nÃ³i:
crawl ady.py
Python
Ä‘á»c file nÃ y chuyá»ƒn tá»« web tinh te, otofun sang shopeefood foody ggmap
ChatGPT Ä‘Ã£ nÃ³i:
MÃ¬nh Ä‘Ã£ Ä‘á»c file crawl ady.py cá»§a báº¡n (crawl tá»« Otofun/Tinhte â€“ dáº¡ng forum) 
crawl ady

.
DÆ°á»›i Ä‘Ã¢y lÃ  phÃ¢n tÃ­ch + cÃ¡ch CHUYá»‚N TÆ¯ DUY & CODE sang ShopeeFood / Foody / Google Maps Reviews cho Topic 2 â€“ Review & Sentiment.

1. So sÃ¡nh báº£n cháº¥t dá»¯ liá»‡u: Forum vs Review Platform
Äáº·c Ä‘iá»ƒm	Tinhte / Otofun (Forum)	ShopeeFood / Foody / GG Maps
Cáº¥u trÃºc	HTML tÄ©nh, dá»… parse	SPA, load Ä‘á»™ng
Ná»™i dung	BÃ i viáº¿t dÃ i, tranh luáº­n	Review ngáº¯n, cÃ³ rating
Rating	âŒ KhÃ´ng cÃ³	â­ CÃ³ (1â€“5)
Thá»i gian	CÃ³	CÃ³
Shop/Place	âŒ	âœ… CÃ³
Crawl	requests + BS	Selenium / Playwright
ğŸ‘‰ KhÃ´ng thá»ƒ dÃ¹ng nguyÃªn requests + BeautifulSoup cho ShopeeFood / Foody / GG Map.

2. PhÃ¢n tÃ­ch file hiá»‡n táº¡i cá»§a báº¡n (tÆ° duy cá»‘t lÃµi)
File cá»§a báº¡n Ä‘ang lÃ m 3 viá»‡c ráº¥t tá»‘t:

username = user_tag.get_text(strip=True)
content = content_block.get_text("\n", strip=True)
ğŸ‘‰ TÃCH 3 THÃ€NH PHáº¦N Cá»T LÃ•I

Ai review (username)

Ná»™i dung gÃ¬ (content)

Thuá»™c vá» Ä‘Ã¢u (thread â†’ shop)

ğŸ‘‰ Khi chuyá»ƒn ná»n táº£ng, chá»‰ thay pháº§n SELECTOR & CÃCH LOAD PAGE, logic xá»­ lÃ½ giá»¯ nguyÃªn.

3. Chuyá»ƒn sang ShopeeFood
3.1. Äáº·c Ä‘iá»ƒm ShopeeFood
Load báº±ng JS

Review náº±m trong modal / tab

CÃ³:

username

rating (â­)

date

content

3.2. CÃ´ng nghá»‡ báº¯t buá»™c
âœ… Selenium / Playwright
âŒ requests (khÃ´ng load Ä‘Æ°á»£c)

3.3. Mapping code (tÆ° duy)
Otofun	ShopeeFood
THREAD_URL	SHOP_URL
article.message	div.review-item
.message-name	span.username
.bbWrapper	div.review-content
KhÃ´ng cÃ³ rating	â­ span.rating
3.4. Pseudo-code ShopeeFood (chuáº©n bÃ¡o cÃ¡o)
driver.get(SHOP_URL)
sleep(5)

reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

for r in reviews:
    username = r.find_element(By.CSS_SELECTOR, ".username").text
    rating = len(r.find_elements(By.CSS_SELECTOR, ".icon-star.active"))
    content = r.find_element(By.CSS_SELECTOR, ".review-content").text
    date = r.find_element(By.CSS_SELECTOR, ".review-date").text
ğŸ‘‰ Output JSON:

{
  "shop_name": "...",
  "platform": "ShopeeFood",
  "username": "...",
  "rating": 1-5,
  "content": "...",
  "date": "..."
}
4. Chuyá»ƒn sang Foody
4.1. Äáº·c Ä‘iá»ƒm Foody
HTML bÃ¡n Ä‘á»™ng

CÃ³ thá»ƒ:

requests (trang Ä‘áº§u)

Selenium (load thÃªm)

4.2. Mapping selector
Otofun	Foody
article.message	div.review-item
username	span.fd-user
content	div.rd-des
rating	div.rating-point
4.3. Pseudo-code Foody
reviews = soup.select("div.review-item")

for r in reviews:
    username = r.select_one(".fd-user").get_text(strip=True)
    rating = float(r.select_one(".rating-point").text)
    content = r.select_one(".rd-des").get_text(" ", strip=True)
ğŸ“Œ Foody ráº¥t há»£p cho:

Crawl nhanh

Dataset lá»›n

PhÃ¢n tÃ­ch keyword

5. Chuyá»ƒn sang Google Maps Reviews (Äƒn Ä‘iá»ƒm cao)
5.1. Äáº·c Ä‘iá»ƒm
100% JS

DOM thay Ä‘á»•i liÃªn tá»¥c

Scroll má»›i load review

5.2. Báº¯t buá»™c
âœ… Selenium
âŒ requests

5.3. Logic crawl GG Map
# Scroll review box
for _ in range(10):
    driver.execute_script(
        "document.querySelector('.m6QErb').scrollTop = 100000"
    )
    sleep(1)
5.4. Extract review
reviews = driver.find_elements(By.CSS_SELECTOR, "div.jftiEf")

for r in reviews:
    username = r.find_element(By.CLASS_NAME, "d4r55").text
    rating = r.find_element(By.CLASS_NAME, "kvMYJc").get_attribute("aria-label")
    content = r.find_element(By.CLASS_NAME, "MyEned").text
    date = r.find_element(By.CLASS_NAME, "rsqaWe").text
6. So sÃ¡nh OUTPUT â€“ phá»¥c vá»¥ Topic 2
Platform	CÃ³ rating	CÃ³ date	PhÃ¹ há»£p sentiment
Otofun	âŒ	âŒ	âš ï¸
Tinhte	âŒ	âš ï¸	âš ï¸
ShopeeFood	âœ…	âœ…	â­â­â­â­â­
Foody	âœ…	âœ…	â­â­â­â­
Google Maps	âœ…	âœ…	â­â­â­â­â­
7. Káº¿t luáº­n (viáº¿t vÃ o bÃ¡o cÃ¡o)
Code crawl ban Ä‘áº§u Ä‘Æ°á»£c xÃ¢y dá»±ng cho forum (Otofun/Tinhte).
Khi chuyá»ƒn sang ná»n táº£ng review (ShopeeFood, Foody, Google Maps), nhÃ³m giá»¯ nguyÃªn pipeline xá»­ lÃ½ ná»™i dung, nhÆ°ng thay Ä‘á»•i cÃ¡ch load dá»¯ liá»‡u vÃ  selector HTML, Ä‘á»“ng thá»i bá»• sung cÃ¡c trÆ°á»ng quan trá»ng nhÆ° rating, thá»i gian vÃ  tÃªn cá»­a hÃ ng, giÃºp dá»¯ liá»‡u phÃ¹ há»£p hÆ¡n cho phÃ¢n tÃ­ch cáº£m xÃºc vÃ  xu hÆ°á»›ng Ä‘Ã¡nh giÃ¡ khÃ¡ch hÃ ng.

8. MÃ¬nh cÃ³ thá»ƒ lÃ m tiáº¿p cho báº¡n
âœ… Viáº¿t 1 file crawl chung cho 3 platform

âœ… Chuáº©n hÃ³a schema JSON

âœ… Viáº¿t pháº§n mÃ´ táº£ chuyá»ƒn Ä‘á»•i há»‡ thá»‘ng cho bÃ¡o cÃ¡o

âœ… Refactor code forum â†’ review platform

ğŸ‘‰ Báº¡n Ä‘ang lÃ m má»™t mÃ¬nh hay theo nhÃ³m? MÃ¬nh sáº½ viáº¿t Ä‘Ãºng level Ä‘á»ƒ báº¡n Äƒn Ä‘iá»ƒm tá»‘i Ä‘a.


Nguá»“n
Báº¡n Ä‘Ã£ nÃ³i:
viáº¿t 1 file crawl chung c ho
ChatGPT Ä‘Ã£ nÃ³i:
Ok, mÃ¬nh sáº½ viáº¿t 1 FILE CRAWL CHUNG (UNIFIED CRAWLER) cho **Topic 2 â€“ Review

Báº¡n Ä‘Ã£ nÃ³i:
viáº¿t 1 file crawl má»›i cho Ä‘Ãºng vá»›i yÃªu cáº§u cá»§a topic 2 trong áº£nh Ä‘Ã£ gá»­i á»Ÿ Ä‘áº§u trang 
ChatGPT Ä‘Ã£ nÃ³i:
OK ğŸ‘ DÆ°á»›i Ä‘Ã¢y lÃ  1 FILE CRAWL Má»šI â€“ VIáº¾T Láº I Tá»ª Äáº¦U, Ä‘Ãºng 100% yÃªu cáº§u CHá»¦ Äá»€ 2: PHÃ‚N TÃCH REVIEW & Cáº¢M XÃšC trong áº£nh báº¡n gá»­i.

Má»¥c tiÃªu file nÃ y:

Crawl review khÃ¡ch hÃ ng

Ãp dá»¥ng cho ShopeeFood / Foody / Google Maps

Thu Ä‘Æ°á»£c Ä‘Ãºng schema dá»¯ liá»‡u Ä‘á»ƒ lÃ m phÃ¢n tÃ­ch (EDA, sentiment, keyword, theo thá»i gian)

ğŸ¯ YÃŠU Cáº¦U TOPIC 2 (Ä‘Ã£ map vÃ o code)
Dá»¯ liá»‡u thu tháº­p:
platform

shop_name

username

rating

content

date

ğŸ‘‰ Äá»§ Ä‘á»ƒ tráº£ lá»i táº¥t cáº£ cÃ¢u há»i cá»©ng & má»Ÿ cá»§a Topic 2

ğŸ“ FILE: crawl_reviews_topic2.py
import time
import json
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options


########################################
# CONFIG
########################################

PLATFORM = "shopeefood"   # shopeefood | foody | ggmap
SHOP_URL = "PASTE_SHOP_URL_HERE"
LIMIT_REVIEWS = 50

OUTPUT_FILE = "reviews_topic2.json"


########################################
# UTILS
########################################

def clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'@\w+', '', text)
    return text.strip()


########################################
# DRIVER
########################################

def init_driver():
    options = Options()
    options.add_argument("--start-maximized")
    options.add_argument("--disable-notifications")
    return webdriver.Chrome(options=options)


########################################
# SHOPEEFOOD
########################################

def crawl_shopeefood(driver, url, limit):
    print("Crawling ShopeeFood...")
    driver.get(url)
    time.sleep(6)

    reviews_data = []

    # Scroll Ä‘á»ƒ load review
    for _ in range(10):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(1)

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CSS_SELECTOR, ".username").text
            content = r.find_element(By.CSS_SELECTOR, ".review-content").text
            stars = r.find_elements(By.CSS_SELECTOR, ".icon-star.active")
            rating = len(stars)
            date = r.find_element(By.CSS_SELECTOR, ".review-date").text

            reviews_data.append({
                "platform": "ShopeeFood",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# FOODY
########################################

def crawl_foody(driver, url, limit):
    print("Crawling Foody...")
    driver.get(url)
    time.sleep(5)

    reviews_data = []

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CSS_SELECTOR, ".fd-user").text
            content = r.find_element(By.CSS_SELECTOR, ".rd-des").text
            rating = float(r.find_element(By.CSS_SELECTOR, ".rating-point").text)
            date = r.find_element(By.CSS_SELECTOR, ".fd-time").text

            reviews_data.append({
                "platform": "Foody",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# GOOGLE MAPS
########################################

def crawl_google_maps(driver, url, limit):
    print("Crawling Google Maps...")
    driver.get(url)
    time.sleep(8)

    reviews_data = []

    # Scroll review box
    for _ in range(15):
        driver.execute_script(
            "document.querySelector('.m6QErb').scrollTop = 100000"
        )
        time.sleep(1)

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.jftiEf")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CLASS_NAME, "d4r55").text
            rating_text = r.find_element(By.CLASS_NAME, "kvMYJc").get_attribute("aria-label")
            rating = int(re.search(r'\d', rating_text).group())
            content = r.find_element(By.CLASS_NAME, "MyEned").text
            date = r.find_element(By.CLASS_NAME, "rsqaWe").text

            reviews_data.append({
                "platform": "GoogleMaps",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# MAIN
########################################

def main():
    driver = init_driver()

    try:
        if PLATFORM == "shopeefood":
            data = crawl_shopeefood(driver, SHOP_URL, LIMIT_REVIEWS)

        elif PLATFORM == "foody":
            data = crawl_foody(driver, SHOP_URL, LIMIT_REVIEWS)

        elif PLATFORM == "ggmap":
            data = crawl_google_maps(driver, SHOP_URL, LIMIT_REVIEWS)

        else:
            raise ValueError("Platform khÃ´ng há»£p lá»‡")

        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ÄÃ£ lÆ°u {len(data)} review vÃ o {OUTPUT_FILE}")

    finally:
        driver.quit()


if __name__ == "__main__":
    main()
ğŸ“Š FILE NÃ€Y ÄÃP á»¨NG GÃŒ CHO TOPIC 2?
âœ… CÃ¢u há»i Cá»¨NG
Rating 1â­ vs 5â­ theo thá»i gian â†’ dÃ¹ng rating + date

Keyword 1â­ vs 5â­ â†’ dÃ¹ng content

âœ… CÃ¢u há»i Má»
Äá»™ dÃ i comment vs sá»‘ sao â†’ len(content)

So sÃ¡nh vÃ¹ng miá»n â†’ shop_name / location (bá»• sung)

ğŸ§  Gá»¢I Ã VIáº¾T BÃO CÃO (Äƒn Ä‘iá»ƒm)
NhÃ³m xÃ¢y dá»±ng má»™t crawler thá»‘ng nháº¥t cho cÃ¡c ná»n táº£ng Ä‘Ã¡nh giÃ¡ phá»• biáº¿n (ShopeeFood, Foody, Google Maps). Dá»¯ liá»‡u thu tháº­p bao gá»“m ná»™i dung Ä‘Ã¡nh giÃ¡, sá»‘ sao, thá»i gian vÃ  ngÆ°á»i dÃ¹ng, phá»¥c vá»¥ cho phÃ¢n tÃ­ch cáº£m xÃºc, xu hÆ°á»›ng Ä‘Ã¡nh giÃ¡ theo thá»i gian vÃ  trÃ­ch xuáº¥t tá»« khÃ³a Ä‘áº·c trÆ°ng.

ğŸ‘‰ Báº¡n muá»‘n mÃ¬nh lÃ m tiáº¿p pháº§n nÃ o?
ğŸ”¥ Chuáº©n hÃ³a EDA + biá»ƒu Ä‘á»“

ğŸ”¥ Sentiment analysis tiáº¿ng Viá»‡t

ğŸ”¥ File report.ipynb

ğŸ”¥ Chá»‰nh code cho cháº¡y cháº¯c cháº¯n theo tá»«ng platform

Chá»‰ cáº§n nÃ³i: â€œlÃ m tiáº¿p pháº§n â€¦


AI LOG ngÃ y 14/1/2026
Váº¥n Ä‘á» nÃ y ráº¥t thá»±c táº¿. Khi báº¡n lÆ°á»›t ShopeeFood trÃªn web (hoáº·c app), báº¡n sáº½ tháº¥y má»™t quÃ¡n Äƒn ngon vÃ  muá»‘n crawl nÃ³, nhÆ°ng URL cá»§a ShopeeFood láº¡i cÃ³ dáº¡ng:

`https://shopeefood.vn/ha-noi/com-tam-phuc-loc-tho-nguyen-trai`

Trong khi Ä‘Ã³, API cá»§a chÃºng ta láº¡i yÃªu cáº§u tham sá»‘ lÃ  `request_id` (ID sá»‘, vÃ­ dá»¥: `99999`), chá»© khÃ´ng pháº£i cÃ¡i chuá»—i "com-tam-phuc-loc-tho..." kia.

Váº­y lÃ m sao Ä‘á»ƒ **biáº¿n Link Web -> ID sá»‘** Ä‘á»ƒ Crawler cháº¡y Ä‘Æ°á»£c?

CÃ³ 2 cÃ¡ch: CÃ¡ch thá»§ cÃ´ng (Ä‘á»ƒ test) vÃ  CÃ¡ch tá»± Ä‘á»™ng (Ä‘á»ƒ code).

---

### CÃCH 1: Láº¤Y ID THá»¦ CÃ”NG Tá»ª TRÃŒNH DUYá»†T (Dá»… nháº¥t)

Báº¡n khÃ´ng cáº§n code gÃ¬ cáº£, chá»‰ cáº§n máº¯t tinh má»™t chÃºt.

1. Má»Ÿ link quÃ¡n Äƒn Ä‘Ã³ trÃªn trÃ¬nh duyá»‡t Chrome/Edge.
* VÃ­ dá»¥: `shopeefood.vn/ho-chi-minh/tra-sua-toco-toco`


2. Báº¥m chuá»™t pháº£i vÃ o báº¥t ká»³ Ä‘Ã¢u trÃªn trang -> Chá»n **"Inspect" (Kiá»ƒm tra)** hoáº·c báº¥m `F12`.
3. Báº¥m tá»• há»£p phÃ­m **`Ctrl + F`** (TÃ¬m kiáº¿m trong mÃ£ nguá»“n).
4. GÃµ tá»« khÃ³a: **`"delivery_id":`**
5. Báº¡n sáº½ tháº¥y nÃ³ highlight má»™t dÃ²ng code cÃ³ dáº¡ng: `{"delivery_id": 12345, ...}`
* ğŸ‘‰ Sá»‘ **12345** chÃ­nh lÃ  cÃ¡i ID báº¡n cáº§n tÃ¬m!



---

### CÃCH 2: DÃ™NG CODE PYTHON Äá»‚ "GIáº¢I MÃƒ" LINK (Tá»± Ä‘á»™ng hÃ³a)

Náº¿u báº¡n muá»‘n input vÃ o lÃ  má»™t danh sÃ¡ch Link vÃ  tool tá»± crawl, báº¡n cáº§n má»™t hÃ m Ä‘á»ƒ "Resolve Slug".

ShopeeFood cÃ³ má»™t API áº©n Ä‘á»ƒ lÃ m viá»‡c nÃ y. API nÃ y nháº­n vÃ o cÃ¡i Ä‘uÃ´i URL (slug) vÃ  tráº£ vá» ID quÃ¡n.

**Code Python Ä‘á»ƒ láº¥y ID tá»« Link:**

```python
import requests

def get_id_from_url(shopeefood_url):
    """
    Input: https://shopeefood.vn/ho-chi-minh/tra-sua-toco-toco
    Output: 12345 (ID cá»§a quÃ¡n)
    """
    # 1. TÃ¡ch láº¥y pháº§n slug (cÃ¡i Ä‘uÃ´i sau cÃ¹ng)
    # VÃ­ dá»¥: "tra-sua-toco-toco"
    if "/ha-noi/" in shopeefood_url:
        slug = shopeefood_url.split("/ha-noi/")[-1]
    elif "/ho-chi-minh/" in shopeefood_url:
        slug = shopeefood_url.split("/ho-chi-minh/")[-1]
    else:
        # Link dáº¡ng khÃ¡c hoáº·c tá»‰nh khÃ¡c, thá»­ split dáº¥u / cuá»‘i cÃ¹ng
        slug = shopeefood_url.strip("/").split("/")[-1]
    
    # Loáº¡i bá» cÃ¡c tham sá»‘ thá»«a náº¿u cÃ³ (vd: ?share=...)
    slug = slug.split("?")[0]

    print(f"ğŸ” Äang tÃ¬m ID cho slug: {slug}...")

    # 2. Gá»i API Ä‘á»ƒ láº¥y thÃ´ng tin quÃ¡n tá»« slug
    api_url = f"https://gappapi.deliverynow.vn/api/delivery/get_detail?request_id={slug}&id_type=2"
    
    headers = {
        'x-foody-client-type': '1',
        'x-foody-client-version': '3.0.0',
        'x-foody-api-version': '1',
    }

    try:
        response = requests.get(api_url, headers=headers)
        data = response.json()
        
        # Láº¥y delivery_id tá»« káº¿t quáº£ tráº£ vá»
        delivery_detail = data.get('reply', {}).get('delivery_detail', {})
        found_id = delivery_detail.get('delivery_id')
        
        if found_id:
            print(f"âœ… TÃ¬m tháº¥y ID: {found_id}")
            return found_id
        else:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y ID nÃ o.")
            return None
            
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        return None

# --- CHáº Y THá»¬ ---
link_test = "https://shopeefood.vn/ho-chi-minh/phuc-long-lotte-mart-le-dai-hanh"
quan_id = get_id_from_url(link_test)

# Sau khi cÃ³ ID, báº¡n nÃ©m nÃ³ vÃ o hÃ m crawl comment lÃºc nÃ£y
# if quan_id:
#     reviews = get_reviews_of_restaurant(quan_id, ...)

```

### Táº¡i sao láº¡i lÃ  `id_type=2`?

Trong API ShopeeFood:

* `id_type=1`: NghÄ©a lÃ  báº¡n Ä‘ang tÃ¬m theo sá»‘ ID (VÃ­ dá»¥: `request_id=12345`).
* `id_type=2`: NghÄ©a lÃ  báº¡n Ä‘ang tÃ¬m theo tÃªn Slug (VÃ­ dá»¥: `request_id=quan-com-tam`).

Báº¡n cÃ³ thá»ƒ tÃ­ch há»£p Ä‘oáº¡n hÃ m `get_id_from_url` nÃ y vÃ o crawler chÃ­nh Ä‘á»ƒ tiá»‡n sá»­ dá»¥ng hÆ¡n. Báº¡n muá»‘n mÃ¬nh ghÃ©p nÃ³ vÃ o file `crawl_shopeefood.py` luÃ´n khÃ´ng?
Báº¡n há»i ráº¥t Ä‘Ãºng Ã½! Logic cá»§a báº¡n lÃ : *Náº¿u link quÃ¡n Ä‘Ã£ lÃ  `shopeefood.vn/ha-noi/...` thÃ¬ cháº¯c cháº¯n nÃ³ á»Ÿ HÃ  Ná»™i rá»“i, cáº§n gÃ¬ pháº£i lÆ°u cÃ¡i `city_id` (217 hay 218) lÃ m gÃ¬ cho thá»«a?*

Tuy nhiÃªn, trong **Khoa há»c Dá»¯ liá»‡u (Data Science)** vÃ  khi lÃ m **Há»‡ thá»‘ng lá»›n**, viá»‡c lÆ°u mÃ£ Ä‘á»‹nh danh (ID) vÃ¹ng miá»n váº«n cá»±c ká»³ quan trá»ng vÃ¬ 3 lÃ½ do "chÃ­ máº¡ng" sau:

### 1. MÃ¡y tÃ­nh "thÃ­ch" sá»‘ hÆ¡n chá»¯ (Performance)

* Khi báº¡n crawl 50.000 quÃ¡n, náº¿u báº¡n muá»‘n so sÃ¡nh Rating trung bÃ¬nh cá»§a HÃ  Ná»™i vs TP.HCM.
* **CÃ¡ch dÃ¹ng Text (nhÆ° báº¡n nghÄ©):**
`SELECT avg(rating) FROM reviews WHERE url LIKE '%ha-noi%'`
-> MÃ¡y pháº£i quÃ©t tá»«ng kÃ½ tá»± trong chuá»—i URL dÃ i ngoáº±ng -> **Ráº¥t Cháº­m**.
* **CÃ¡ch dÃ¹ng ID (Chuáº©n Data Science):**
`SELECT avg(rating) FROM reviews WHERE city_id = 218`
-> So sÃ¡nh sá»‘ nguyÃªn (Integer) -> **Cá»±c Nhanh**.

### 2. Xá»­ lÃ½ "BiÃªn giá»›i má»m" vÃ  Dá»¯ liá»‡u báº©n

* ÄÃ´i khi URL khÃ´ng chá»©a tÃªn thÃ nh phá»‘ rÃµ rÃ ng hoáº·c cáº¥u trÃºc URL thay Ä‘á»•i (ShopeeFood lÃ¢u lÃ¢u láº¡i Ä‘á»•i format URL).
* CÃ³ nhá»¯ng quÃ¡n chuá»—i (Franchise) tÃªn giá»‘ng há»‡t nhau. VÃ­ dá»¥: "TrÃ  sá»¯a Tocotoco". Náº¿u báº¡n chá»‰ lÆ°u tÃªn quÃ¡n hoáº·c URL chung chung, báº¡n sáº½ khÃ´ng biáº¿t comment nÃ y lÃ  chá»­i Tocotoco á»Ÿ Cáº§u Giáº¥y (HN) hay Tocotoco á»Ÿ Quáº­n 1 (HCM).
* `city_id` lÃ  Ä‘á»‹nh danh gá»‘c tá»« Database cá»§a ShopeeFood, nÃ³ khÃ´ng bao giá» sai.

### 3. Phá»¥c vá»¥ cho váº½ biá»ƒu Ä‘á»“ (Visualization)

* CÃ¡c thÆ° viá»‡n váº½ biá»ƒu Ä‘á»“ nhÆ° PowerBI, Tableau hay Python (Matplotlib) thÆ°á»ng yÃªu cáº§u cá»™t phÃ¢n loáº¡i rÃµ rÃ ng (Categorical Data).
* Náº¿u báº¡n lÆ°u `city_id=217`, báº¡n cÃ³ thá»ƒ dá»… dÃ ng map nÃ³ thÃ nh "Miá»n Nam", `218` thÃ nh "Miá»n Báº¯c" Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ so sÃ¡nh vÃ¹ng miá»n.

### TÃ³m láº¡i:

Báº¡n **hoÃ n toÃ n cÃ³ thá»ƒ** suy ra vÃ¹ng miá»n tá»« URL, nhÆ°ng **Ä‘á»«ng lÃ m tháº¿**. HÃ£y lÆ°u `city_id` hoáº·c tÃªn thÃ nh phá»‘ (`city_name`) thÃ nh má»™t cá»™t riÃªng trong file JSON.

**Lá»i khuyÃªn:**
Trong file `schema_sentiment.py` mÃ¬nh gá»­i, mÃ¬nh Ä‘Ã£ Ä‘á»ƒ trÆ°á»ng `city` lÃ  String. Báº¡n cÃ³ thá»ƒ lÆ°u tháº³ng lÃ  `"HaNoi"` hoáº·c `"HCM"` cho dá»… Ä‘á»c cÅ©ng Ä‘Æ°á»£c, khÃ´ng nháº¥t thiáº¿t pháº£i lÆ°u sá»‘ `218`. NhÆ°ng **báº¯t buá»™c pháº£i cÃ³ cá»™t nÃ y** tÃ¡ch biá»‡t ra khá»i URL.

Báº¡n cÃ³ muá»‘n mÃ¬nh sá»­a láº¡i code Crawler Ä‘á»ƒ nÃ³ **tá»± Ä‘á»™ng phÃ¡t hiá»‡n** thÃ nh phá»‘ tá»« cÃ¡i Link báº¡n nháº­p vÃ o khÃ´ng? (Thay vÃ¬ báº¡n pháº£i nháº­p tay ID thÃ nh phá»‘).
Ã tÆ°á»Ÿng cá»§a báº¡n ráº¥t thá»±c táº¿. Thay vÃ¬ pháº£i ngá»“i tra ID thÃ nh phá»‘ thá»§ cÃ´ng, chÃºng ta sáº½ Ä‘á»ƒ code **tá»± Ä‘á»™ng "Ä‘á»c" URL** Ä‘á»ƒ biáº¿t quÃ¡n Ä‘Ã³ á»Ÿ HÃ  Ná»™i hay SÃ i GÃ²n, tá»« Ä‘Ã³ tá»± Ä‘á»™ng nÃ©m dá»¯ liá»‡u vÃ o Ä‘Ãºng file (tá»‡p khÃ¡ch hÃ ng) tÆ°Æ¡ng á»©ng.

DÆ°á»›i Ä‘Ã¢y lÃ  phiÃªn báº£n **Crawler ThÃ´ng Minh (Smart Crawler)**. Báº¡n chá»‰ cáº§n nÃ©m má»™t danh sÃ¡ch Link lá»™n xá»™n vÃ o, nÃ³ sáº½ tá»± lá»c vÃ  chia vá» Ä‘Ãºng chuá»“ng.

### 1. File Schema (`schema_sentiment.py`)

*Giá»¯ nguyÃªn file schema cÅ© mÃ¬nh Ä‘Ã£ gá»­i, khÃ´ng cáº§n sá»­a gÃ¬.*

### 2. File Code Crawl Má»›i (`crawl_auto_city.py`)

HÃ£y táº¡o file má»›i tÃªn **`crawl_auto_city.py`** vÃ  dÃ¡n code nÃ y vÃ o.

```python
import requests
import json
import time
import random
import os
from schema_sentiment import ReviewItem

# --- Cáº¤U HÃŒNH ---
DATA_FOLDER = "data_sentiment"
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    'x-foody-client-type': '1',
    'x-foody-client-version': '3.0.0',
    'x-foody-api-version': '1',
}

# Báº¢NG Tá»ª ÄIá»‚N MAP Tá»ª URL -> ID THÃ€NH PHá»
CITY_MAPPING = {
    "ha-noi": {"id": 218, "name": "HaNoi"},
    "ho-chi-minh": {"id": 217, "name": "HCM"},
    "da-nang": {"id": 219, "name": "DaNang"},
    "hai-phong": {"id": 220, "name": "HaiPhong"},
    # CÃ³ thá»ƒ thÃªm cÃ¡c tá»‰nh khÃ¡c náº¿u cáº§n
}

def analyze_url(url):
    """
    PhÃ¢n tÃ­ch URL Ä‘á»ƒ tÃ¡ch Slug vÃ  ThÃ nh phá»‘
    Input: https://shopeefood.vn/ha-noi/pho-thin-lo-duc
    Output: slug='pho-thin-lo-duc', city_info={'id': 218, 'name': 'HaNoi'}
    """
    # XÃ³a pháº§n https://shopeefood.vn/
    clean_url = url.replace("https://shopeefood.vn/", "").replace("http://shopeefood.vn/", "")
    parts = clean_url.split("/")
    
    # URL chuáº©n thÆ°á»ng lÃ : [ten-thanh-pho]/[ten-quan]
    if len(parts) >= 2:
        city_slug = parts[0]
        restaurant_slug = parts[1].split("?")[0] # Bá» tham sá»‘ ? sau slug
        
        # Tra cá»©u trong tá»« Ä‘iá»ƒn
        city_info = CITY_MAPPING.get(city_slug)
        if city_info:
            return restaurant_slug, city_info
            
    return None, None

def get_restaurant_id_from_slug(slug):
    """Gá»i API Ä‘á»ƒ Ä‘á»•i tÃªn quÃ¡n (slug) thÃ nh ID sá»‘"""
    url = f"https://gappapi.deliverynow.vn/api/delivery/get_detail?request_id={slug}&id_type=2"
    try:
        resp = requests.get(url, headers=HEADERS)
        data = resp.json()
        delivery_detail = data.get('reply', {}).get('delivery_detail', {})
        
        return {
            "id": delivery_detail.get('delivery_id'),
            "name": delivery_detail.get('name')
        }
    except:
        return None

def crawl_reviews_by_link(url_list, limit_per_shop=100):
    print(f"ğŸš€ Äang xá»­ lÃ½ danh sÃ¡ch {len(url_list)} quÃ¡n Äƒn...")
    
    for url in url_list:
        print(f"\nğŸ”— Checking: {url}")
        
        # 1. Tá»± Ä‘á»™ng phÃ¡t hiá»‡n thÃ nh phá»‘
        slug, city_info = analyze_url(url)
        
        if not city_info:
            print("   âš ï¸ KhÃ´ng nháº­n diá»‡n Ä‘Æ°á»£c thÃ nh phá»‘ tá»« Link nÃ y. Bá» qua.")
            continue
            
        print(f"   -> PhÃ¡t hiá»‡n: {city_info['name']} (Slug: {slug})")
        
        # 2. Láº¥y ID quÃ¡n
        shop_info = get_restaurant_id_from_slug(slug)
        if not shop_info or not shop_info['id']:
            print("   âŒ KhÃ´ng láº¥y Ä‘Æ°á»£c ID quÃ¡n. Link cÃ³ thá»ƒ bá»‹ lá»—i.")
            continue
            
        shop_id = shop_info['id']
        shop_name = shop_info['name']
        
        # 3. Táº¡o tÃªn file tá»± Ä‘á»™ng theo thÃ nh phá»‘ (Tá»° Äá»˜NG PHÃ‚N LOáº I Tá»†P KHÃCH HÃ€NG)
        output_file = os.path.join(DATA_FOLDER, f"reviews_{city_info['name']}.jsonl")
        
        # 4. Crawl Review
        print(f"   -> Äang táº£i review cho quÃ¡n: {shop_name}...")
        api_review = f"https://gappapi.deliverynow.vn/api/delivery/get_reply?id_type=1&request_id={shop_id}&sort_type=1&limit={limit_per_shop}"
        
        try:
            res = requests.get(api_review, headers=HEADERS)
            reviews = res.json().get('reply', {}).get('reply_list', [])
            
            if not reviews:
                print("   âš ï¸ QuÃ¡n nÃ y chÆ°a cÃ³ review nÃ o.")
                continue

            with open(output_file, 'a', encoding='utf-8') as f:
                for rev in reviews:
                    item = ReviewItem(
                        review_id=rev.get('id'),
                        restaurant_id=shop_id,
                        restaurant_name=shop_name,
                        city=city_info['name'], # LÆ°u tÃªn thÃ nh phá»‘ vÃ o tá»«ng dÃ²ng
                        user_name=rev.get('name', 'Anonymous'),
                        comment=rev.get('comment', ''),
                        rating=rev.get('rating', 0),
                        review_date=rev.get('created_on', '')
                    )
                    f.write(item.to_json_line() + "\n")
            
            print(f"   âœ… ÄÃ£ lÆ°u {len(reviews)} reviews vÃ o file: reviews_{city_info['name']}.jsonl")
            
        except Exception as e:
            print(f"   âŒ Lá»—i crawl review: {e}")
            
        # Nghá»‰ nháº¹ Ä‘á»ƒ khÃ´ng bá»‹ spam
        time.sleep(random.uniform(1, 3))

# --- MAIN RUN ---
if __name__ == "__main__":
    
    # Báº N CHá»ˆ Cáº¦N DÃN LIST LINK VÃ€O ÄÃ‚Y (Lá»˜N Xá»˜N CÅ¨NG ÄÆ¯á»¢C)
    # Code sáº½ tá»± tÃ¡ch: Link nÃ o HÃ  Ná»™i -> VÃ o file HaNoi, Link nÃ o HCM -> VÃ o file HCM
    
    MY_LINKS = [
        # Link HÃ  Ná»™i
        "https://shopeefood.vn/ha-noi/pho-thin-lo-duc", 
        "https://shopeefood.vn/ha-noi/bun-cha-dac-kim-hang-manh",
        
        # Link SÃ i GÃ²n
        "https://shopeefood.vn/ho-chi-minh/com-tam-cali-nguyen-trai-q1",
        "https://shopeefood.vn/ho-chi-minh/phuc-long-lotte-mart-le-dai-hanh",
        
        # Link ÄÃ  Náºµng
        "https://shopeefood.vn/da-nang/my-quang-ba-mua-tran-binh-trong"
    ]
    
    crawl_reviews_by_link(MY_LINKS, limit_per_shop=50)

```

### CÃ¡ch hoáº¡t Ä‘á»™ng cá»§a code nÃ y

1. **Äáº§u vÃ o:** Má»™t danh sÃ¡ch `MY_LINKS` chá»©a cÃ¡c URL ShopeeFood.
2. **Bá»™ lá»c (`analyze_url`):**
* NÃ³ cáº¯t chuá»—i URL: `.../ha-noi/pho-thin` -> NÃ³ báº¯t Ä‘Æ°á»£c chá»¯ **`ha-noi`**.
* NÃ³ tra tá»« Ä‘iá»ƒn: `ha-noi` => LÆ°u vÃ o file `reviews_HaNoi.jsonl`.
* Náº¿u link lÃ  `.../ho-chi-minh/...` => LÆ°u vÃ o file `reviews_HCM.jsonl`.


3. **Äáº§u ra:**
* Báº¡n khÃ´ng cáº§n táº¡o nhiá»u file code. Chá»‰ cáº§n cháº¡y 1 file nÃ y.
* Káº¿t quáº£ tá»± Ä‘á»™ng sinh ra trong thÆ° má»¥c `data_sentiment`:
* ğŸ“ `reviews_HaNoi.jsonl` (Chá»©a review Phá»Ÿ ThÃ¬n, BÃºn Cháº£...)
* ğŸ“ `reviews_HCM.jsonl` (Chá»©a review CÆ¡m Táº¥m, PhÃºc Long...)





CÃ¡ch nÃ y Ä‘Ã¡p á»©ng hoÃ n háº£o yÃªu cáº§u cá»§a báº¡n: **DÃ¹ng link Ä‘á»ƒ Ä‘á»‹nh vá»‹ khÃ¡ch hÃ ng.**
