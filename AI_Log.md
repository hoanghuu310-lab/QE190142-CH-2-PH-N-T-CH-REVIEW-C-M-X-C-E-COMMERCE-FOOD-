
B·∫°n ƒë√£ n√≥i:
ƒê√£ t·∫£i l√™n ·∫£nh
h√£y ƒë·ªçc v√† ph√¢n t√≠ch topic 2
ChatGPT ƒë√£ n√≥i:
M√¨nh ƒë√£ ƒë·ªçc CH·ª¶ ƒê·ªÄ 2: PH√ÇN T√çCH REVIEW & C·∫¢M X√öC (E-COMMERCE / FOOD). D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt + g·ª£i √Ω c√°ch tri·ªÉn khai (ƒë√∫ng ki·ªÉu b√†i Data/DS cho sinh vi√™n SE ·ªü FPT üëç).

1. M·ª•c ti√™u c·ªßa Topic 2 (Hi·ªÉu b√†i to√°n)
Topic n√†y xoay quanh khai th√°c v√† ph√¢n t√≠ch c·∫£m x√∫c kh√°ch h√†ng th√¥ng qua review tr√™n c√°c n·ªÅn t·∫£ng TMƒêT/ƒë·ªì ƒÉn.

üëâ Tr·∫£ l·ªùi c√°c c√¢u h·ªèi:

Ch·∫•t l∆∞·ª£ng d·ªãch v·ª•/s·∫£n ph·∫©m thay ƒë·ªïi theo th·ªùi gian kh√¥ng?

Kh√°ch ch√™ v√† khen ƒëi·ªÅu g√¨ nhi·ªÅu nh·∫•t?

C√≥ kh√°c bi·ªát h√†nh vi review theo v√πng mi·ªÅn hay kh√¥ng?

Review d√†i/ng·∫Øn c√≥ li√™n quan ƒë·∫øn m·ª©c ƒë·ªô h√†i l√≤ng kh√¥ng?

2. D·ªØ li·ªáu c·∫ßn crawl (Input Data)
üìå Ngu·ªìn d·ªØ li·ªáu
ShopeeFood

Foody

Shopee

Google Maps Reviews

üìå C√°c tr∆∞·ªùng b·∫Øt bu·ªôc
Tr∆∞·ªùng	√ù nghƒ©a
content	N·ªôi dung review
rating	S·ªë sao (1‚Äì5)
date	Ng√†y review
shop_name	T√™n qu√°n/shop
üëâ C√≥ th·ªÉ m·ªü r·ªông:

location (qu·∫≠n/t·ªânh)

reply_from_shop

platform

3. Ph√¢n t√≠ch C√ÇU H·ªéI C·ª®NG
üîπ 3.1. T·ª∑ l·ªá rating 1‚≠ê vs 5‚≠ê theo th·ªùi gian
M·ª•c ti√™u

Xem ch·∫•t l∆∞·ª£ng d·ªãch v·ª• c√≥ c·∫£i thi·ªán hay xu·ªëng c·∫•p kh√¥ng

C√°ch l√†m

Group theo th√°ng ho·∫∑c qu√Ω

T√≠nh:

% review 1 sao

% review 5 sao

V·∫Ω:

Line chart

Stacked bar chart

Insight mong ƒë·ª£i

1‚≠ê tƒÉng ƒë·ªôt bi·∫øn ‚Üí c√≥ th·ªÉ do:

Thay ƒë·ªïi shipper

TƒÉng gi√°

Ch·∫•t l∆∞·ª£ng gi·∫£m

5‚≠ê tƒÉng ‚Üí qu√°n c·∫£i thi·ªán d·ªãch v·ª•

üîπ 3.2. Top 10 keyword trong review 1‚≠ê v√† 5‚≠ê
M·ª•c ti√™u
üëâ Hi·ªÉu nguy√™n nh√¢n b·ªã ch√™ / ƒë∆∞·ª£c khen

C√°ch l√†m

T√°ch review:

Nh√≥m 1‚≠ê

Nh√≥m 5‚≠ê

Ti·ªÅn x·ª≠ l√Ω NLP:

Lowercase

Remove stopwords

Tokenization

Th·ªëng k√™ t·∫ßn su·∫•t t·ª´

V√≠ d·ª• k·∫øt qu·∫£

‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê:

‚Äúngon‚Äù, ‚Äúnhi·ªÅu‚Äù, ‚Äúr·∫ª‚Äù, ‚Äúship nhanh‚Äù, ‚Äúƒë√≥ng g√≥i k·ªπ‚Äù

‚≠ê:

‚Äúngu·ªôi‚Äù, ‚Äútr·ªÖ‚Äù, ‚Äú√≠t‚Äù, ‚Äúm·∫∑n‚Äù, ‚Äúshipper‚Äù

üëâ C√≥ th·ªÉ v·∫Ω:

WordCloud

Bar chart top keywords

4. Ph√¢n t√≠ch C√ÇU H·ªéI M·ªû (ƒÇn ƒëi·ªÉm)
üîπ 4.1. Kh√°c bi·ªát v√πng mi·ªÅn (B·∫Øc / Nam)
Gi·∫£ thuy·∫øt

Mi·ªÅn B·∫Øc:

Review d√†i

Hay g√≥p √Ω chi ti·∫øt

Mi·ªÅn Nam:

Review ng·∫Øn

ƒê√°nh gi√° c·∫£m t√≠nh

Ph√¢n t√≠ch

So s√°nh:

ƒê·ªô d√†i comment trung b√¨nh

T·ª∑ l·ªá 1‚≠ê / 5‚≠ê

Test th·ªëng k√™:

T-test / Mann-Whitney

üîπ 4.2. ƒê·ªô d√†i comment c√≥ li√™n quan ƒë·∫øn s·ªë sao kh√¥ng?
C√°ch l√†m

T√≠nh:

comment_length = s·ªë t·ª´

Scatter plot:

X: s·ªë sao

Y: ƒë·ªô d√†i

Correlation (Pearson / Spearman)

Insight th∆∞·ªùng g·∫∑p

Review 1‚≠ê ‚Üí d√†i (b·ª©c x√∫c)

Review 5‚≠ê ‚Üí ng·∫Øn (‚Äúngon‚Äù, ‚Äúok‚Äù)

5. K·ªπ thu·∫≠t & C√¥ng c·ª• n√™n d√πng
üìå Crawl
Selenium / Playwright

Google Maps API (n·∫øu c√≥ key)

üìå X·ª≠ l√Ω
Python: pandas, regex

NLP:

underthesea (ti·∫øng Vi·ªát)

VnCoreNLP

üìå Ph√¢n t√≠ch n√¢ng cao (n·∫øu mu·ªën n·ªïi b·∫≠t)
Sentiment classification (rule-based ho·∫∑c ML)

Topic Modeling (LDA)

So s√°nh gi·ªØa platforms

6. V√¨ sao Topic 2 r·∫•t ‚Äúngon‚Äù cho b√°o c√°o?
‚úÖ D·ªØ li·ªáu d·ªÖ ki·∫øm
‚úÖ C√≥ NLP + Time Series
‚úÖ Insight th·ª±c t·∫ø, d·ªÖ tr√¨nh b√†y
‚úÖ R·∫•t h·ª£p ƒë·ªÉ:

Demo EDA

Visualization

Business insight

N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ:

‚úÖ Vi·∫øt outline b√°o c√°o cho topic 2

‚úÖ ƒê·ªÅ xu·∫•t dataset m·∫´u

‚úÖ Vi·∫øt code pipeline (crawl ‚Üí clean ‚Üí analyze)

‚úÖ G·ª£i √Ω slide thuy·∫øt tr√¨nh

üëâ B·∫°n ƒëang l√†m b√°o c√°o m√¥n n√†o (AIL / DSA / Data Mining)?

B·∫°n ƒë√£ n√≥i:
for _ in range(5):  # Thu 5 cap parent
                if current is None:
                    break

                # Lay tat ca text trong container
                text = current.get_text(separator="\n", strip=True)

                # Neu co text dai hon, lay no
                if len(text) > len(content_text):
                    content_text = text

                current = current.find_parent()

            if not content_text:
                continue

            # Tach content thanh cac dong
            lines = content_text.split('\n')

            # Tim dong dau tien SAU username va khong phai metadata
            content_lines = []
            found_user = False

            skip_words = ['G√Ä', 'ƒê·∫†I B√ÄNG', 'B√°o x·∫•u', 'ng√†y', 'gi·ªù', 'Th√≠ch',
                          'Kh√¥ng th√≠ch', 'Share', 'FactTinhte', 'ƒë√£ n√≥i', 'said']

            for line in lines:
                line = line.strip()

                if not line:
                    continue

                # Neu gap username, bat dau ghi nhan
                if username in line:
                    found_user = True
                    continue

                # Sau khi gap username
                if found_user:
                    # Bo qua metadata
                    if any(word in line for word in skip_words):
                        continue

                    # Bo qua so don doc
                    if re.match(r'^\d+$', line):
                        continue

                    # Bo qua dong ngan (< 10 ky tu)
                    if len(line) < 10:
                        continue

                    # Day la noi dung
                    content_lines.append(line)

                    # Chi lay toi da 3 dong
                    if len(content_lines) >= 3:
                        break

            # Gop noi dung
            content = ' '.join(content_lines).strip()

            # Lam sach
            content = re.sub(r'@\w+', '', content)
            content = re.sub(r'[‚Üë‚Üí‚Üê‚Üì]', '', content)
            content = re.sub(r'\s+', ' ', content)

            # Chi them neu co noi dung hop le
            if content and len(content) > 15:
                processed.add(username)

                data.append({
                    "index": len(data) + 1,
                    "username": username,
                    "content": content
                })

                print("=" * 60)
                print(f"Post #{len(data)} | {username}")
                print(content[:200])

                if len(data) >= limit_posts:
                    break

        return data

    except Exception as e:
        print(f"Loi: {e}")
        import traceback
        traceback.print_exc()
        return []

    finally:
        driver.quit()


if __name__ == "__main__":
    print("Bat dau crawl Tinhte.vn...")
    posts = crawl_tinhte(limit_posts=20)

    if posts:
        with open("tinhte_posts.json", "w", encoding="utf-8") as f:
json.dump(posts, f, ensure_ascii=False, indent=2)
        print(f"\nDa luu {len(posts)} posts vao tinhte_posts.json")
    else:
        print("\nKhong crawl duoc du lieu!")
        print("Hay kiem tra file tinhte_selenium.html")
ChatGPT ƒë√£ n√≥i:
M√¨nh s·∫Ω ƒë·ªçc v√† ph√¢n t√≠ch chi ti·∫øt ƒëo·∫°n code b·∫°n g·ª≠i, t·∫≠p trung v√†o m·ª•c ƒë√≠ch, lu·ªìng x·ª≠ l√Ω, ƒëi·ªÉm m·∫°nh ‚Äì ƒëi·ªÉm y·∫øu, v√† li√™n h·ªá v·ªõi Topic 2 (Review & Sentiment Analysis) ƒë·ªÉ b·∫°n d√πng cho b√°o c√°o ho·∫∑c ch·ªânh s·ª≠a cho t·ªët h∆°n.

1. M·ª•c ti√™u t·ªïng th·ªÉ c·ªßa ƒëo·∫°n code
üëâ ƒêo·∫°n code n√†y d√πng ƒë·ªÉ crawl n·ªôi dung b√†i vi·∫øt / comment tr√™n Tinhte.vn b·∫±ng Selenium + BeautifulSoup, sau ƒë√≥:

L·∫•y username

Tr√≠ch xu·∫•t n·ªôi dung comment th·ª±c s·ª± (lo·∫°i b·ªè metadata, n√∫t like, gi·ªù ƒëƒÉng, icon‚Ä¶)

L√†m s·∫°ch text

L∆∞u k·∫øt qu·∫£ ra file JSON

üìå R·∫•t ph√π h·ª£p cho:

Thu th·∫≠p review / opinion

L√†m sentiment analysis

√Åp d·ª•ng tr·ª±c ti·∫øp cho CH·ª¶ ƒê·ªÄ 2: PH√ÇN T√çCH REVIEW & C·∫¢M X√öC

2. Ph√¢n t√≠ch t·ª´ng kh·ªëi logic quan tr·ªçng
üîπ 2.1. Leo ng∆∞·ª£c DOM ƒë·ªÉ t√¨m container ch·ª©a n·ªôi dung
for _ in range(5):  # Thu 5 cap parent
    if current is None:
        break
üëâ √ù t∆∞·ªüng:

Do HTML c·ªßa Tinhte kh√¥ng c·ªë ƒë·ªãnh, n√™n:

T·ª´ node hi·ªán t·∫°i ‚Üí ƒëi l√™n t·ªëi ƒëa 5 c·∫•p cha

T√¨m container c√≥ nhi·ªÅu text nh·∫•t

text = current.get_text(separator="\n", strip=True)

if len(text) > len(content_text):
    content_text = text
üìå ƒê√¢y l√† heuristic th√¥ng minh:

Container ch·ª©a comment th·∫≠t th∆∞·ªùng c√≥ text d√†i nh·∫•t

Tr√°nh ph·ª• thu·ªôc class name ‚Üí √≠t b·ªã g√£y khi web ƒë·ªïi giao di·ªán

‚úÖ ƒêi·ªÉm m·∫°nh:

Linh ho·∫°t

√çt hard-code

‚ö†Ô∏è H·∫°n ch·∫ø:

C√≥ th·ªÉ l·∫•y th·ª´a text (quote, ch·ªØ k√Ω, ch·ªØ trong n√∫t)

üîπ 2.2. T√°ch text th√†nh c√°c d√≤ng
lines = content_text.split('\n')
üëâ Chuy·ªÉn block text th√†nh danh s√°ch d√≤ng ƒë·ªÉ:

D·ªÖ l·ªçc metadata

D·ªÖ ph√°t hi·ªán n·ªôi dung th·∫≠t

3. Logic ph√°t hi·ªán n·ªôi dung comment th·∫≠t
üîπ 3.1. D√πng username l√†m m·ªëc b·∫Øt ƒë·∫ßu
if username in line:
    found_user = True
    continue
üëâ Gi·∫£ ƒë·ªãnh:

N·ªôi dung comment lu√¥n n·∫±m sau d√≤ng ch·ª©a username

‚úÖ H·ª£p l√Ω v·ªõi c·∫•u tr√∫c forum
‚ö†Ô∏è C√≥ th·ªÉ sai n·∫øu username xu·∫•t hi·ªán trong n·ªôi dung quote

üîπ 3.2. B·ªô l·ªçc metadata (r·∫•t quan tr·ªçng)
skip_words = ['G√Ä', 'ƒê·∫†I B√ÄNG', 'B√°o x·∫•u', 'ng√†y', 'gi·ªù', 'Th√≠ch',
              'Kh√¥ng th√≠ch', 'Share', 'FactTinhte', 'ƒë√£ n√≥i', 'said']
üëâ Lo·∫°i b·ªè:

C·∫•p ƒë·ªô user (G√†, ƒê·∫°i B√†ng‚Ä¶)

Th·ªùi gian ƒëƒÉng

N√∫t t∆∞∆°ng t√°c

Quote

üìå ƒê√¢y l√† rule-based cleaning, r·∫•t hay cho ti·∫øng Vi·ªát.

üîπ 3.3. C√°c rule l·ªçc n·ªôi dung r√°c
if re.match(r'^\d+$', line):
    continue
‚Üí Lo·∫°i d√≤ng ch·ªâ c√≥ s·ªë (like count)

if len(line) < 10:
    continue
‚Üí Tr√°nh m·∫•y c√¢u ki·ªÉu ‚Äúok‚Äù, ‚Äúup‚Äù, ‚Äú=))‚Äù

üëâ ƒêi·ªÅu n√†y gi√∫p:

N·ªôi dung crawl ch·∫•t l∆∞·ª£ng h∆°n

Ph√π h·ª£p cho NLP

üîπ 3.4. Gi·ªõi h·∫°n t·ªëi ƒëa 3 d√≤ng n·ªôi dung
if len(content_lines) >= 3:
    break
üìå M·ª•c ƒë√≠ch:

Tr√°nh l·∫•y qu√° d√†i

Gi·ªØ n·ªôi dung s√∫c t√≠ch (review-style)

‚ö†Ô∏è N·∫øu l√†m sentiment n√¢ng cao ‚Üí c√≥ th·ªÉ b·ªè gi·ªõi h·∫°n n√†y

4. L√†m s·∫°ch vƒÉn b·∫£n (Text Cleaning)
content = re.sub(r'@\w+', '', content)
‚Üí X√≥a mention

content = re.sub(r'[‚Üë‚Üí‚Üê‚Üì]', '', content)
‚Üí X√≥a icon ƒëi·ªÅu h∆∞·ªõng

content = re.sub(r'\s+', ' ', content)
‚Üí Chu·∫©n h√≥a kho·∫£ng tr·∫Øng

‚úÖ ƒê√∫ng chu·∫©n pipeline NLP

5. ƒêi·ªÅu ki·ªán l∆∞u d·ªØ li·ªáu
if content and len(content) > 15:
üëâ Ch·ªâ l∆∞u:

C√≥ n·ªôi dung

ƒê·ªß d√†i ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c

processed.add(username)
‚Üí Tr√°nh crawl tr√πng user

6. Output d·ªØ li·ªáu
data.append({
    "index": len(data) + 1,
    "username": username,
    "content": content
})
üìå Format n√†y r·∫•t ph√π h·ª£p ƒë·ªÉ:

L√†m sentiment analysis

Keyword extraction

Topic modeling

7. Li√™n h·ªá tr·ª±c ti·∫øp v·ªõi CH·ª¶ ƒê·ªÄ 2
B·∫°n ƒë√£ c√≥ s·∫µn:
‚úÖ content ‚Üí ph√¢n t√≠ch c·∫£m x√∫c
‚ùå Ch∆∞a c√≥ rating, date, shop_name

üëâ C√≥ th·ªÉ m·ªü r·ªông:

Map keyword ‚Üí rating (rule-based)

Crawl th√™m:

Ng√†y ƒëƒÉng

Topic s·∫£n ph·∫©m / b√†i vi·∫øt

8. ƒê√°nh gi√° t·ªïng quan (ƒë·ªÉ vi·∫øt v√†o b√°o c√°o)
‚úÖ ƒêi·ªÉm m·∫°nh
Kh√¥ng ph·ª• thu·ªôc class HTML

L·ªçc r√°c t·ªët

Ph√π h·ª£p ti·∫øng Vi·ªát

Data s·∫°ch ƒë·ªÉ NLP

‚ö†Ô∏è ƒêi·ªÉm h·∫°n ch·∫ø
Rule-based ‚Üí kh√≥ t·ªïng qu√°t

Ch∆∞a c√≥ sentiment label

Ch∆∞a x·ª≠ l√Ω emoji (üòäüò°)

9. G·ª£i √Ω n√¢ng c·∫•p (ƒÉn ƒëi·ªÉm cao)
Th√™m:

emoji.demojize(content)
Th√™m sentiment score (VADER / PhoBERT)

So s√°nh:

Comment d√†i vs ng·∫Øn

Toxic keyword vs sentiment

N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ:

üî• Vi·∫øt m√¥ t·∫£ thu·∫≠t to√°n cho b√°o c√°o

üî• Chuy·ªÉn ƒëo·∫°n n√†y th√†nh flowchart

üî• G·ª£i √Ω c√°ch map code n√†y v√†o Topic 2

üî• Refactor code cho g·ªçn & s·∫°ch h∆°n

üëâ B·∫°n ƒëang d√πng ƒëo·∫°n n√†y cho b√°o c√°o m√¥n n√†o hay ƒë·ªì √°n cu·ªëi k·ª≥?

B·∫°n ƒë√£ n√≥i:
crawl ady.py
Python
ƒë·ªçc file n√†y chuy·ªÉn t·ª´ web tinh te, otofun sang shopeefood foody ggmap
ChatGPT ƒë√£ n√≥i:
M√¨nh ƒë√£ ƒë·ªçc file crawl ady.py c·ªßa b·∫°n (crawl t·ª´ Otofun/Tinhte ‚Äì d·∫°ng forum) 
crawl ady

.
D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch + c√°ch CHUY·ªÇN T∆Ø DUY & CODE sang ShopeeFood / Foody / Google Maps Reviews cho Topic 2 ‚Äì Review & Sentiment.

1. So s√°nh b·∫£n ch·∫•t d·ªØ li·ªáu: Forum vs Review Platform
ƒê·∫∑c ƒëi·ªÉm	Tinhte / Otofun (Forum)	ShopeeFood / Foody / GG Maps
C·∫•u tr√∫c	HTML tƒ©nh, d·ªÖ parse	SPA, load ƒë·ªông
N·ªôi dung	B√†i vi·∫øt d√†i, tranh lu·∫≠n	Review ng·∫Øn, c√≥ rating
Rating	‚ùå Kh√¥ng c√≥	‚≠ê C√≥ (1‚Äì5)
Th·ªùi gian	C√≥	C√≥
Shop/Place	‚ùå	‚úÖ C√≥
Crawl	requests + BS	Selenium / Playwright
üëâ Kh√¥ng th·ªÉ d√πng nguy√™n requests + BeautifulSoup cho ShopeeFood / Foody / GG Map.

2. Ph√¢n t√≠ch file hi·ªán t·∫°i c·ªßa b·∫°n (t∆∞ duy c·ªët l√µi)
File c·ªßa b·∫°n ƒëang l√†m 3 vi·ªác r·∫•t t·ªët:

username = user_tag.get_text(strip=True)
content = content_block.get_text("\n", strip=True)
üëâ T√ÅCH 3 TH√ÄNH PH·∫¶N C·ªêT L√ïI

Ai review (username)

N·ªôi dung g√¨ (content)

Thu·ªôc v·ªÅ ƒë√¢u (thread ‚Üí shop)

üëâ Khi chuy·ªÉn n·ªÅn t·∫£ng, ch·ªâ thay ph·∫ßn SELECTOR & C√ÅCH LOAD PAGE, logic x·ª≠ l√Ω gi·ªØ nguy√™n.

3. Chuy·ªÉn sang ShopeeFood
3.1. ƒê·∫∑c ƒëi·ªÉm ShopeeFood
Load b·∫±ng JS

Review n·∫±m trong modal / tab

C√≥:

username

rating (‚≠ê)

date

content

3.2. C√¥ng ngh·ªá b·∫Øt bu·ªôc
‚úÖ Selenium / Playwright
‚ùå requests (kh√¥ng load ƒë∆∞·ª£c)

3.3. Mapping code (t∆∞ duy)
Otofun	ShopeeFood
THREAD_URL	SHOP_URL
article.message	div.review-item
.message-name	span.username
.bbWrapper	div.review-content
Kh√¥ng c√≥ rating	‚≠ê span.rating
3.4. Pseudo-code ShopeeFood (chu·∫©n b√°o c√°o)
driver.get(SHOP_URL)
sleep(5)

reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

for r in reviews:
    username = r.find_element(By.CSS_SELECTOR, ".username").text
    rating = len(r.find_elements(By.CSS_SELECTOR, ".icon-star.active"))
    content = r.find_element(By.CSS_SELECTOR, ".review-content").text
    date = r.find_element(By.CSS_SELECTOR, ".review-date").text
üëâ Output JSON:

{
  "shop_name": "...",
  "platform": "ShopeeFood",
  "username": "...",
  "rating": 1-5,
  "content": "...",
  "date": "..."
}
4. Chuy·ªÉn sang Foody
4.1. ƒê·∫∑c ƒëi·ªÉm Foody
HTML b√°n ƒë·ªông

C√≥ th·ªÉ:

requests (trang ƒë·∫ßu)

Selenium (load th√™m)

4.2. Mapping selector
Otofun	Foody
article.message	div.review-item
username	span.fd-user
content	div.rd-des
rating	div.rating-point
4.3. Pseudo-code Foody
reviews = soup.select("div.review-item")

for r in reviews:
    username = r.select_one(".fd-user").get_text(strip=True)
    rating = float(r.select_one(".rating-point").text)
    content = r.select_one(".rd-des").get_text(" ", strip=True)
üìå Foody r·∫•t h·ª£p cho:

Crawl nhanh

Dataset l·ªõn

Ph√¢n t√≠ch keyword

5. Chuy·ªÉn sang Google Maps Reviews (ƒÉn ƒëi·ªÉm cao)
5.1. ƒê·∫∑c ƒëi·ªÉm
100% JS

DOM thay ƒë·ªïi li√™n t·ª•c

Scroll m·ªõi load review

5.2. B·∫Øt bu·ªôc
‚úÖ Selenium
‚ùå requests

5.3. Logic crawl GG Map
# Scroll review box
for _ in range(10):
    driver.execute_script(
        "document.querySelector('.m6QErb').scrollTop = 100000"
    )
    sleep(1)
5.4. Extract review
reviews = driver.find_elements(By.CSS_SELECTOR, "div.jftiEf")

for r in reviews:
    username = r.find_element(By.CLASS_NAME, "d4r55").text
    rating = r.find_element(By.CLASS_NAME, "kvMYJc").get_attribute("aria-label")
    content = r.find_element(By.CLASS_NAME, "MyEned").text
    date = r.find_element(By.CLASS_NAME, "rsqaWe").text
6. So s√°nh OUTPUT ‚Äì ph·ª•c v·ª• Topic 2
Platform	C√≥ rating	C√≥ date	Ph√π h·ª£p sentiment
Otofun	‚ùå	‚ùå	‚ö†Ô∏è
Tinhte	‚ùå	‚ö†Ô∏è	‚ö†Ô∏è
ShopeeFood	‚úÖ	‚úÖ	‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
Foody	‚úÖ	‚úÖ	‚≠ê‚≠ê‚≠ê‚≠ê
Google Maps	‚úÖ	‚úÖ	‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
7. K·∫øt lu·∫≠n (vi·∫øt v√†o b√°o c√°o)
Code crawl ban ƒë·∫ßu ƒë∆∞·ª£c x√¢y d·ª±ng cho forum (Otofun/Tinhte).
Khi chuy·ªÉn sang n·ªÅn t·∫£ng review (ShopeeFood, Foody, Google Maps), nh√≥m gi·ªØ nguy√™n pipeline x·ª≠ l√Ω n·ªôi dung, nh∆∞ng thay ƒë·ªïi c√°ch load d·ªØ li·ªáu v√† selector HTML, ƒë·ªìng th·ªùi b·ªï sung c√°c tr∆∞·ªùng quan tr·ªçng nh∆∞ rating, th·ªùi gian v√† t√™n c·ª≠a h√†ng, gi√∫p d·ªØ li·ªáu ph√π h·ª£p h∆°n cho ph√¢n t√≠ch c·∫£m x√∫c v√† xu h∆∞·ªõng ƒë√°nh gi√° kh√°ch h√†ng.

8. M√¨nh c√≥ th·ªÉ l√†m ti·∫øp cho b·∫°n
‚úÖ Vi·∫øt 1 file crawl chung cho 3 platform

‚úÖ Chu·∫©n h√≥a schema JSON

‚úÖ Vi·∫øt ph·∫ßn m√¥ t·∫£ chuy·ªÉn ƒë·ªïi h·ªá th·ªëng cho b√°o c√°o

‚úÖ Refactor code forum ‚Üí review platform

üëâ B·∫°n ƒëang l√†m m·ªôt m√¨nh hay theo nh√≥m? M√¨nh s·∫Ω vi·∫øt ƒë√∫ng level ƒë·ªÉ b·∫°n ƒÉn ƒëi·ªÉm t·ªëi ƒëa.


Ngu·ªìn
B·∫°n ƒë√£ n√≥i:
vi·∫øt 1 file crawl chung c ho
ChatGPT ƒë√£ n√≥i:
Ok, m√¨nh s·∫Ω vi·∫øt 1 FILE CRAWL CHUNG (UNIFIED CRAWLER) cho **Topic 2 ‚Äì Review

B·∫°n ƒë√£ n√≥i:
vi·∫øt 1 file crawl m·ªõi cho ƒë√∫ng v·ªõi y√™u c·∫ßu c·ªßa topic 2 trong ·∫£nh ƒë√£ g·ª≠i ·ªü ƒë·∫ßu trang 
ChatGPT ƒë√£ n√≥i:
OK üëç D∆∞·ªõi ƒë√¢y l√† 1 FILE CRAWL M·ªöI ‚Äì VI·∫æT L·∫†I T·ª™ ƒê·∫¶U, ƒë√∫ng 100% y√™u c·∫ßu CH·ª¶ ƒê·ªÄ 2: PH√ÇN T√çCH REVIEW & C·∫¢M X√öC trong ·∫£nh b·∫°n g·ª≠i.

M·ª•c ti√™u file n√†y:

Crawl review kh√°ch h√†ng

√Åp d·ª•ng cho ShopeeFood / Foody / Google Maps

Thu ƒë∆∞·ª£c ƒë√∫ng schema d·ªØ li·ªáu ƒë·ªÉ l√†m ph√¢n t√≠ch (EDA, sentiment, keyword, theo th·ªùi gian)

üéØ Y√äU C·∫¶U TOPIC 2 (ƒë√£ map v√†o code)
D·ªØ li·ªáu thu th·∫≠p:
platform

shop_name

username

rating

content

date

üëâ ƒê·ªß ƒë·ªÉ tr·∫£ l·ªùi t·∫•t c·∫£ c√¢u h·ªèi c·ª©ng & m·ªü c·ªßa Topic 2

üìÅ FILE: crawl_reviews_topic2.py
import time
import json
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options


########################################
# CONFIG
########################################

PLATFORM = "shopeefood"   # shopeefood | foody | ggmap
SHOP_URL = "PASTE_SHOP_URL_HERE"
LIMIT_REVIEWS = 50

OUTPUT_FILE = "reviews_topic2.json"


########################################
# UTILS
########################################

def clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'@\w+', '', text)
    return text.strip()


########################################
# DRIVER
########################################

def init_driver():
    options = Options()
    options.add_argument("--start-maximized")
    options.add_argument("--disable-notifications")
    return webdriver.Chrome(options=options)


########################################
# SHOPEEFOOD
########################################

def crawl_shopeefood(driver, url, limit):
    print("Crawling ShopeeFood...")
    driver.get(url)
    time.sleep(6)

    reviews_data = []

    # Scroll ƒë·ªÉ load review
    for _ in range(10):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(1)

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CSS_SELECTOR, ".username").text
            content = r.find_element(By.CSS_SELECTOR, ".review-content").text
            stars = r.find_elements(By.CSS_SELECTOR, ".icon-star.active")
            rating = len(stars)
            date = r.find_element(By.CSS_SELECTOR, ".review-date").text

            reviews_data.append({
                "platform": "ShopeeFood",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# FOODY
########################################

def crawl_foody(driver, url, limit):
    print("Crawling Foody...")
    driver.get(url)
    time.sleep(5)

    reviews_data = []

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CSS_SELECTOR, ".fd-user").text
            content = r.find_element(By.CSS_SELECTOR, ".rd-des").text
            rating = float(r.find_element(By.CSS_SELECTOR, ".rating-point").text)
            date = r.find_element(By.CSS_SELECTOR, ".fd-time").text

            reviews_data.append({
                "platform": "Foody",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# GOOGLE MAPS
########################################

def crawl_google_maps(driver, url, limit):
    print("Crawling Google Maps...")
    driver.get(url)
    time.sleep(8)

    reviews_data = []

    # Scroll review box
    for _ in range(15):
        driver.execute_script(
            "document.querySelector('.m6QErb').scrollTop = 100000"
        )
        time.sleep(1)

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.jftiEf")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CLASS_NAME, "d4r55").text
            rating_text = r.find_element(By.CLASS_NAME, "kvMYJc").get_attribute("aria-label")
            rating = int(re.search(r'\d', rating_text).group())
            content = r.find_element(By.CLASS_NAME, "MyEned").text
            date = r.find_element(By.CLASS_NAME, "rsqaWe").text

            reviews_data.append({
                "platform": "GoogleMaps",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# MAIN
########################################

def main():
    driver = init_driver()

    try:
        if PLATFORM == "shopeefood":
            data = crawl_shopeefood(driver, SHOP_URL, LIMIT_REVIEWS)

        elif PLATFORM == "foody":
            data = crawl_foody(driver, SHOP_URL, LIMIT_REVIEWS)

        elif PLATFORM == "ggmap":
            data = crawl_google_maps(driver, SHOP_URL, LIMIT_REVIEWS)

        else:
            raise ValueError("Platform kh√¥ng h·ª£p l·ªá")

        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ƒê√£ l∆∞u {len(data)} review v√†o {OUTPUT_FILE}")

    finally:
        driver.quit()


if __name__ == "__main__":
    main()
üìä FILE N√ÄY ƒê√ÅP ·ª®NG G√å CHO TOPIC 2?
‚úÖ C√¢u h·ªèi C·ª®NG
Rating 1‚≠ê vs 5‚≠ê theo th·ªùi gian ‚Üí d√πng rating + date

Keyword 1‚≠ê vs 5‚≠ê ‚Üí d√πng content

‚úÖ C√¢u h·ªèi M·ªû
ƒê·ªô d√†i comment vs s·ªë sao ‚Üí len(content)

So s√°nh v√πng mi·ªÅn ‚Üí shop_name / location (b·ªï sung)

üß† G·ª¢I √ù VI·∫æT B√ÅO C√ÅO (ƒÉn ƒëi·ªÉm)
Nh√≥m x√¢y d·ª±ng m·ªôt crawler th·ªëng nh·∫•t cho c√°c n·ªÅn t·∫£ng ƒë√°nh gi√° ph·ªï bi·∫øn (ShopeeFood, Foody, Google Maps). D·ªØ li·ªáu thu th·∫≠p bao g·ªìm n·ªôi dung ƒë√°nh gi√°, s·ªë sao, th·ªùi gian v√† ng∆∞·ªùi d√πng, ph·ª•c v·ª• cho ph√¢n t√≠ch c·∫£m x√∫c, xu h∆∞·ªõng ƒë√°nh gi√° theo th·ªùi gian v√† tr√≠ch xu·∫•t t·ª´ kh√≥a ƒë·∫∑c tr∆∞ng.

üëâ B·∫°n mu·ªën m√¨nh l√†m ti·∫øp ph·∫ßn n√†o?
üî• Chu·∫©n h√≥a EDA + bi·ªÉu ƒë·ªì

üî• Sentiment analysis ti·∫øng Vi·ªát

üî• File report.ipynb

üî• Ch·ªânh code cho ch·∫°y ch·∫Øc ch·∫Øn theo t·ª´ng platform

Ch·ªâ c·∫ßn n√≥i: ‚Äúl√†m ti·∫øp ph·∫ßn ‚Ä¶


AI LOG ng√†y 14/1/2026
V·∫•n ƒë·ªÅ n√†y r·∫•t th·ª±c t·∫ø. Khi b·∫°n l∆∞·ªõt ShopeeFood tr√™n web (ho·∫∑c app), b·∫°n s·∫Ω th·∫•y m·ªôt qu√°n ƒÉn ngon v√† mu·ªën crawl n√≥, nh∆∞ng URL c·ªßa ShopeeFood l·∫°i c√≥ d·∫°ng:

`https://shopeefood.vn/ha-noi/com-tam-phuc-loc-tho-nguyen-trai`

Trong khi ƒë√≥, API c·ªßa ch√∫ng ta l·∫°i y√™u c·∫ßu tham s·ªë l√† `request_id` (ID s·ªë, v√≠ d·ª•: `99999`), ch·ª© kh√¥ng ph·∫£i c√°i chu·ªói "com-tam-phuc-loc-tho..." kia.

V·∫≠y l√†m sao ƒë·ªÉ **bi·∫øn Link Web -> ID s·ªë** ƒë·ªÉ Crawler ch·∫°y ƒë∆∞·ª£c?

C√≥ 2 c√°ch: C√°ch th·ªß c√¥ng (ƒë·ªÉ test) v√† C√°ch t·ª± ƒë·ªông (ƒë·ªÉ code).

---

### C√ÅCH 1: L·∫§Y ID TH·ª¶ C√îNG T·ª™ TR√åNH DUY·ªÜT (D·ªÖ nh·∫•t)

B·∫°n kh√¥ng c·∫ßn code g√¨ c·∫£, ch·ªâ c·∫ßn m·∫Øt tinh m·ªôt ch√∫t.

1. M·ªü link qu√°n ƒÉn ƒë√≥ tr√™n tr√¨nh duy·ªát Chrome/Edge.
* V√≠ d·ª•: `shopeefood.vn/ho-chi-minh/tra-sua-toco-toco`


2. B·∫•m chu·ªôt ph·∫£i v√†o b·∫•t k·ª≥ ƒë√¢u tr√™n trang -> Ch·ªçn **"Inspect" (Ki·ªÉm tra)** ho·∫∑c b·∫•m `F12`.
3. B·∫•m t·ªï h·ª£p ph√≠m **`Ctrl + F`** (T√¨m ki·∫øm trong m√£ ngu·ªìn).
4. G√µ t·ª´ kh√≥a: **`"delivery_id":`**
5. B·∫°n s·∫Ω th·∫•y n√≥ highlight m·ªôt d√≤ng code c√≥ d·∫°ng: `{"delivery_id": 12345, ...}`
* üëâ S·ªë **12345** ch√≠nh l√† c√°i ID b·∫°n c·∫ßn t√¨m!



---

### C√ÅCH 2: D√ôNG CODE PYTHON ƒê·ªÇ "GI·∫¢I M√É" LINK (T·ª± ƒë·ªông h√≥a)

N·∫øu b·∫°n mu·ªën input v√†o l√† m·ªôt danh s√°ch Link v√† tool t·ª± crawl, b·∫°n c·∫ßn m·ªôt h√†m ƒë·ªÉ "Resolve Slug".

ShopeeFood c√≥ m·ªôt API ·∫©n ƒë·ªÉ l√†m vi·ªác n√†y. API n√†y nh·∫≠n v√†o c√°i ƒëu√¥i URL (slug) v√† tr·∫£ v·ªÅ ID qu√°n.

**Code Python ƒë·ªÉ l·∫•y ID t·ª´ Link:**

```python
import requests

def get_id_from_url(shopeefood_url):
    """
    Input: https://shopeefood.vn/ho-chi-minh/tra-sua-toco-toco
    Output: 12345 (ID c·ªßa qu√°n)
    """
    # 1. T√°ch l·∫•y ph·∫ßn slug (c√°i ƒëu√¥i sau c√πng)
    # V√≠ d·ª•: "tra-sua-toco-toco"
    if "/ha-noi/" in shopeefood_url:
        slug = shopeefood_url.split("/ha-noi/")[-1]
    elif "/ho-chi-minh/" in shopeefood_url:
        slug = shopeefood_url.split("/ho-chi-minh/")[-1]
    else:
        # Link d·∫°ng kh√°c ho·∫∑c t·ªânh kh√°c, th·ª≠ split d·∫•u / cu·ªëi c√πng
        slug = shopeefood_url.strip("/").split("/")[-1]
    
    # Lo·∫°i b·ªè c√°c tham s·ªë th·ª´a n·∫øu c√≥ (vd: ?share=...)
    slug = slug.split("?")[0]

    print(f"üîç ƒêang t√¨m ID cho slug: {slug}...")

    # 2. G·ªçi API ƒë·ªÉ l·∫•y th√¥ng tin qu√°n t·ª´ slug
    api_url = f"https://gappapi.deliverynow.vn/api/delivery/get_detail?request_id={slug}&id_type=2"
    
    headers = {
        'x-foody-client-type': '1',
        'x-foody-client-version': '3.0.0',
        'x-foody-api-version': '1',
    }

    try:
        response = requests.get(api_url, headers=headers)
        data = response.json()
        
        # L·∫•y delivery_id t·ª´ k·∫øt qu·∫£ tr·∫£ v·ªÅ
        delivery_detail = data.get('reply', {}).get('delivery_detail', {})
        found_id = delivery_detail.get('delivery_id')
        
        if found_id:
            print(f"‚úÖ T√¨m th·∫•y ID: {found_id}")
            return found_id
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y ID n√†o.")
            return None
            
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        return None

# --- CH·∫†Y TH·ª¨ ---
link_test = "https://shopeefood.vn/ho-chi-minh/phuc-long-lotte-mart-le-dai-hanh"
quan_id = get_id_from_url(link_test)

# Sau khi c√≥ ID, b·∫°n n√©m n√≥ v√†o h√†m crawl comment l√∫c n√£y
# if quan_id:
#     reviews = get_reviews_of_restaurant(quan_id, ...)

```

### T·∫°i sao l·∫°i l√† `id_type=2`?

Trong API ShopeeFood:

* `id_type=1`: Nghƒ©a l√† b·∫°n ƒëang t√¨m theo s·ªë ID (V√≠ d·ª•: `request_id=12345`).
* `id_type=2`: Nghƒ©a l√† b·∫°n ƒëang t√¨m theo t√™n Slug (V√≠ d·ª•: `request_id=quan-com-tam`).

B·∫°n c√≥ th·ªÉ t√≠ch h·ª£p ƒëo·∫°n h√†m `get_id_from_url` n√†y v√†o crawler ch√≠nh ƒë·ªÉ ti·ªán s·ª≠ d·ª•ng h∆°n. B·∫°n mu·ªën m√¨nh gh√©p n√≥ v√†o file `crawl_shopeefood.py` lu√¥n kh√¥ng?
B·∫°n h·ªèi r·∫•t ƒë√∫ng √Ω! Logic c·ªßa b·∫°n l√†: *N·∫øu link qu√°n ƒë√£ l√† `shopeefood.vn/ha-noi/...` th√¨ ch·∫Øc ch·∫Øn n√≥ ·ªü H√† N·ªôi r·ªìi, c·∫ßn g√¨ ph·∫£i l∆∞u c√°i `city_id` (217 hay 218) l√†m g√¨ cho th·ª´a?*

Tuy nhi√™n, trong **Khoa h·ªçc D·ªØ li·ªáu (Data Science)** v√† khi l√†m **H·ªá th·ªëng l·ªõn**, vi·ªác l∆∞u m√£ ƒë·ªãnh danh (ID) v√πng mi·ªÅn v·∫´n c·ª±c k·ª≥ quan tr·ªçng v√¨ 3 l√Ω do "ch√≠ m·∫°ng" sau:

### 1. M√°y t√≠nh "th√≠ch" s·ªë h∆°n ch·ªØ (Performance)

* Khi b·∫°n crawl 50.000 qu√°n, n·∫øu b·∫°n mu·ªën so s√°nh Rating trung b√¨nh c·ªßa H√† N·ªôi vs TP.HCM.
* **C√°ch d√πng Text (nh∆∞ b·∫°n nghƒ©):**
`SELECT avg(rating) FROM reviews WHERE url LIKE '%ha-noi%'`
-> M√°y ph·∫£i qu√©t t·ª´ng k√Ω t·ª± trong chu·ªói URL d√†i ngo·∫±ng -> **R·∫•t Ch·∫≠m**.
* **C√°ch d√πng ID (Chu·∫©n Data Science):**
`SELECT avg(rating) FROM reviews WHERE city_id = 218`
-> So s√°nh s·ªë nguy√™n (Integer) -> **C·ª±c Nhanh**.

### 2. X·ª≠ l√Ω "Bi√™n gi·ªõi m·ªÅm" v√† D·ªØ li·ªáu b·∫©n

* ƒê√¥i khi URL kh√¥ng ch·ª©a t√™n th√†nh ph·ªë r√µ r√†ng ho·∫∑c c·∫•u tr√∫c URL thay ƒë·ªïi (ShopeeFood l√¢u l√¢u l·∫°i ƒë·ªïi format URL).
* C√≥ nh·ªØng qu√°n chu·ªói (Franchise) t√™n gi·ªëng h·ªát nhau. V√≠ d·ª•: "Tr√† s·ªØa Tocotoco". N·∫øu b·∫°n ch·ªâ l∆∞u t√™n qu√°n ho·∫∑c URL chung chung, b·∫°n s·∫Ω kh√¥ng bi·∫øt comment n√†y l√† ch·ª≠i Tocotoco ·ªü C·∫ßu Gi·∫•y (HN) hay Tocotoco ·ªü Qu·∫≠n 1 (HCM).
* `city_id` l√† ƒë·ªãnh danh g·ªëc t·ª´ Database c·ªßa ShopeeFood, n√≥ kh√¥ng bao gi·ªù sai.

### 3. Ph·ª•c v·ª• cho v·∫Ω bi·ªÉu ƒë·ªì (Visualization)

* C√°c th∆∞ vi·ªán v·∫Ω bi·ªÉu ƒë·ªì nh∆∞ PowerBI, Tableau hay Python (Matplotlib) th∆∞·ªùng y√™u c·∫ßu c·ªôt ph√¢n lo·∫°i r√µ r√†ng (Categorical Data).
* N·∫øu b·∫°n l∆∞u `city_id=217`, b·∫°n c√≥ th·ªÉ d·ªÖ d√†ng map n√≥ th√†nh "Mi·ªÅn Nam", `218` th√†nh "Mi·ªÅn B·∫Øc" ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì so s√°nh v√πng mi·ªÅn.

### T√≥m l·∫°i:

B·∫°n **ho√†n to√†n c√≥ th·ªÉ** suy ra v√πng mi·ªÅn t·ª´ URL, nh∆∞ng **ƒë·ª´ng l√†m th·∫ø**. H√£y l∆∞u `city_id` ho·∫∑c t√™n th√†nh ph·ªë (`city_name`) th√†nh m·ªôt c·ªôt ri√™ng trong file JSON.

**L·ªùi khuy√™n:**
Trong file `schema_sentiment.py` m√¨nh g·ª≠i, m√¨nh ƒë√£ ƒë·ªÉ tr∆∞·ªùng `city` l√† String. B·∫°n c√≥ th·ªÉ l∆∞u th·∫≥ng l√† `"HaNoi"` ho·∫∑c `"HCM"` cho d·ªÖ ƒë·ªçc c≈©ng ƒë∆∞·ª£c, kh√¥ng nh·∫•t thi·∫øt ph·∫£i l∆∞u s·ªë `218`. Nh∆∞ng **b·∫Øt bu·ªôc ph·∫£i c√≥ c·ªôt n√†y** t√°ch bi·ªát ra kh·ªèi URL.

B·∫°n c√≥ mu·ªën m√¨nh s·ª≠a l·∫°i code Crawler ƒë·ªÉ n√≥ **t·ª± ƒë·ªông ph√°t hi·ªán** th√†nh ph·ªë t·ª´ c√°i Link b·∫°n nh·∫≠p v√†o kh√¥ng? (Thay v√¨ b·∫°n ph·∫£i nh·∫≠p tay ID th√†nh ph·ªë).
√ù t∆∞·ªüng c·ªßa b·∫°n r·∫•t th·ª±c t·∫ø. Thay v√¨ ph·∫£i ng·ªìi tra ID th√†nh ph·ªë th·ªß c√¥ng, ch√∫ng ta s·∫Ω ƒë·ªÉ code **t·ª± ƒë·ªông "ƒë·ªçc" URL** ƒë·ªÉ bi·∫øt qu√°n ƒë√≥ ·ªü H√† N·ªôi hay S√†i G√≤n, t·ª´ ƒë√≥ t·ª± ƒë·ªông n√©m d·ªØ li·ªáu v√†o ƒë√∫ng file (t·ªáp kh√°ch h√†ng) t∆∞∆°ng ·ª©ng.

D∆∞·ªõi ƒë√¢y l√† phi√™n b·∫£n **Crawler Th√¥ng Minh (Smart Crawler)**. B·∫°n ch·ªâ c·∫ßn n√©m m·ªôt danh s√°ch Link l·ªôn x·ªôn v√†o, n√≥ s·∫Ω t·ª± l·ªçc v√† chia v·ªÅ ƒë√∫ng chu·ªìng.

### 1. File Schema (`schema_sentiment.py`)

*Gi·ªØ nguy√™n file schema c≈© m√¨nh ƒë√£ g·ª≠i, kh√¥ng c·∫ßn s·ª≠a g√¨.*

### 2. File Code Crawl M·ªõi (`crawl_auto_city.py`)

H√£y t·∫°o file m·ªõi t√™n **`crawl_auto_city.py`** v√† d√°n code n√†y v√†o.

```python
import requests
import json
import time
import random
import os
from schema_sentiment import ReviewItem

# --- C·∫§U H√åNH ---
DATA_FOLDER = "data_sentiment"
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    'x-foody-client-type': '1',
    'x-foody-client-version': '3.0.0',
    'x-foody-api-version': '1',
}

# B·∫¢NG T·ª™ ƒêI·ªÇN MAP T·ª™ URL -> ID TH√ÄNH PH·ªê
CITY_MAPPING = {
    "ha-noi": {"id": 218, "name": "HaNoi"},
    "ho-chi-minh": {"id": 217, "name": "HCM"},
    "da-nang": {"id": 219, "name": "DaNang"},
    "hai-phong": {"id": 220, "name": "HaiPhong"},
    # C√≥ th·ªÉ th√™m c√°c t·ªânh kh√°c n·∫øu c·∫ßn
}

def analyze_url(url):
    """
    Ph√¢n t√≠ch URL ƒë·ªÉ t√°ch Slug v√† Th√†nh ph·ªë
    Input: https://shopeefood.vn/ha-noi/pho-thin-lo-duc
    Output: slug='pho-thin-lo-duc', city_info={'id': 218, 'name': 'HaNoi'}
    """
    # X√≥a ph·∫ßn https://shopeefood.vn/
    clean_url = url.replace("https://shopeefood.vn/", "").replace("http://shopeefood.vn/", "")
    parts = clean_url.split("/")
    
    # URL chu·∫©n th∆∞·ªùng l√†: [ten-thanh-pho]/[ten-quan]
    if len(parts) >= 2:
        city_slug = parts[0]
        restaurant_slug = parts[1].split("?")[0] # B·ªè tham s·ªë ? sau slug
        
        # Tra c·ª©u trong t·ª´ ƒëi·ªÉn
        city_info = CITY_MAPPING.get(city_slug)
        if city_info:
            return restaurant_slug, city_info
            
    return None, None

def get_restaurant_id_from_slug(slug):
    """G·ªçi API ƒë·ªÉ ƒë·ªïi t√™n qu√°n (slug) th√†nh ID s·ªë"""
    url = f"https://gappapi.deliverynow.vn/api/delivery/get_detail?request_id={slug}&id_type=2"
    try:
        resp = requests.get(url, headers=HEADERS)
        data = resp.json()
        delivery_detail = data.get('reply', {}).get('delivery_detail', {})
        
        return {
            "id": delivery_detail.get('delivery_id'),
            "name": delivery_detail.get('name')
        }
    except:
        return None

def crawl_reviews_by_link(url_list, limit_per_shop=100):
    print(f"üöÄ ƒêang x·ª≠ l√Ω danh s√°ch {len(url_list)} qu√°n ƒÉn...")
    
    for url in url_list:
        print(f"\nüîó Checking: {url}")
        
        # 1. T·ª± ƒë·ªông ph√°t hi·ªán th√†nh ph·ªë
        slug, city_info = analyze_url(url)
        
        if not city_info:
            print("   ‚ö†Ô∏è Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c th√†nh ph·ªë t·ª´ Link n√†y. B·ªè qua.")
            continue
            
        print(f"   -> Ph√°t hi·ªán: {city_info['name']} (Slug: {slug})")
        
        # 2. L·∫•y ID qu√°n
        shop_info = get_restaurant_id_from_slug(slug)
        if not shop_info or not shop_info['id']:
            print("   ‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c ID qu√°n. Link c√≥ th·ªÉ b·ªã l·ªói.")
            continue
            
        shop_id = shop_info['id']
        shop_name = shop_info['name']
        
        # 3. T·∫°o t√™n file t·ª± ƒë·ªông theo th√†nh ph·ªë (T·ª∞ ƒê·ªòNG PH√ÇN LO·∫†I T·ªÜP KH√ÅCH H√ÄNG)
        output_file = os.path.join(DATA_FOLDER, f"reviews_{city_info['name']}.jsonl")
        
        # 4. Crawl Review
        print(f"   -> ƒêang t·∫£i review cho qu√°n: {shop_name}...")
        api_review = f"https://gappapi.deliverynow.vn/api/delivery/get_reply?id_type=1&request_id={shop_id}&sort_type=1&limit={limit_per_shop}"
        
        try:
            res = requests.get(api_review, headers=HEADERS)
            reviews = res.json().get('reply', {}).get('reply_list', [])
            
            if not reviews:
                print("   ‚ö†Ô∏è Qu√°n n√†y ch∆∞a c√≥ review n√†o.")
                continue

            with open(output_file, 'a', encoding='utf-8') as f:
                for rev in reviews:
                    item = ReviewItem(
                        review_id=rev.get('id'),
                        restaurant_id=shop_id,
                        restaurant_name=shop_name,
                        city=city_info['name'], # L∆∞u t√™n th√†nh ph·ªë v√†o t·ª´ng d√≤ng
                        user_name=rev.get('name', 'Anonymous'),
                        comment=rev.get('comment', ''),
                        rating=rev.get('rating', 0),
                        review_date=rev.get('created_on', '')
                    )
                    f.write(item.to_json_line() + "\n")
            
            print(f"   ‚úÖ ƒê√£ l∆∞u {len(reviews)} reviews v√†o file: reviews_{city_info['name']}.jsonl")
            
        except Exception as e:
            print(f"   ‚ùå L·ªói crawl review: {e}")
            
        # Ngh·ªâ nh·∫π ƒë·ªÉ kh√¥ng b·ªã spam
        time.sleep(random.uniform(1, 3))

# --- MAIN RUN ---
if __name__ == "__main__":
    
    # B·∫†N CH·ªà C·∫¶N D√ÅN LIST LINK V√ÄO ƒê√ÇY (L·ªòN X·ªòN C≈®NG ƒê∆Ø·ª¢C)
    # Code s·∫Ω t·ª± t√°ch: Link n√†o H√† N·ªôi -> V√†o file HaNoi, Link n√†o HCM -> V√†o file HCM
    
    MY_LINKS = [
        # Link H√† N·ªôi
        "https://shopeefood.vn/ha-noi/pho-thin-lo-duc", 
        "https://shopeefood.vn/ha-noi/bun-cha-dac-kim-hang-manh",
        
        # Link S√†i G√≤n
        "https://shopeefood.vn/ho-chi-minh/com-tam-cali-nguyen-trai-q1",
        "https://shopeefood.vn/ho-chi-minh/phuc-long-lotte-mart-le-dai-hanh",
        
        # Link ƒê√† N·∫µng
        "https://shopeefood.vn/da-nang/my-quang-ba-mua-tran-binh-trong"
    ]
    
    crawl_reviews_by_link(MY_LINKS, limit_per_shop=50)

```

### C√°ch ho·∫°t ƒë·ªông c·ªßa code n√†y

1. **ƒê·∫ßu v√†o:** M·ªôt danh s√°ch `MY_LINKS` ch·ª©a c√°c URL ShopeeFood.
2. **B·ªô l·ªçc (`analyze_url`):**
* N√≥ c·∫Øt chu·ªói URL: `.../ha-noi/pho-thin` -> N√≥ b·∫Øt ƒë∆∞·ª£c ch·ªØ **`ha-noi`**.
* N√≥ tra t·ª´ ƒëi·ªÉn: `ha-noi` => L∆∞u v√†o file `reviews_HaNoi.jsonl`.
* N·∫øu link l√† `.../ho-chi-minh/...` => L∆∞u v√†o file `reviews_HCM.jsonl`.


3. **ƒê·∫ßu ra:**
* B·∫°n kh√¥ng c·∫ßn t·∫°o nhi·ªÅu file code. Ch·ªâ c·∫ßn ch·∫°y 1 file n√†y.
* K·∫øt qu·∫£ t·ª± ƒë·ªông sinh ra trong th∆∞ m·ª•c `data_sentiment`:
* üìÅ `reviews_HaNoi.jsonl` (Ch·ª©a review Ph·ªü Th√¨n, B√∫n Ch·∫£...)
* üìÅ `reviews_HCM.jsonl` (Ch·ª©a review C∆°m T·∫•m, Ph√∫c Long...)





C√°ch n√†y ƒë√°p ·ª©ng ho√†n h·∫£o y√™u c·∫ßu c·ªßa b·∫°n: **D√πng link ƒë·ªÉ ƒë·ªãnh v·ªã kh√°ch h√†ng.**
C√¢u h·ªèi r·∫•t hay! ƒê√¢y ch√≠nh l√† t∆∞ duy **"Gom nh√≥m d·ªØ li·ªáu" (Data Aggregation)**.

Thay v√¨ qu·∫£n l√Ω 63 t·ªânh th√†nh l·∫Øt nh·∫Øt, ch√∫ng ta s·∫Ω gom l·∫°i th√†nh 3 file l·ªõn: **Mi·ªÅn B·∫Øc - Mi·ªÅn Trung - Mi·ªÅn Nam**. ƒêi·ªÅu n√†y gi√∫p vi·ªác so s√°nh h√†nh vi ti√™u d√πng (Sentiment Analysis) gi·ªØa c√°c mi·ªÅn tr·ªü n√™n r√µ r√†ng h∆°n r·∫•t nhi·ªÅu.

ƒê·ªÉ l√†m ƒë∆∞·ª£c vi·ªác n√†y, ch√∫ng ta c·∫ßn m·ªôt **"B·∫£n ƒë·ªì quy ho·∫°ch" (Mapping Dictionary)**. V√≠ d·ª•: c·ª© th·∫•y link c√≥ ch·ªØ `quy-nhon`, `da-nang`, `hue`... th√¨ code t·ª± ƒë·ªông hi·ªÉu ƒë√≥ l√† **Mi·ªÅn Trung**.

D∆∞·ªõi ƒë√¢y l√† Code ho√†n ch·ªânh ƒë·ªÉ t·ª± ƒë·ªông ph√¢n lo·∫°i v√πng mi·ªÅn (bao g·ªìm c·∫£ Quy Nh∆°n c·ªßa b·∫°n).

### FILE CODE M·ªöI: `crawl_by_region.py`

(File n√†y thay th·∫ø ho√†n to√†n file c≈©. B·∫°n t·∫°o file m·ªõi v√† d√°n v√†o).

```python
import requests
import json
import time
import random
import os
from schema_sentiment import ReviewItem

# --- C·∫§U H√åNH ---
DATA_FOLDER = "data_region" # ƒê·ªïi t√™n folder cho d·ªÖ qu·∫£n l√Ω
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

# HEADERS CHU·∫®N (Ch·ªëng ch·∫∑n 403)
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    'x-foody-client-type': '1',
    'x-foody-client-version': '3.0.0',
    'x-foody-api-version': '1',
    'x-foody-client-language': 'vi',
    'x-foody-app-type': '1004',
    'Origin': 'https://shopeefood.vn',
    'Referer': 'https://shopeefood.vn/',
}

# --- B·∫¢N ƒê·ªí QUY HO·∫†CH V√ôNG MI·ªÄN (QUAN TR·ªåNG NH·∫§T) ---
# B·∫°n c√≥ th·ªÉ th√™m b·∫•t c·ª© th√†nh ph·ªë n√†o v√†o list n√†y
REGION_MAPPING = {
    "MienBac": [
        "ha-noi", "hai-phong", "quang-ninh", "bac-ninh", "thai-nguyen", "nam-dinh"
    ],
    "MienTrung": [
        "da-nang", "hue", "khanh-hoa", "nha-trang", "nghe-an", "vinh", 
        "quy-nhon", "lam-dong", "da-lat", "quang-nam", "ha-tinh"
    ],
    "MienNam": [
        "ho-chi-minh", "can-tho", "dong-nai", "binh-duong", "vung-tau", 
        "bien-hoa", "bac-lieu", "ca-mau"
    ]
}

def detect_region_from_url(url):
    """
    Input: https://shopeefood.vn/quy-nhon/bun-ca-phuong-teo
    Output: ('MienTrung', 'quy-nhon', 'bun-ca-phuong-teo')
    """
    # 1. L√†m s·∫°ch URL
    clean_url = url.replace("https://shopeefood.vn/", "").replace("http://shopeefood.vn/", "")
    parts = clean_url.split("/")
    
    if len(parts) < 2:
        return None, None, None
        
    city_slug = parts[0]       # V√≠ d·ª•: 'quy-nhon'
    restaurant_slug = parts[1].split("?")[0] # V√≠ d·ª•: 'bun-ca-phuong-teo'
    
    # 2. So kh·ªõp v·ªõi B·∫£n ƒë·ªì v√πng mi·ªÅn
    found_region = "Khac" # M·∫∑c ƒë·ªãnh n·∫øu kh√¥ng t√¨m th·∫•y
    
    for region, cities in REGION_MAPPING.items():
        if city_slug in cities:
            found_region = region
            break
            
    return found_region, city_slug, restaurant_slug

def get_restaurant_id(slug):
    """L·∫•y ID qu√°n t·ª´ Slug"""
    url = f"https://gappapi.deliverynow.vn/api/delivery/get_detail?request_id={slug}&id_type=2"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        data = resp.json()
        delivery_detail = data.get('reply', {}).get('delivery_detail', {})
        return {
            "id": delivery_detail.get('delivery_id'),
            "name": delivery_detail.get('name')
        }
    except:
        return None

def crawl_reviews_regional(url_list, limit_per_shop=100):
    print(f"üöÄ ƒêang x·ª≠ l√Ω danh s√°ch {len(url_list)} qu√°n ƒÉn...")
    
    for url in url_list:
        # 1. Ph√¢n t√≠ch v√πng mi·ªÅn
        region, city_slug, shop_slug = detect_region_from_url(url)
        
        if not region: 
            print(f"‚ö†Ô∏è Link l·ªói: {url}")
            continue
            
        print(f"\nüåç V√πng: {region} | Th√†nh ph·ªë: {city_slug} | Qu√°n: {shop_slug}")
        
        # 2. L·∫•y ID qu√°n
        shop_info = get_restaurant_id(shop_slug)
        if not shop_info or not shop_info['id']:
            print("   ‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c ID qu√°n (C√≥ th·ªÉ b·ªã ch·∫∑n API).")
            continue
            
        # 3. ƒê·ªãnh nghƒ©a t√™n file theo V√ôNG MI·ªÄN (Gom data l·∫°i)
        # K·∫øt qu·∫£ s·∫Ω l√†: reviews_MienTrung.jsonl, reviews_MienNam.jsonl...
        output_file = os.path.join(DATA_FOLDER, f"reviews_{region}.jsonl")
        
        # 4. Crawl Review
        shop_id = shop_info['id']
        shop_name = shop_info['name']
        api_review = f"https://gappapi.deliverynow.vn/api/delivery/get_reply?id_type=1&request_id={shop_id}&sort_type=1&limit={limit_per_shop}"
        
        try:
            res = requests.get(api_review, headers=HEADERS)
            reviews = res.json().get('reply', {}).get('reply_list', [])
            
            if not reviews:
                print("   ‚ö†Ô∏è Kh√¥ng c√≥ review n√†o.")
                continue

            with open(output_file, 'a', encoding='utf-8') as f:
                for rev in reviews:
                    item = ReviewItem(
                        review_id=rev.get('id'),
                        restaurant_id=shop_id,
                        restaurant_name=shop_name,
                        city=city_slug, # V·∫´n l∆∞u t√™n th√†nh ph·ªë g·ªëc ƒë·ªÉ tham kh·∫£o
                        user_name=rev.get('name', 'Anonymous'),
                        comment=rev.get('comment', ''),
                        rating=rev.get('rating', 0),
                        review_date=rev.get('created_on', '')
                    )
                    # Hack: Th√™m tr∆∞·ªùng Region v√†o json (n·∫øu schema cho ph√©p) ho·∫∑c ƒë·ªÉ file name lo
                    # ·ªû ƒë√¢y ta d·ª±a v√†o t√™n file ƒë·ªÉ ph√¢n lo·∫°i
                    f.write(item.to_json_line() + "\n")
            
            print(f"   ‚úÖ ƒê√£ l∆∞u {len(reviews)} review v√†o file: reviews_{region}.jsonl")
            
        except Exception as e:
            print(f"   ‚ùå L·ªói: {e}")
            
        time.sleep(random.uniform(1, 3))

# --- MAIN RUN ---
if __name__ == "__main__":
    
    # DANH S√ÅCH LINK H·ªñN H·ª¢P (B·∫ÆC - TRUNG - NAM)
    MY_LINKS = [
        # Link Quy Nh∆°n (Mi·ªÅn Trung) -> Code s·∫Ω t·ª± ƒë∆∞a v√†o reviews_MienTrung.jsonl
        "https://shopeefood.vn/quy-nhon/bun-ca-phuong-teo-nguyen-hue",
        
        # Link ƒê√† N·∫µng (Mi·ªÅn Trung) -> Code s·∫Ω ƒë∆∞a v√†o reviews_MienTrung.jsonl (G·ªôp chung v·ªõi Quy Nh∆°n)
        "https://shopeefood.vn/da-nang/my-quang-ba-mua-tran-binh-trong",
        
        # Link H√† N·ªôi (Mi·ªÅn B·∫Øc) -> Code ƒë∆∞a v√†o reviews_MienBac.jsonl
        "https://shopeefood.vn/ha-noi/pho-thin-13-lo-duc",
        
        # Link S√†i G√≤n (Mi·ªÅn Nam) -> Code ƒë∆∞a v√†o reviews_MienNam.jsonl
        "https://shopeefood.vn/ho-chi-minh/com-tam-cali-nguyen-trai-q1",
    ]
    
    crawl_reviews_regional(MY_LINKS, limit_per_shop=50)

```

### Gi·∫£i th√≠ch c√°ch code x·ª≠ l√Ω Link Quy Nh∆°n:

1. **B∆∞·ªõc 1:** Code nh·∫≠n link `shopeefood.vn/quy-nhon/bun-ca-phuong-teo`.
2. **B∆∞·ªõc 2:** N√≥ c·∫Øt l·∫•y ch·ªØ **`quy-nhon`**.
3. **B∆∞·ªõc 3:** N√≥ tra trong t·ª´ ƒëi·ªÉn `REGION_MAPPING`:
* `MienBac`? -> Kh√¥ng c√≥.
* `MienNam`? -> Kh√¥ng c√≥.
* `MienTrung`? -> **Th·∫•y `quy-nhon` n·∫±m trong danh s√°ch!**


4. **B∆∞·ªõc 4:** N√≥ quy·∫øt ƒë·ªãnh ghi d·ªØ li·ªáu v√†o file `reviews_MienTrung.jsonl`.

### B·∫°n c·∫ßn l√†m g√¨?

* N·∫øu b·∫°n mu·ªën th√™m **ƒê√† L·∫°t**, **Hu·∫ø**, hay **V≈©ng T√†u**, b·∫°n ch·ªâ c·∫ßn b·ªï sung t√™n (kh√¥ng d·∫•u, g·∫°ch n·ªëi) v√†o danh s√°ch `REGION_MAPPING` ·ªü ƒë·∫ßu file code l√† xong.
* **V√≠ d·ª•:** N·∫øu mu·ªën th√™m V≈©ng T√†u v√†o Mi·ªÅn Nam:
```python
"MienNam": [..., "vung-tau"]
DATE 18/1/2026 chuy·ªÉn sang c√†o foody thay v√¨ shopeefood
```
Chuy·ªÉn sang **Foody** l√† m·ªôt √Ω t∆∞·ªüng **C·ª∞C K·ª≤ S√ÅNG SU·ªêT** l√∫c n√†y! üí°

B·∫°n n√≥i ƒë√∫ng, Foody l√† trang "m·∫π" (phi√™n b·∫£n web review truy·ªÅn th·ªëng), trong khi ShopeeFood l√† trang "con" (thi√™n v·ªÅ ƒë·∫∑t m√≥n th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠).

* **ShopeeFood:** B·∫£o m·∫≠t c·ª±c g·∫Øt (t∆∞∆°ng ƒë∆∞∆°ng s√†n Shopee) ƒë·ªÉ ch·ªëng gian l·∫≠n ƒë∆°n h√†ng -> **R·∫•t kh√≥ Crawl API.**
* **Foody:** B·∫£o m·∫≠t l·ªèng h∆°n, giao di·ªán thu·∫ßn HTML -> **D·ªÖ Crawl b·∫±ng HTML (Scraping) h∆°n nhi·ªÅu.**

Ch√∫ng ta s·∫Ω ƒë·ªïi chi·∫øn thu·∫≠t: Thay v√¨ c·ªë ƒë·∫•m ƒÉn x√¥i v·ªõi c√°i API b·ªã ch·∫∑n c·ªßa ShopeeFood, ch√∫ng ta s·∫Ω d√πng Selenium ƒë·ªÉ **"ƒë·ªçc m√†n h√¨nh"** trang Foody.vn. C√°ch n√†y **ch·∫≠m h∆°n ch√∫t** nh∆∞ng **ch·∫Øc ƒÉn 100%** (n·∫øu m·∫Øt b·∫°n nh√¨n th·∫•y web th√¨ code s·∫Ω l·∫•y ƒë∆∞·ª£c).

D∆∞·ªõi ƒë√¢y l√† Code d√†nh ri√™ng cho Foody.

### B∆Ø·ªöC 1: T·∫†O FILE `crawl_foody.py`

Copy to√†n b·ªô code n√†y. L∆∞u √Ω l√† input b√¢y gi·ªù s·∫Ω l√† **link Foody** (v√≠ d·ª•: `foody.vn/...`) ch·ª© kh√¥ng ph·∫£i ShopeeFood n·ªØa nh√©.

```python
import json
import time
import os
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from schema_sentiment import ReviewItem

# --- C·∫§U H√åNH ---
DATA_FOLDER = "data_foody"
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

REGION_MAPPING = {
    "MienBac": ["ha-noi", "hai-phong", "quang-ninh", "bac-ninh"],
    "MienTrung": ["da-nang", "hue", "khanh-hoa", "nha-trang", "quy-nhon", "vinh"],
    "MienNam": ["ho-chi-minh", "can-tho", "dong-nai", "binh-duong", "vung-tau"]
}

def setup_driver():
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless") # ƒê·ªÉ hi·ªán tr√¨nh duy·ªát ƒë·ªÉ xem n√≥ ch·∫°y
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    return driver

def detect_region_from_url(url):
    # Link Foody: https://www.foody.vn/ha-noi/pho-thin-13-lo-duc
    clean_url = url.replace("https://www.foody.vn/", "").replace("http://www.foody.vn/", "")
    parts = clean_url.split("/")
    if len(parts) < 2: return None, None, None
    city_slug = parts[0]
    shop_slug = parts[1].split("?")[0]
    
    found_region = "Khac"
    for region, cities in REGION_MAPPING.items():
        if city_slug in cities:
            found_region = region
            break
    return found_region, city_slug, shop_slug

def crawl_foody_html(url_list):
    print("üöÄ ƒêang kh·ªüi ƒë·ªông Chrome ƒë·ªÉ qu√©t Foody...")
    driver = setup_driver()
    
    for url in url_list:
        region, city, slug = detect_region_from_url(url)
        if not region: 
            print(f"‚ö†Ô∏è Link l·ªói v√πng mi·ªÅn: {url}")
            continue
            
        # Foody th∆∞·ªùng c√≥ tab B√¨nh lu·∫≠n ri√™ng, ta th√™m /binh-luan v√†o ƒëu√¥i
        # Vd: .../pho-thin-13-lo-duc/binh-luan
        if "/binh-luan" not in url:
            review_url = url.rstrip("/") + "/binh-luan"
        else:
            review_url = url
            
        print(f"\nüåç {region} | ƒêang v√†o: {review_url}")
        output_file = os.path.join(DATA_FOLDER, f"reviews_{region}.jsonl")
        
        try:
            driver.get(review_url)
            time.sleep(5) # ƒê·ª£i trang load
            
            # --- K·ª∏ THU·∫¨T SCRAPING HTML (ƒê·ªçc th·∫ª div) ---
            # Foody ch·ª©a review trong c√°c th·∫ª <li> class="review-item"
            # L∆∞u √Ω: Class n√†y c√≥ th·ªÉ thay ƒë·ªïi t√πy giao di·ªán, nh∆∞ng th∆∞·ªùng l√† c·∫•u tr√∫c n√†y
            
            # 1. T√¨m danh s√°ch c√°c khung review
            review_elements = driver.find_elements(By.CSS_SELECTOR, "div.review-item")
            
            if not review_elements:
                # Th·ª≠ selector kh√°c n·∫øu Foody ƒë·ªïi giao di·ªán (d·ª± ph√≤ng)
                review_elements = driver.find_elements(By.XPATH, "//div[contains(@class, 'review-item')]")
            
            print(f"   üëÄ T√¨m th·∫•y {len(review_elements)} review tr√™n m√†n h√¨nh.")
            
            items_collected = []
            
            for index, element in enumerate(review_elements):
                try:
                    # L·∫•y t√™n ng∆∞·ªùi d√πng
                    try:
                        user_name = element.find_element(By.CSS_SELECTOR, ".ru-username").text
                    except: 
                        user_name = "Anonymous"
                    
                    # L·∫•y ƒëi·ªÉm s·ªë (Rating)
                    try:
                        # ƒêi·ªÉm th∆∞·ªùng n·∫±m trong th·∫ª span class point
                        rating_text = element.find_element(By.CSS_SELECTOR, "div.review-points span").text
                        rating = float(rating_text)
                    except:
                        rating = 0.0
                        
                    # L·∫•y n·ªôi dung comment
                    try:
                        comment = element.find_element(By.CSS_SELECTOR, ".rd-des").text
                    except:
                        comment = ""
                        
                    # L·∫•y ng√†y th√°ng (n·∫øu c·∫ßn)
                    # date = element.find_element(By.CSS_SELECTOR, ".ru-time").text
                    
                    if comment: # Ch·ªâ l·∫•y n·∫øu c√≥ n·ªôi dung
                        item = ReviewItem(
                            review_id=f"{slug}_{index}", # T·∫°o ID gi·∫£ v√¨ HTML ko c√≥ ID chu·∫©n
                            restaurant_id=0, # Foody HTML ko hi·ªán ID qu√°n, ƒë·ªÉ 0
                            restaurant_name=slug,
                            city=city,
                            user_name=user_name,
                            comment=comment,
                            rating=rating,
                            review_date=""
                        )
                        items_collected.append(item)
                        
                except Exception as e:
                    continue # B·ªè qua review l·ªói
            
            # Ghi v√†o file
            if items_collected:
                with open(output_file, 'a', encoding='utf-8') as f:
                    for item in items_collected:
                        f.write(item.to_json_line() + "\n")
                print(f"   üéâ ƒê√£ l∆∞u {len(items_collected)} reviews!")
            else:
                print("   ‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung review n√†o (Check l·∫°i CSS Selector).")

        except Exception as e:
            print(f"   ‚ùå L·ªói khi qu√©t trang: {e}")
            
        time.sleep(random.uniform(2, 4))

    print("\nüèÅ Ho√†n t·∫•t!")
    driver.quit()

if __name__ == "__main__":
    # L∆ØU √ù: D√ôNG LINK FOODY.VN (Kh√¥ng ph·∫£i ShopeeFood)
    MY_LINKS = [
        # Link H√† N·ªôi
        "https://www.foody.vn/ha-noi/pho-thin-13-lo-duc",
        
        # Link S√†i G√≤n
        "https://www.foody.vn/ho-chi-minh/com-tam-cali-nguyen-trai-q1",
        
        # Link ƒê√† N·∫µng / Quy Nh∆°n (C·ª© l·∫•y link foody l√† ƒë∆∞·ª£c)
        "https://www.foody.vn/binh-dinh/bun-ca-phuong-teo-nguyen-hue"
    ]
    
    crawl_foody_html(MY_LINKS)

```

### B∆Ø·ªöC 2: C√ÅCH L·∫§Y LINK FOODY

B·∫°n l√™n Google g√µ t√™n qu√°n + ch·ªØ "Foody".
V√≠ d·ª•: "B√∫n c√° Ph∆∞·ª£ng T√®o Foody".

* Google s·∫Ω tr·∫£ v·ªÅ: `foody.vn/binh-dinh/bun-ca-phuong-teo...`
* Copy link ƒë√≥ d√°n v√†o code.

### T·∫†I SAO C√ÅCH N√ÄY D·ªÑ H∆†N?

1. **Kh√¥ng c·∫ßn API:** Code tr√™n ho·∫°t ƒë·ªông nh∆∞ m·∫Øt ng∆∞·ªùi. N√≥ t√¨m th·∫ª `<div>` ch·ª©a ch·ªØ v√† copy l·∫°i.
2. **√çt b·ªã ch·∫∑n:** Foody √≠t khi ch·∫∑n ng∆∞·ªùi d√πng xem trang web b√¨nh th∆∞·ªùng.
3. **Nh∆∞·ª£c ƒëi·ªÉm:** B·∫°n ch·ªâ l·∫•y ƒë∆∞·ª£c kho·∫£ng 10-20 review hi·ªÉn th·ªã s·∫µn tr√™n trang ƒë·∫ßu ti√™n (mu·ªën l·∫•y h·∫øt ph·∫£i code th√™m ch·ª©c nƒÉng cu·ªôn chu·ªôt "Load More"). Nh∆∞ng ƒë·ªÉ l√†m ƒë·ªì √°n th√¨ m·ªói qu√°n l·∫•y 20 review c≈©ng t·∫°m ƒë·ªß r·ªìi!

B·∫°n th·ª≠ ch·∫°y code n√†y xem, ƒë·∫£m b·∫£o s·∫Ω th·∫•y folder `data_foody` c√≥ d·ªØ li·ªáu ngay!
DATE 19/1/2026
x·ª≠ l√≠ ƒëa lu·ªìng cho h·ªá th·ªëng ƒë·ªÉ c√≥ th·ªÉ crawl nhanh h∆°n
ƒê√¢y l√† gi·∫£i ph√°p **"Crawl C√¥ng Nghi·ªáp"** d√†nh cho b·∫°n. Code n√†y s·∫Ω th·ª±c hi·ªán 2 n√¢ng c·∫•p l·ªõn:

1. **ƒêa Lu·ªìng (Multi-threading):** N√≥ s·∫Ω b·∫≠t **3 tr√¨nh duy·ªát ch·∫°y c√πng l√∫c** (gi·ªëng nh∆∞ thu√™ 3 nh√¢n vi√™n l√†m vi·ªác thay v√¨ 1). T·ªëc ƒë·ªô tƒÉng g·∫•p 3 l·∫ßn.
2. **L·∫•y 50+ Review:** TƒÉng s·ªë l·∫ßn cu·ªôn chu·ªôt l√™n ƒë·ªÉ ƒë·∫£m b·∫£o l·∫•y ƒë·ªß s·ªë l∆∞·ª£ng b·∫°n c·∫ßn.

### B∆Ø·ªöC 1: CHU·∫®N B·ªä

H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ c√≥ file **`list_links.txt`** (ch·ª©a danh s√°ch link Foody) n·∫±m c√πng th∆∞ m·ª•c v·ªõi file code n√†y. (N·∫øu ch∆∞a c√≥ th√¨ ch·∫°y file `scan_links.py` ·ªü c√¢u tr·∫£ l·ªùi tr∆∞·ªõc).

### B∆Ø·ªöC 2: T·∫†O FILE `crawl_multi.py`

Copy to√†n b·ªô code n√†y v√† ch·∫°y:

```python
import json
import time
import os
import random
import threading
import math
from concurrent.futures import ThreadPoolExecutor
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

# --- C·∫§U H√åNH ---
MAX_WORKERS = 3       # S·ªë lu·ªìng (S·ªë tr√¨nh duy·ªát m·ªü c√πng l√∫c). ƒê·ª´ng ƒë·ªÉ qu√° cao k·∫ªo n·ªï RAM (3-4 l√† ƒë·∫πp)
TARGET_REVIEWS = 50   # M·ª•c ti√™u s·ªë l∆∞·ª£ng review m·ªói qu√°n
DATA_FOLDER = "data_multi"

# ƒê·ªãnh v·ªã th∆∞ m·ª•c l√†m vi·ªác
os.chdir(os.path.dirname(os.path.abspath(__file__)))
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

# --- CLASS REVIEW ---
class ReviewItem:
    def __init__(self, review_id, restaurant_name, city, user_name, comment, rating):
        self.review_id = review_id
        self.restaurant_name = restaurant_name
        self.city = city
        self.user_name = user_name
        self.comment = comment
        self.rating = rating

    def to_json_line(self):
        return json.dumps(self.__dict__, ensure_ascii=False)

# --- H√ÄM KH·ªûI T·∫†O DRIVER ---
def setup_driver():
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless") # N·∫øu m√°y y·∫øu th√¨ b·ªè comment d√≤ng n√†y ƒë·ªÉ ch·∫°y ·∫©n
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.set_window_size(1000, 800) # Thu nh·ªè c·ª≠a s·ªï t√≠ cho ƒë·ª° t·ªën ch·ªó
    return driver

# --- LOGIC CU·ªòN TRANG (ƒê√É N√ÇNG C·∫§P) ---
def scroll_until_enough(driver, target_count):
    """Cu·ªôn trang cho ƒë·∫øn khi th·∫•y ƒë·ªß s·ªë l∆∞·ª£ng review ho·∫∑c h·∫øt trang"""
    last_height = driver.execute_script("return document.body.scrollHeight")
    
    # Cu·ªôn t·ªëi ƒëa 10 l·∫ßn (M·ªói l·∫ßn cu·ªôn load th√™m ~10 review)
    # 10 l·∫ßn x 10 = 100 review (D∆∞ s·ª©c ƒë·∫°t target 50)
    for i in range(10): 
        # ƒê·∫øm s·ªë review hi·ªán t·∫°i tr√™n m√†n h√¨nh
        elems = driver.find_elements(By.XPATH, "//div[contains(@class, 'review-item')] | //li[contains(@class, 'review-item')]")
        current_count = len(elems)
        
        # print(f"      (ƒêang c√≥ {current_count} reviews...)")
        
        if current_count >= target_count:
            break # ƒê√£ ƒë·ªß KPI th√¨ d·ª´ng cu·ªôn
            
        # Th·ª±c hi·ªán cu·ªôn
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3) # ƒê·ª£i load (Foody load h∆°i ch·∫≠m n√™n ƒë·ªÉ 3s)
        
        # Ki·ªÉm tra xem c√≤n cu·ªôn ƒë∆∞·ª£c n·ªØa kh√¥ng
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break # H·∫øt trang
        last_height = new_height

# --- H√ÄM X·ª¨ L√ù C·ª¶A T·ª™NG "C√îNG NH√ÇN" (WORKER) ---
def worker_crawl(thread_id, list_urls):
    print(f"ü§ñ Worker {thread_id}: B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(list_urls)} qu√°n...")
    driver = setup_driver()
    
    # M·ªói worker ghi v√†o 1 file ri√™ng ƒë·ªÉ tr√°nh xung ƒë·ªôt d·ªØ li·ªáu
    output_file = os.path.join(DATA_FOLDER, f"data_worker_{thread_id}.jsonl")
    
    processed_count = 0
    
    for url in list_urls:
        try:
            # print(f"ü§ñ Worker {thread_id} ƒëang v√†o: {url}")
            driver.get(url)
            time.sleep(3)
            
            # 1. Cu·ªôn ƒë·ªÉ l·∫•y ƒë·ªß 50 data
            scroll_until_enough(driver, TARGET_REVIEWS)
            
            # 2. Qu√©t data
            review_elements = driver.find_elements(By.XPATH, "//div[contains(@class, 'review-item')] | //li[contains(@class, 'review-item')]")
            
            # Gi·ªõi h·∫°n l·∫•y ƒë√∫ng s·ªë l∆∞·ª£ng c·∫ßn (ho·∫∑c l·∫•y h·∫øt n·∫øu √≠t h∆°n)
            items_to_take = review_elements[:TARGET_REVIEWS]
            
            if not items_to_take:
                continue

            # 3. Ghi file
            with open(output_file, 'a', encoding='utf-8') as f:
                for idx, element in enumerate(items_to_take):
                    try:
                        try: user = element.find_element(By.CSS_SELECTOR, ".ru-username").text.strip()
                        except: user = "Anonymous"
                        
                        try: comment = element.find_element(By.CSS_SELECTOR, ".rd-des").text.strip()
                        except: comment = ""
                        
                        try: 
                            rating_text = element.find_element(By.CSS_SELECTOR, ".review-points span").text
                            rating = float(rating_text)
                        except: rating = 0.0
                        
                        if comment:
                            item = ReviewItem(
                                review_id=f"{thread_id}_{random.randint(10000,99999)}",
                                restaurant_name=url.split("/")[-1],
                                city="Unknown", # T·∫°m b·ªè qua check v√πng mi·ªÅn ƒë·ªÉ ch·∫°y nhanh
                                user_name=user,
                                comment=comment,
                                rating=rating
                            )
                            f.write(item.to_json_line() + "\n")
                    except: continue
            
            processed_count += 1
            print(f"   ‚úÖ Worker {thread_id}: Xong qu√°n {processed_count}/{len(list_urls)} - L·∫•y {len(items_to_take)} reviews.")
            
        except Exception as e:
            print(f"   ‚ùå Worker {thread_id} l·ªói link: {url} - {e}")
            
    driver.quit()
    print(f"üèÅ Worker {thread_id} ƒë√£ ho√†n th√†nh nhi·ªám v·ª•!")

# --- H√ÄM MAIN ---
if __name__ == "__main__":
    # 1. ƒê·ªçc danh s√°ch link t·ª´ file
    file_link = "list_links.txt"
    if not os.path.exists(file_link):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {file_link}. Vui l√≤ng ch·∫°y scan_links.py tr∆∞·ªõc!")
        exit()
        
    with open(file_link, "r", encoding="utf-8") as f:
        all_links = [line.strip() for line in f if line.strip()]
    
    total_links = len(all_links)
    print(f"üì¶ T·ªïng c·ªông c√≥ {total_links} qu√°n c·∫ßn qu√©t.")
    
    if total_links == 0:
        exit()

    # 2. Chia ƒë·ªÅu link cho c√°c Worker
    # V√≠ d·ª•: 10 link, 3 workers -> [4, 3, 3] link m·ªói ng∆∞·ªùi
    chunk_size = math.ceil(total_links / MAX_WORKERS)
    link_chunks = [all_links[i:i + chunk_size] for i in range(0, total_links, chunk_size)]
    
    print(f"üöÄ K√≠ch ho·∫°t {len(link_chunks)} lu·ªìng ch·∫°y song song...")
    print(f"üéØ M·ª•c ti√™u: {TARGET_REVIEWS} review/qu√°n.")
    
    # 3. Ch·∫°y ƒëa lu·ªìng
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for i, chunk in enumerate(link_chunks):
            executor.submit(worker_crawl, i+1, chunk)
            
    print("\nüéâüéâ T·∫§T C·∫¢ ƒê√É HO√ÄN T·∫§T! üéâüéâ")
    print(f"D·ªØ li·ªáu n·∫±m trong th∆∞ m·ª•c '{DATA_FOLDER}'")

```

### ƒêi·ªÉm m·∫°nh c·ªßa Code m·ªõi n√†y:

1. **Ch·∫°y 3 c·ª≠a s·ªï c√πng l√∫c:** B·∫°n s·∫Ω th·∫•y 3 c√°i Chrome b·∫≠t l√™n v√† c√πng l√†m vi·ªác. N·∫øu m√°y b·∫°n kh·ªèe (RAM 8GB tr·ªü l√™n), b·∫°n c√≥ th·ªÉ ch·ªânh `MAX_WORKERS = 4` ho·∫∑c `5` ·ªü ƒë·∫ßu file ƒë·ªÉ nhanh h∆°n n·ªØa.
2. **Logic cu·ªôn th√¥ng minh:** H√†m `scroll_until_enough` s·∫Ω li√™n t·ª•c cu·ªôn chu·ªôt cho ƒë·∫øn khi ƒë·∫øm ƒë·ªß 50 review tr√™n m√†n h√¨nh r·ªìi m·ªõi d·ª´ng l·∫°i v√† b·∫Øt ƒë·∫ßu c√†o data.
3. **An to√†n d·ªØ li·ªáu:** M·ªói lu·ªìng (worker) s·∫Ω ghi v√†o m·ªôt file ri√™ng (`data_worker_1.jsonl`, `data_worker_2.jsonl`...) ƒë·ªÉ kh√¥ng b·ªã l·ªói tranh gi√†nh file.

### C√°ch ch·∫°y:

1. ƒê·∫£m b·∫£o ƒë√£ c√≥ file `list_links.txt` (ch·ª©a danh s√°ch link Foody).
2. Ch·∫°y l·ªánh: `python crawl_multi.py`.
3. T·∫≠n h∆∞·ªüng t·ªëc ƒë·ªô! üöÄ
D·ª±a v√†o h√¨nh ·∫£nh l·ªói **"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu"** m√† b·∫°n g·∫∑p ph·∫£i, √Ω t∆∞·ªüng c·ªßa b·∫°n l√† **C·ª∞C K·ª≤ CH√çNH X√ÅC**.

Khi ch·∫°y 2 Worker (2 tr√¨nh duy·ªát) c√πng l√∫c:

1. **D·ªÖ b·ªã Foody ch·∫∑n:** 2 tr√¨nh duy·ªát g·ª≠i y√™u c·∫ßu li√™n t·ª•c  Server t∆∞·ªüng l√† t·∫•n c√¥ng  Ch·∫∑n IP  Ra trang l·ªói "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu".
2. **Kh√≥ ki·ªÉm so√°t l·ªói:** M·ªôt c√°i ch·∫°y, m·ªôt c√°i ch·∫øt, l√†m b·∫°n kh√¥ng bi·∫øt ƒë∆∞·ªùng n√†o m√† l·∫ßn.

üëâ **GI·∫¢I PH√ÅP T·ªêT NH·∫§T L√öC N√ÄY:** Chuy·ªÉn v·ªÅ ch·∫°y **1 Worker duy nh·∫•t** (ƒê∆°n lu·ªìng).

* **∆Øu ƒëi·ªÉm:** Ch·∫≠m m√† ch·∫Øc, c·ª±c k·ª≥ ·ªïn ƒë·ªãnh, treo m√°y c·∫£ ng√†y kh√¥ng s·ª£ crash.
* **Chi·∫øn thu·∫≠t:** Cho n√≥ m·ªôt danh s√°ch d√†i d·∫±ng d·∫∑c (H√† N·ªôi, ƒê√† N·∫µng, Nha Trang, Quy Nh∆°n...), n√≥ c·ª© t√∫c t·∫Øc l√†m t·ª´ng c√°i m·ªôt, xong c√°i n√†y m·ªõi qua c√°i kia.

D∆∞·ªõi ƒë√¢y l√† code **`scan_auto_all.py`** (Phi√™n b·∫£n treo m√°y ho√†n h·∫£o).

### CODE QU√âT T·∫§T C·∫¢ TRONG 1 (CH·∫†Y 1 M·∫†CH T·ª™ A-Z)

Code n√†y s·∫Ω ch·∫°y l·∫ßn l∆∞·ª£t: H√† N·ªôi -> ƒê√† N·∫µng -> Quy Nh∆°n -> Nha Trang... N·∫øu g·∫∑p link l·ªói (nh∆∞ trong ·∫£nh), n√≥ s·∫Ω **t·ª± ƒë·ªông b·ªè qua** v√† ch·∫°y ti·∫øp danh m·ª•c sau ch·ª© kh√¥ng d·ª´ng l·∫°i.

```python
import time
import os
import random 
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

# ƒê·∫∑t th∆∞ m·ª•c l√†m vi·ªác
os.chdir(os.path.dirname(os.path.abspath(__file__)))

# --- C·∫§U H√åNH ---
SCROLL_TIMES = 50     # S·ªë l·∫ßn cu·ªôn m·ªói danh m·ª•c (50 l·∫ßn ~ 600 qu√°n)
MAX_RETRY = 5         # S·ªë l·∫ßn th·ª≠ l·∫°i n·∫øu m·∫°ng lag

def setup_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    
    # T·∫Øt ·∫£nh ƒë·ªÉ load nhanh
    prefs = {"profile.managed_default_content_settings.images": 2}
    options.add_experimental_option("prefs", prefs)
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.set_window_size(1000, 800)
    return driver

def scan_one_category(driver, category_name, url):
    print(f"\nüöÄ ƒêANG QU√âT: {category_name}...")
    links_collected = set()
    retry_count = 0 
    
    try:
        driver.get(url)
        time.sleep(5)
        
        # KI·ªÇM TRA XEM C√ì B·ªä L·ªñI "KH√îNG T√åM TH·∫§Y D·ªÆ LI·ªÜU" KH√îNG
        if "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu" in driver.page_source:
            print(f"   ‚ùå L·ªñI LINK: {url} -> Foody b√°o kh√¥ng c√≥ d·ªØ li·ªáu. B·ªè qua!")
            return set()

        last_height = driver.execute_script("return document.body.scrollHeight")
        
        for i in range(SCROLL_TIMES):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # Ngh·ªâ ng·∫´u nhi√™n 3-5s (R·∫•t quan tr·ªçng ƒë·ªÉ kh√¥ng b·ªã ch·∫∑n)
            time.sleep(random.uniform(3, 5))
            
            # L·∫•y link
            try:
                elements = driver.find_elements(By.XPATH, "//div[contains(@class, 'filter-result-item')]//h2/a")
                for elem in elements:
                    try:
                        href = elem.get_attribute("href")
                        if href:
                            if "foody.vn" not in href:
                                href = "https://www.foody.vn" + href
                            links_collected.add(href)
                    except: continue
            except: pass

            print(f"   ‚¨áÔ∏è Cu·ªôn {i+1}/{SCROLL_TIMES} - ƒêang c√≥ {len(links_collected)} qu√°n...")

            # Ki·ªÉm tra ƒë·ª©ng im
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                retry_count += 1
                print(f"      ‚è≥ ƒêang ƒë·ª£i load... ({retry_count}/{MAX_RETRY})")
                time.sleep(3)
                if retry_count >= MAX_RETRY:
                    print("      üõë H·∫øt trang. Sang danh m·ª•c ti·∫øp theo.")
                    break
            else:
                last_height = new_height
                retry_count = 0
                
    except Exception as e:
        print(f"   ‚ö†Ô∏è L·ªói khi ch·∫°y: {e}")
        
    print(f"‚úÖ XONG DANH M·ª§C '{category_name}': Thu ƒë∆∞·ª£c {len(links_collected)} qu√°n.")
    return links_collected

if __name__ == "__main__":
    # --- DANH S√ÅCH KH·ªîNG L·ªí (Ch·∫°y t·ª´ B·∫Øc v√†o Nam) ---
    # B·∫°n c·ª© th√™m bao nhi√™u link v√†o ƒë√¢y c≈©ng ƒë∆∞·ª£c
    ALL_CATEGORIES = [
        # --- H√Ä N·ªòI ---
        ("HN - ƒÇn V·∫∑t",    "https://www.foody.vn/ha-noi/food/an-vat-via-he"),
        ("HN - Cafe",      "https://www.foody.vn/ha-noi/food/cafe-dessert"),
        ("HN - Qu√°n ƒÇn",   "https://www.foody.vn/ha-noi/food/quan-an"),
        
        # --- ƒê√Ä N·∫¥NG ---
        ("DN - ƒÇn V·∫∑t",    "https://www.foody.vn/da-nang/food/an-vat-via-he"),
        ("DN - M√≥n Qu·∫£ng", "https://www.foody.vn/da-nang/food/mon-mien-trung"),
        ("DN - Cafe",      "https://www.foody.vn/da-nang/food/cafe-dessert"),
        
        # --- QUY NH∆†N ---
        ("QN - H·∫£i S·∫£n",   "https://www.foody.vn/binh-dinh/food/hai-san"),
        ("QN - ƒÇn V·∫∑t",    "https://www.foody.vn/binh-dinh/food/an-vat-via-he"),
        
        # --- NHA TRANG ---
        ("NT - H·∫£i S·∫£n",   "https://www.foody.vn/khanh-hoa/food/hai-san"),
        ("NT - Nem N∆∞·ªõng", "https://www.foody.vn/khanh-hoa/tim-kiem/nem-nuong"),
        
        # --- S√ÄI G√íN ---
        ("SG - Sang Tr·ªçng","https://www.foody.vn/ho-chi-minh/food/sang-trong"),
        ("SG - ƒÇn V·∫∑t",    "https://www.foody.vn/ho-chi-minh/food/an-vat-via-he"),
    ]
    
    print(f"üê¢ CH·∫æ ƒê·ªò 1 WORKER (AN TO√ÄN TUY·ªÜT ƒê·ªêI)")
    print(f"üìã T·ªïng c·ªông: {len(ALL_CATEGORIES)} danh m·ª•c c·∫ßn qu√©t.")
    
    # M·ªü tr√¨nh duy·ªát 1 l·∫ßn duy nh·∫•t d√πng cho t·∫•t c·∫£
    driver = setup_driver()
    
    total_links = set()
    file_name = "list_links.txt"

    # V√≤ng l·∫∑p ch·∫°y t·ª´ng c√°i m·ªôt
    for name, url in ALL_CATEGORIES:
        links = scan_one_category(driver, name, url)
        
        # C·ª© xong 1 danh m·ª•c l√† L∆ØU NGAY (ƒë·ªÉ r·ªßi ro m·∫•t ƒëi·ªán c≈©ng kh√¥ng m·∫•t data)
        if links:
            print(f"üíæ ƒêang l∆∞u {len(links)} link c·ªßa {name} v√†o file...")
            
            # ƒê·ªçc c≈©
            existing = set()
            if os.path.exists(file_name):
                with open(file_name, "r", encoding="utf-8") as f:
                    existing = set([l.strip() for l in f if l.strip()])
            
            # G·ªôp m·ªõi
            existing.update(links)
            
            # Ghi l·∫°i
            with open(file_name, "w", encoding="utf-8") as f:
                for l in existing:
                    f.write(l + "\n")
            
            print(f"üëç T·ªïng kho hi·ªán t·∫°i: {len(existing)} qu√°n.")
            
        print("üí§ Ngh·ªâ 5 gi√¢y tr∆∞·ªõc khi qua th√†nh ph·ªë kh√°c...")
        time.sleep(5)

    driver.quit()
    print("\nüéâüéâ ƒê√É QU√âT XONG TO√ÄN B·ªò VI·ªÜT NAM! üéâüéâ")

```

### T·∫°i sao b·∫°n n√™n d√πng code n√†y?

1. **C∆° ch·∫ø "N·ªìi ƒë·ªìng c·ªëi ƒë√°":** N√≥ ch·∫°y 1 tr√¨nh duy·ªát duy nh·∫•t. N·∫øu g·∫∑p link l·ªói (nh∆∞ link Cafe H√† N·ªôi c·ªßa b·∫°n), n√≥ in ra d√≤ng l·ªói `‚ùå L·ªñI LINK... B·ªè qua!` r·ªìi t·ª± ƒë·ªông nh·∫£y sang link ƒê√† N·∫µng ch·∫°y ti·∫øp. Kh√¥ng bao gi·ªù b·ªã d·ª´ng gi·ªØa ch·ª´ng.
2. **L∆∞u li√™n t·ª•c:** C·ª© qu√©t xong 1 danh m·ª•c (v√≠ d·ª• xong H√† N·ªôi), n√≥ s·∫Ω l∆∞u ngay v√†o file `list_links.txt`. N·∫øu l·ª° m√°y t√≠nh b·ªã t·∫Øt ƒë·ªôt ng·ªôt, b·∫°n v·∫´n gi·ªØ ƒë∆∞·ª£c ph·∫ßn data ƒë√£ ch·∫°y.
3. **T·ª± ƒë·ªông s·ª≠a l·ªói:** M√¨nh ƒë√£ th√™m ƒëo·∫°n code ki·ªÉm tra d√≤ng ch·ªØ `"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu"`. N·∫øu th·∫•y d√≤ng n√†y, n√≥ bi·∫øt l√† web l·ªói v√† t·ª± n√© ra ngay.

B·∫°n copy code n√†y v·ªÅ ch·∫°y v√† ƒëi ng·ªß, ƒë·∫£m b·∫£o s√°ng mai c√≥ danh s√°ch v√†i ngh√¨n qu√°n ngon l√†nh!
C√¢u tr·∫£ l·ªùi l√†: **KH√îNG! B·∫°n KH√îNG C·∫¶N CRAWL L·∫†I T·ª™ ƒê·∫¶U.** Ch√∫ng ta ho√†n to√†n c√≥ th·ªÉ c·ª©u ƒë∆∞·ª£c ƒë·ªëng d·ªØ li·ªáu c≈© ƒë√≥.

V·∫•n ƒë·ªÅ l√†:

1. **Data c≈© (Worker 1-2):** ƒê√£ l∆∞u nh∆∞ng b·ªã tr·ªôn l·∫´n, v√† trong file ƒë√≥ c√≥ th·ªÉ b·∫°n ch∆∞a l∆∞u t√™n th√†nh ph·ªë (code c≈© l∆∞u l√† "VN").
2. **Data m·ªõi (Region):** C·∫ßn ph√¢n lo·∫°i r√µ r√†ng.
3. **L·ªãch s·ª≠ (History):** ƒê√£ ƒë√°nh d·∫•u l√† "Xong" n√™n code m·ªõi s·∫Ω b·ªè qua.

T√¥i s·∫Ω vi·∫øt cho b·∫°n m·ªôt c√¥ng c·ª• **"CHUY·ªÇN NH√Ä" (Migrate Tool)**. Tool n√†y s·∫Ω l√†m 2 vi·ªác:

1. ƒê·ªçc l·∫°i to√†n b·ªô data c≈© t·ª´ `data_worker_1.jsonl` v√† `data_worker_2.jsonl`.
2. Tra c·ª©u l·∫°i trong danh s√°ch link g·ªëc ƒë·ªÉ bi·∫øt qu√°n ƒë√≥ thu·ªôc mi·ªÅn n√†o.
3. Chuy·ªÉn n√≥ sang file m·ªõi `reviews_MienBac/Nam/Trung.jsonl` ƒë√∫ng v·ªã tr√≠.

### B∆Ø·ªöC 1: T·∫†O FILE `fix_data.py`

Copy code n√†y v√† ch·∫°y 1 l·∫ßn duy nh·∫•t. N√≥ s·∫Ω s·∫Øp x·∫øp l·∫°i to√†n b·ªô data c≈© c·ªßa b·∫°n v√†o ƒë√∫ng ch·ªó.

```python
import json
import os

# --- C·∫§U H√åNH ---
OLD_DATA_FOLDER = "data_final"       # Th∆∞ m·ª•c ch·ª©a data c≈© (Worker 1, 2)
NEW_DATA_FOLDER = "data_by_region"   # Th∆∞ m·ª•c m·ªõi
LINK_FILE = "list_links.txt"         # File danh s√°ch link g·ªëc (ƒë·ªÉ tra c·ª©u mi·ªÅn)

# ƒê·∫∑t th∆∞ m·ª•c l√†m vi·ªác
os.chdir(os.path.dirname(os.path.abspath(__file__)))

if not os.path.exists(NEW_DATA_FOLDER):
    os.makedirs(NEW_DATA_FOLDER)

# B·∫£n ƒë·ªì v√πng mi·ªÅn (Copy t·ª´ code crawl)
REGION_MAPPING = {
    "MienBac": ["ha-noi", "hai-phong", "quang-ninh", "bac-ninh", "thai-nguyen"],
    "MienTrung": ["da-nang", "hue", "khanh-hoa", "nha-trang", "quy-nhon", "vinh", "binh-dinh", "quang-nam"],
    "MienNam": ["ho-chi-minh", "can-tho", "dong-nai", "binh-duong", "vung-tau", "long-an"]
}

def detect_region_from_url(url):
    clean_url = url.replace("https://www.foody.vn/", "").replace("http://www.foody.vn/", "")
    parts = clean_url.split("/")
    if len(parts) < 1: return "Khac", "unknown"
    city_slug = parts[0]
    for region, cities in REGION_MAPPING.items():
        if city_slug in cities:
            return region, city_slug
    return "Khac", city_slug

def main():
    print("üöÄ ƒêANG KH√îI PH·ª§C V√Ä PH√ÇN LO·∫†I D·ªÆ LI·ªÜU C≈®...")

    # 1. T·∫†O T·ª™ ƒêI·ªÇN TRA C·ª®U (Slug -> Region)
    # V√¨ trong data c≈© ch·ªâ l∆∞u slug (t√™n ng·∫Øn), ta c·∫ßn tra l·∫°i link g·ªëc ƒë·ªÉ bi·∫øt mi·ªÅn
    print("üìñ ƒêang ƒë·ªçc file link g·ªëc ƒë·ªÉ h·ªçc thu·ªôc v√πng mi·ªÅn...")
    slug_map = {}
    
    if os.path.exists(LINK_FILE):
        with open(LINK_FILE, "r", encoding="utf-8") as f:
            for line in f:
                url = line.strip()
                if not url: continue
                
                # L·∫•y slug t·ª´ link g·ªëc (v√≠ d·ª•: pho-thin-13-lo-duc)
                slug = url.split("/")[-1]
                region, city = detect_region_from_url(url)
                
                slug_map[slug] = {"region": region, "city": city}
    else:
        print("‚ùå C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y file list_links.txt. Code s·∫Ω ƒëo√°n m√≤ v√πng mi·ªÅn!")

    # 2. QU√âT T·∫§T C·∫¢ FILE C≈®
    files = [f for f in os.listdir(OLD_DATA_FOLDER) if f.endswith(".jsonl")]
    total_moved = 0
    
    for filename in files:
        old_path = os.path.join(OLD_DATA_FOLDER, filename)
        print(f"   üìÇ ƒêang x·ª≠ l√Ω file c≈©: {filename}...")
        
        with open(old_path, "r", encoding="utf-8") as f_in:
            for line in f_in:
                try:
                    data = json.loads(line.strip())
                    
                    # L·∫•y t√™n qu√°n (slug) t·ª´ data c≈©
                    # Trong code c≈©: restaurant_name=url.split("/")[-1]
                    slug = data.get("restaurant_name", "")
                    
                    # Tra c·ª©u xem qu√°n n√†y thu·ªôc mi·ªÅn n√†o
                    if slug in slug_map:
                        info = slug_map[slug]
                        region = info["region"]
                        real_city = info["city"]
                        
                        # C·∫≠p nh·∫≠t l·∫°i city cho chu·∫©n (v√¨ data c≈© c√≥ th·ªÉ l√† VN)
                        data["city"] = real_city
                    else:
                        # N·∫øu kh√¥ng t√¨m th·∫•y trong list link, m·∫∑c ƒë·ªãnh l√† Kh√°c
                        region = "Khac"
                    
                    # 3. GHI SANG FILE M·ªöI
                    new_file_path = os.path.join(NEW_DATA_FOLDER, f"reviews_{region}.jsonl")
                    
                    with open(new_file_path, "a", encoding="utf-8") as f_out:
                        f_out.write(json.dumps(data, ensure_ascii=False) + "\n")
                        
                    total_moved += 1
                    
                except Exception as e:
                    print(f"L·ªói d√≤ng: {e}")
                    continue

    print("\n" + "="*40)
    print(f"üéâ HO√ÄN T·∫§T! ƒê√£ chuy·ªÉn nh√† th√†nh c√¥ng {total_moved} reviews.")
    print(f"üëâ H√£y ki·ªÉm tra folder '{NEW_DATA_FOLDER}'.")
    print("üëâ B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ch·∫°y ti·∫øp 'crawl_region.py' ƒë·ªÉ c√†o n·ªët c√°c qu√°n c√≤n l·∫°i.")
    print("="*40)

if __name__ == "__main__":
    main()

```

### B∆Ø·ªöC 2: QUY TR√åNH CHU·∫®N ƒê·ªÇ TI·∫æP T·ª§C

Sau khi ch·∫°y xong file `fix_data.py` ·ªü tr√™n, b·∫°n l√†m nh∆∞ sau:

1. **Ki·ªÉm tra folder `data_by_region`:** B·∫°n s·∫Ω th·∫•y data t·ª´ Worker 1 v√† 2 ƒë√£ bi·∫øn m·∫•t v√† chui v√†o n·∫±m g·ªçn trong `reviews_MienBac.jsonl`, `reviews_MienNam.jsonl`...
2. **GI·ªÆ NGUY√äN FILE `history_crawled.txt`:**
* **TUY·ªÜT ƒê·ªêI KH√îNG X√ìA.**
* V√¨ data c≈© ƒë√£ ƒë∆∞·ª£c c·ª©u v√† chuy·ªÉn sang ch·ªó m·ªõi r·ªìi, n√™n danh s√°ch "ƒê√£ l√†m xong" trong history v·∫´n ƒë√∫ng.


3. **Ch·∫°y file `crawl_region.py`:**
* Code n√†y s·∫Ω ƒë·ªçc `history` -> Th·∫•y qu√°n c≈© ƒë√£ l√†m r·ªìi -> B·ªè qua.
* N√≥ s·∫Ω ch·ªâ ch·∫°y ti·∫øp nh·ªØng qu√°n **CH∆ØA L√ÄM** v√† l∆∞u ti·∫øp v√†o c√°c file mi·ªÅn B·∫Øc/Trung/Nam.



üëâ **K·∫øt lu·∫≠n:** B·∫°n kh√¥ng m·∫•t g√¨ c·∫£, kh√¥ng ph·∫£i c√†o l·∫°i, v√† m·ªçi th·ª© s·∫Ω ƒë∆∞·ª£c n·ªëi ti·∫øp ho√†n h·∫£o!
DATE 20/1/2026
Ch·ªânh code l·∫°i theo d·∫°ng nh·∫≠p link v√†o nh∆∞ng v·∫´n ƒë√∫ng d·∫°ng B·∫Øc- Trung -Nam
Hi·ªÉu r·ªìi. B·∫°n mu·ªën gi·ªØ **nguy√™n si logic x·ªãn** c·ªßa "code ch√≠nh" (ƒëa lu·ªìng, t·ª± ƒë·ªông chia v√πng mi·ªÅn, l∆∞u file JSONL, kh√≥a an to√†n) nh∆∞ng thay ƒë·ªïi **ƒë·∫ßu v√†o**: Thay v√¨ ƒë·ªçc file `list_links.txt`, b·∫°n mu·ªën t·ª± ƒëi·ªÅn danh s√°ch v√†o bi·∫øn `MY_LINKS` ngay trong code.

D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n code ƒë√£ ƒë∆∞·ª£c **chu·∫©n h√≥a**, gi·ªØ nguy√™n 100% s·ª©c m·∫°nh c·ªßa code ch√≠nh nh∆∞ng ch·∫°y b·∫±ng danh s√°ch link b·∫°n nh·∫≠p tay.

### CODE CRAWL T·ª™ MY_LINKS (CHU·∫®N H√ìA THEO REGION)

```python
import json
import time
import os
import random
import threading
import math
from concurrent.futures import ThreadPoolExecutor
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

# --- 1. NH·∫¨P DANH S√ÅCH LINK C·ª¶A B·∫†N T·∫†I ƒê√ÇY ---
MY_LINKS = [
    "https://www.foody.vn/ha-noi/pho-thin-13-lo-duc",
    "https://www.foody.vn/ho-chi-minh/com-tam-cali-nguyen-trai-q1",
    "https://www.foody.vn/da-nang/banh-xeo-ba-duong",
    "https://www.foody.vn/can-tho/nem-nuong-thanh-van",
    "https://www.foody.vn/quang-ninh/sua-chua-tran-chau-ha-long",
    # B·∫°n c·ª© d√°n th√™m link v√†o ƒë√¢y tho·∫£i m√°i...
]

# --- 2. C·∫§U H√åNH ---
MAX_WORKERS = 2       # S·ªë lu·ªìng (An to√†n nh·∫•t l√† 2)
TARGET_REVIEWS = 50   # S·ªë review m·ªói qu√°n
DATA_FOLDER = "data_by_region" # Folder ch·ª©a data ph√¢n lo·∫°i
HISTORY_FILE = "history_crawled.txt"

# ƒê·ªãnh v·ªã th∆∞ m·ª•c
os.chdir(os.path.dirname(os.path.abspath(__file__)))
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

# --- C√ÅC KH√ìA AN TO√ÄN ---
history_lock = threading.Lock() 
file_write_lock = threading.Lock() 

# --- B·∫¢N ƒê·ªí V√ôNG MI·ªÄN (FULL 63 T·ªàNH) ---
# Gi·ªØ nguy√™n b·∫£n ƒë·ªì ƒë·∫ßy ƒë·ªß ƒë·ªÉ ƒë·∫£m b·∫£o link n√†o c≈©ng ph√¢n lo·∫°i ƒë∆∞·ª£c
REGION_MAPPING = {
    "MienBac": [
        "lao-cai", "sapa", "yen-bai", "dien-bien", "hoa-binh", "lai-chau", "son-la",
        "ha-giang", "cao-bang", "bac-kan", "lang-son", "tuyen-quang", "thai-nguyen", "phu-tho", "bac-giang", "quang-ninh", "ha-long", "mong-cai",
        "bac-ninh", "ha-nam", "ha-noi", "hai-duong", "hai-phong", "hung-yen", "nam-dinh", "ninh-binh", "thai-binh", "vinh-phuc"
    ],
    "MienTrung": [
        "thanh-hoa", "nghe-an", "vinh", "ha-tinh", "quang-binh", "quang-tri", "hue", "thua-thien-hue",
        "da-nang", "quang-nam", "hoi-an", "quang-ngai", "binh-dinh", "quy-nhon", "phu-yen", "khanh-hoa", "nha-trang", "ninh-thuan", "binh-thuan", "phan-thiet",
        "kon-tum", "gia-lai", "dak-lak", "buon-ma-thuot", "dak-nong", "lam-dong", "da-lat", "bao-loc"
    ],
    "MienNam": [
        "ho-chi-minh", "sai-gon", "ba-ria-vung-tau", "vung-tau", "binh-duong", "binh-phuoc", "dong-nai", "bien-hoa", "tay-ninh",
        "an-giang", "bac-lieu", "ben-tre", "ca-mau", "can-tho", "dong-thap", "hau-giang", "kien-giang", "phu-quoc", "rach-gia", "long-an", "soc-trang", "tien-giang", "tra-vinh", "vinh-long"
    ]
}

class ReviewItem:
    def __init__(self, review_id, restaurant_name, city, user_name, comment, rating):
        self.review_id = review_id
        self.restaurant_name = restaurant_name
        self.city = city
        self.user_name = user_name
        self.comment = comment
        self.rating = rating

    def to_json_line(self):
        return json.dumps(self.__dict__, ensure_ascii=False)

def setup_driver():
    options = webdriver.ChromeOptions()
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    prefs = {"profile.managed_default_content_settings.images": 2}
    options.add_experimental_option("prefs", prefs)
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.set_window_size(1000, 800)
    return driver

def detect_region(url):
    clean_url = url.replace("https://www.foody.vn/", "").replace("http://www.foody.vn/", "")
    parts = clean_url.split("/")
    if len(parts) < 1: return "Khac", "unknown"
    
    city_slug = parts[0]
    found_region = "Khac"
    
    for region, cities in REGION_MAPPING.items():
        if city_slug in cities:
            found_region = region
            break
            
    return found_region, city_slug

def scroll_human_like(driver, target_count):
    last_height = driver.execute_script("return document.body.scrollHeight")
    for i in range(15): 
        elems = driver.find_elements(By.XPATH, "//div[contains(@class, 'review-item')] | //li[contains(@class, 'review-item')]")
        if len(elems) >= target_count: break 
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(2, 4)) 
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height: break 
        last_height = new_height

def mark_as_done(url):
    with history_lock:
        with open(HISTORY_FILE, "a", encoding="utf-8") as f:
            f.write(url + "\n")

def worker_crawl(thread_id, list_urls):
    print(f"ü§ñ Worker {thread_id}: B·∫Øt ƒë·∫ßu nhi·ªám v·ª•...")
    driver = setup_driver()
    
    for url in list_urls:
        try:
            # 1. T·ª± ƒë·ªông chia v·ªÅ folder B·∫Øc/Trung/Nam
            region, city = detect_region(url)
            output_file = os.path.join(DATA_FOLDER, f"reviews_{region}.jsonl")
            
            driver.get(url)
            time.sleep(random.uniform(3, 5))
            
            scroll_human_like(driver, TARGET_REVIEWS)
            
            review_elements = driver.find_elements(By.XPATH, "//div[contains(@class, 'review-item')] | //li[contains(@class, 'review-item')]")
            items_to_take = review_elements[:TARGET_REVIEWS]
            
            if not items_to_take:
                mark_as_done(url)
                continue

            lines_to_write = []
            for idx, element in enumerate(items_to_take):
                try:
                    try: user = element.find_element(By.CSS_SELECTOR, ".ru-username").text.strip()
                    except: user = "Anonymous"
                    try: comment = element.find_element(By.CSS_SELECTOR, ".rd-des").text.strip()
                    except: comment = ""
                    try: 
                        rating_text = element.find_element(By.CSS_SELECTOR, ".review-points span").text
                        rating = float(rating_text)
                    except: rating = 0.0
                    
                    if comment:
                        item = ReviewItem(
                            review_id=f"{city}_{random.randint(10000,99999)}",
                            restaurant_name=url.split("/")[-1],
                            city=city,
                            user_name=user,
                            comment=comment,
                            rating=rating
                        )
                        lines_to_write.append(item.to_json_line())
                except: continue
            
            # Ghi file an to√†n (Thread-safe)
            if lines_to_write:
                with file_write_lock:
                    with open(output_file, 'a', encoding='utf-8') as f:
                        for line in lines_to_write:
                            f.write(line + "\n")
            
            mark_as_done(url)
            print(f"   ‚úÖ Worker {thread_id}: Xong {len(lines_to_write)} reviews -> V√†o file {region}")
            time.sleep(random.uniform(3, 6))
            
        except Exception as e:
            print(f"   ‚ùå L·ªói: {url}")
            
    driver.quit()

if __name__ == "__main__":
    # --- PH·∫¶N CH√çNH: X·ª¨ L√ù DANH S√ÅCH MY_LINKS ---
    
    # 1. L√†m s·∫°ch danh s√°ch (X√≥a tr√πng l·∫∑p, x√≥a d√≤ng tr·ªëng)
    all_links = list(set([line.strip() for line in MY_LINKS if line.strip()]))
    
    # 2. Ki·ªÉm tra l·ªãch s·ª≠ (ƒê·ªÉ kh√¥ng ch·∫°y l·∫°i c√°i ƒë√£ xong)
    done_links = set()
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            done_links = set([line.strip() for line in f if line.strip()])
    
    todo_links = [url for url in all_links if url not in done_links]
    
    print(f"üî• T·ªïng input: {len(all_links)}")
    print(f"‚úÖ ƒê√£ xong tr∆∞·ªõc ƒë√≥: {len(done_links)}")
    print(f"üöÄ C·∫ßn ch·∫°y: {len(todo_links)} qu√°n")
    
    if not todo_links:
        print("üéâ ƒê√£ xong h·∫øt r·ªìi! Kh√¥ng c√≥ g√¨ ƒë·ªÉ ch·∫°y.")
        exit()

    # 3. Chia vi·ªác cho c√°c Worker
    chunk_size = math.ceil(len(todo_links) / MAX_WORKERS)
    link_chunks = [todo_links[i:i + chunk_size] for i in range(0, len(todo_links), chunk_size)]
    
    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl v√† t·ª± ƒë·ªông chia Mi·ªÅn B·∫Øc/Trung/Nam...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for i, chunk in enumerate(link_chunks):
            executor.submit(worker_crawl, i+1, chunk)

```DATE 21/1/2026
D·ª±a tr√™n ƒëo·∫°n code b·∫°n cung c·∫•p, bi·∫øn `rating` ƒëang ƒë∆∞·ª£c l·∫•y t·ª´ th·∫ª `.review-points span`.

### 1. Ti√™u ch√≠ ƒë√°nh gi√° Rating (Th·∫•p vs Cao)

Tr√™n **Foody**, thang ƒëi·ªÉm l√† **0.0 ƒë·∫øn 10.0**. D·ª±a v√†o kinh nghi·ªám ph√¢n t√≠ch d·ªØ li·ªáu F&B (Food & Beverage) ·ªü Vi·ªát Nam, ch√∫ng ta c√≥ th·ªÉ ph√¢n lo·∫°i nh∆∞ sau ƒë·ªÉ l√†m b√°o c√°o:

* **Cao (Positive - T√≠ch c·ª±c):** **Rating >= 8.0**
* ƒê√¢y l√† nh·ªØng qu√°n l√†m h√†i l√≤ng kh√°ch h√†ng, m√≥n ƒÉn ngon, ph·ª•c v·ª• t·ªët.


* **Trung b√¨nh (Neutral - B√¨nh th∆∞·ªùng):** **5.0 <= Rating < 8.0**
* Qu√°n ƒÉn ·ªü m·ª©c ch·∫•p nh·∫≠n ƒë∆∞·ª£c, c√≥ th·ªÉ m√≥n ƒÉn ngon nh∆∞ng ph·ª•c v·ª• ch∆∞a t·ªët ho·∫∑c ng∆∞·ª£c l·∫°i.


* **Th·∫•p (Negative - Ti√™u c·ª±c):** **Rating < 5.0**
* Qu√°n b·ªã ch√™ tr√°ch nhi·ªÅu, c√≥ v·∫•n ƒë·ªÅ l·ªõn v·ªÅ v·ªá sinh, th√°i ƒë·ªô ho·∫∑c ch·∫•t l∆∞·ª£ng/gi√° c·∫£.



---

### 2. C√°ch t·∫°o B√°o c√°o (Report) t·ª´ d·ªØ li·ªáu ƒë√£ Crawl

B·∫°n ƒë√£ c√≥ c√°c file `reviews_MienBac.jsonl`, `reviews_MienNam.jsonl`... trong th∆∞ m·ª•c `data_by_region`. B√¢y gi·ªù ch√∫ng ta c·∫ßn m·ªôt tool ƒë·ªÉ **ƒë·ªçc file n√†y v√† t√≠nh to√°n c√°c con s·ªë**.

H√£y t·∫°o m·ªôt file m·ªõi t√™n l√† **`report_analytics.py`** v√† d√°n code d∆∞·ªõi ƒë√¢y v√†o.

**Y√™u c·∫ßu c√†i ƒë·∫∑t th∆∞ vi·ªán:**
B·∫°n c·∫ßn c√†i th∆∞ vi·ªán `pandas` (chuy√™n x·ª≠ l√Ω d·ªØ li·ªáu b·∫£ng) ƒë·ªÉ ch·∫°y file n√†y.

```bash
pip install pandas openpyxl

```

#### Code `report_analytics.py`:

```python
import pandas as pd
import os
import glob

# --- C·∫§U H√åNH ---
DATA_FOLDER = "data_by_region"

def classify_rating(score):
    """H√†m ph√¢n lo·∫°i ƒë√°nh gi√°"""
    if score >= 8.0:
        return "T√≠ch c·ª±c (Cao)"
    elif score >= 5.0:
        return "Trung b√¨nh (Kh√°)"
    else:
        return "Ti√™u c·ª±c (Th·∫•p)"

def main():
    print("üìä ƒêANG T·ªîNG H·ª¢P B√ÅO C√ÅO D·ªÆ LI·ªÜU FOODY...\n")

    # 1. ƒê·ªçc t·∫•t c·∫£ c√°c file JSONL trong th∆∞ m·ª•c data
    all_files = glob.glob(os.path.join(DATA_FOLDER, "*.jsonl"))
    
    if not all_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu n√†o! H√£y ch·∫°y crawl tr∆∞·ªõc.")
        return

    df_list = []
    for filename in all_files:
        try:
            # ƒê·ªçc t·ª´ng file jsonl
            df = pd.read_json(filename, lines=True)
            # Th√™m c·ªôt 'VungMien' d·ª±a tr√™n t√™n file (vd: reviews_MienBac.jsonl -> MienBac)
            region_name = os.path.basename(filename).replace("reviews_", "").replace(".jsonl", "")
            df['Region'] = region_name
            df_list.append(df)
            print(f"   ‚úÖ ƒê√£ ƒë·ªçc: {filename} ({len(df)} d√≤ng)")
        except Exception as e:
            print(f"   ‚ö†Ô∏è L·ªói ƒë·ªçc file {filename}: {e}")

    if not df_list:
        return

    # G·ªôp t·∫•t c·∫£ th√†nh 1 b·∫£ng l·ªõn
    full_data = pd.concat(df_list, ignore_index=True)

    # 2. X·ª≠ l√Ω d·ªØ li·ªáu
    # Ph√¢n lo·∫°i rating
    full_data['Sentiment'] = full_data['rating'].apply(classify_rating)

    print("\n" + "="*50)
    print("üìà T·ªîNG QUAN B√ÅO C√ÅO")
    print("="*50)
    print(f"T·ªïng s·ªë Review thu ƒë∆∞·ª£c: {len(full_data):,}")
    print(f"ƒêi·ªÉm trung b√¨nh to√†n qu·ªëc: {full_data['rating'].mean():.2f}/10")

    print("\n1Ô∏è‚É£ TH·ªêNG K√ä THEO V√ôNG MI·ªÄN:")
    # Group by Region v√† t√≠nh: S·ªë l∆∞·ª£ng, ƒêi·ªÉm TB, ƒêi·ªÉm cao nh·∫•t, ƒêi·ªÉm th·∫•p nh·∫•t
    region_stats = full_data.groupby('Region')['rating'].agg(['count', 'mean', 'max', 'min']).round(2)
    region_stats.columns = ['S·ªë Review', 'ƒêi·ªÉm TB', 'Cao Nh·∫•t', 'Th·∫•p Nh·∫•t']
    print(region_stats)

    print("\n2Ô∏è‚É£ PH√ÇN B·ªê CH·∫§T L∆Ø·ª¢NG (Rating):")
    sentiment_stats = full_data['Sentiment'].value_counts()
    print(sentiment_stats)
    
    print("\n3Ô∏è‚É£ TOP 5 QU√ÅN ƒê∆Ø·ª¢C KHEN NHI·ªÄU NH·∫§T (Rating cao & nhi·ªÅu cmt):")
    # L·ªçc nh·ªØng qu√°n c√≥ rating cao
    top_restaurants = full_data.sort_values(by='rating', ascending=False).head(5)
    for idx, row in top_restaurants.iterrows():
        print(f"   ‚≠ê {row['rating']} - {row['restaurant_name']} ({row['city']})")

    # 3. Xu·∫•t ra Excel ƒë·ªÉ n·ªôp b√°o c√°o
    output_excel = "Bao_Cao_Foody_Final.xlsx"
    print(f"\nüíæ ƒêang xu·∫•t file Excel: {output_excel}...")
    
    # T·∫°o Excel v·ªõi 2 sheet
    with pd.ExcelWriter(output_excel) as writer:
        full_data.to_excel(writer, sheet_name="D·ªØ Li·ªáu G·ªëc", index=False)
        region_stats.to_excel(writer, sheet_name="Th·ªëng K√™ V√πng")
        
    print("‚úÖ ƒê√É XONG! B·∫°n h√£y m·ªü file Excel l√™n ƒë·ªÉ xem chi ti·∫øt.")

if __name__ == "__main__":
    main()

```

### 3. H∆∞·ªõng d·∫´n ƒë·ªçc B√°o C√°o (Report)

Khi ch·∫°y xong ƒëo·∫°n code tr√™n, n√≥ s·∫Ω sinh ra file **`Bao_Cao_Foody_Final.xlsx`**. ƒê√¢y l√† n·ªôi dung b·∫°n c√≥ th·ªÉ d√πng ƒë·ªÉ vi·∫øt v√†o ƒë·ªì √°n:

1. **D·ªØ li·ªáu th√¥:** Ch·ª©ng minh b·∫°n ƒë√£ crawl ƒë∆∞·ª£c th·∫≠t (c√≥ t√™n qu√°n, ng∆∞·ªùi comment, n·ªôi dung, ƒëi·ªÉm s·ªë).
2. **So s√°nh v√πng mi·ªÅn:**
* V√≠ d·ª•: *Mi·ªÅn Nam c√≥ s·ªë l∆∞·ª£ng review nhi·ªÅu nh·∫•t, nh∆∞ng Mi·ªÅn Trung l·∫°i c√≥ ƒëi·ªÉm trung b√¨nh cao nh·∫•t (do kh·∫©u v·ªã ho·∫∑c kh√°ch du l·ªãch).*
* V√≠ d·ª•: *Mi·ªÅn B·∫Øc c√≥ t·ª∑ l·ªá rating th·∫•p nhi·ªÅu h∆°n c√°c mi·ªÅn kh√°c.*


3. **Ph√¢n t√≠ch c·∫£m x√∫c (Sentiment):**
*
* D·ª±a v√†o c·ªôt `Sentiment` m√† code t·∫°o ra: Bao nhi√™u % l√† khen (T√≠ch c·ª±c), bao nhi√™u % l√† ch√™ (Ti√™u c·ª±c).


4. **Word Cloud (N√¢ng cao - G·ª£i √Ω th√™m):**
* B·∫°n c√≥ th·ªÉ l·∫•y c·ªôt `comment` c·ªßa nh·ªØng d√≤ng c√≥ Rating < 5.0 ƒë·ªÉ xem m·ªçi ng∆∞·ªùi hay ch√™ v·ªÅ c√°i g√¨ (Vd: "th√°i ƒë·ªô", "ƒë·∫Øt", "b·∫©n").
* L·∫•y c·ªôt `comment` c·ªßa Rating > 8.0 ƒë·ªÉ xem h·ªç khen g√¨ (Vd: "ngon", "nhi·ªát t√¨nh", "view ƒë·∫πp").



C√°ch l√†m n√†y s·∫Ω gi√∫p ƒë·ªì √°n c·ªßa b·∫°n c·ª±c k·ª≥ thuy·∫øt ph·ª•c v√¨ c√≥ s·ªë li·ªáu th·ªëng k√™ r√µ r√†ng ch·ª© kh√¥ng ch·ªâ l√† "em crawl ƒë∆∞·ª£c ƒë·ªëng text".
