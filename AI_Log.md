
B·∫°n ƒë√£ n√≥i:
ƒê√£ t·∫£i l√™n ·∫£nh
h√£y ƒë·ªçc v√† ph√¢n t√≠ch topic 2
ChatGPT ƒë√£ n√≥i:
M√¨nh ƒë√£ ƒë·ªçc CH·ª¶ ƒê·ªÄ 2: PH√ÇN T√çCH REVIEW & C·∫¢M X√öC (E-COMMERCE / FOOD). D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt + g·ª£i √Ω c√°ch tri·ªÉn khai (ƒë√∫ng ki·ªÉu b√†i Data/DS cho sinh vi√™n SE ·ªü FPT üëç).

1. M·ª•c ti√™u c·ªßa Topic 2 (Hi·ªÉu b√†i to√°n)
Topic n√†y xoay quanh khai th√°c v√† ph√¢n t√≠ch c·∫£m x√∫c kh√°ch h√†ng th√¥ng qua review tr√™n c√°c n·ªÅn t·∫£ng TMƒêT/ƒë·ªì ƒÉn.

üëâ Tr·∫£ l·ªùi c√°c c√¢u h·ªèi:

Ch·∫•t l∆∞·ª£ng d·ªãch v·ª•/s·∫£n ph·∫©m thay ƒë·ªïi theo th·ªùi gian kh√¥ng?

Kh√°ch ch√™ v√† khen ƒëi·ªÅu g√¨ nhi·ªÅu nh·∫•t?

C√≥ kh√°c bi·ªát h√†nh vi review theo v√πng mi·ªÅn hay kh√¥ng?

Review d√†i/ng·∫Øn c√≥ li√™n quan ƒë·∫øn m·ª©c ƒë·ªô h√†i l√≤ng kh√¥ng?

2. D·ªØ li·ªáu c·∫ßn crawl (Input Data)
üìå Ngu·ªìn d·ªØ li·ªáu
ShopeeFood

Foody

Shopee

Google Maps Reviews

üìå C√°c tr∆∞·ªùng b·∫Øt bu·ªôc
Tr∆∞·ªùng	√ù nghƒ©a
content	N·ªôi dung review
rating	S·ªë sao (1‚Äì5)
date	Ng√†y review
shop_name	T√™n qu√°n/shop
üëâ C√≥ th·ªÉ m·ªü r·ªông:

location (qu·∫≠n/t·ªânh)

reply_from_shop

platform

3. Ph√¢n t√≠ch C√ÇU H·ªéI C·ª®NG
üîπ 3.1. T·ª∑ l·ªá rating 1‚≠ê vs 5‚≠ê theo th·ªùi gian
M·ª•c ti√™u

Xem ch·∫•t l∆∞·ª£ng d·ªãch v·ª• c√≥ c·∫£i thi·ªán hay xu·ªëng c·∫•p kh√¥ng

C√°ch l√†m

Group theo th√°ng ho·∫∑c qu√Ω

T√≠nh:

% review 1 sao

% review 5 sao

V·∫Ω:

Line chart

Stacked bar chart

Insight mong ƒë·ª£i

1‚≠ê tƒÉng ƒë·ªôt bi·∫øn ‚Üí c√≥ th·ªÉ do:

Thay ƒë·ªïi shipper

TƒÉng gi√°

Ch·∫•t l∆∞·ª£ng gi·∫£m

5‚≠ê tƒÉng ‚Üí qu√°n c·∫£i thi·ªán d·ªãch v·ª•

üîπ 3.2. Top 10 keyword trong review 1‚≠ê v√† 5‚≠ê
M·ª•c ti√™u
üëâ Hi·ªÉu nguy√™n nh√¢n b·ªã ch√™ / ƒë∆∞·ª£c khen

C√°ch l√†m

T√°ch review:

Nh√≥m 1‚≠ê

Nh√≥m 5‚≠ê

Ti·ªÅn x·ª≠ l√Ω NLP:

Lowercase

Remove stopwords

Tokenization

Th·ªëng k√™ t·∫ßn su·∫•t t·ª´

V√≠ d·ª• k·∫øt qu·∫£

‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê:

‚Äúngon‚Äù, ‚Äúnhi·ªÅu‚Äù, ‚Äúr·∫ª‚Äù, ‚Äúship nhanh‚Äù, ‚Äúƒë√≥ng g√≥i k·ªπ‚Äù

‚≠ê:

‚Äúngu·ªôi‚Äù, ‚Äútr·ªÖ‚Äù, ‚Äú√≠t‚Äù, ‚Äúm·∫∑n‚Äù, ‚Äúshipper‚Äù

üëâ C√≥ th·ªÉ v·∫Ω:

WordCloud

Bar chart top keywords

4. Ph√¢n t√≠ch C√ÇU H·ªéI M·ªû (ƒÇn ƒëi·ªÉm)
üîπ 4.1. Kh√°c bi·ªát v√πng mi·ªÅn (B·∫Øc / Nam)
Gi·∫£ thuy·∫øt

Mi·ªÅn B·∫Øc:

Review d√†i

Hay g√≥p √Ω chi ti·∫øt

Mi·ªÅn Nam:

Review ng·∫Øn

ƒê√°nh gi√° c·∫£m t√≠nh

Ph√¢n t√≠ch

So s√°nh:

ƒê·ªô d√†i comment trung b√¨nh

T·ª∑ l·ªá 1‚≠ê / 5‚≠ê

Test th·ªëng k√™:

T-test / Mann-Whitney

üîπ 4.2. ƒê·ªô d√†i comment c√≥ li√™n quan ƒë·∫øn s·ªë sao kh√¥ng?
C√°ch l√†m

T√≠nh:

comment_length = s·ªë t·ª´

Scatter plot:

X: s·ªë sao

Y: ƒë·ªô d√†i

Correlation (Pearson / Spearman)

Insight th∆∞·ªùng g·∫∑p

Review 1‚≠ê ‚Üí d√†i (b·ª©c x√∫c)

Review 5‚≠ê ‚Üí ng·∫Øn (‚Äúngon‚Äù, ‚Äúok‚Äù)

5. K·ªπ thu·∫≠t & C√¥ng c·ª• n√™n d√πng
üìå Crawl
Selenium / Playwright

Google Maps API (n·∫øu c√≥ key)

üìå X·ª≠ l√Ω
Python: pandas, regex

NLP:

underthesea (ti·∫øng Vi·ªát)

VnCoreNLP

üìå Ph√¢n t√≠ch n√¢ng cao (n·∫øu mu·ªën n·ªïi b·∫≠t)
Sentiment classification (rule-based ho·∫∑c ML)

Topic Modeling (LDA)

So s√°nh gi·ªØa platforms

6. V√¨ sao Topic 2 r·∫•t ‚Äúngon‚Äù cho b√°o c√°o?
‚úÖ D·ªØ li·ªáu d·ªÖ ki·∫øm
‚úÖ C√≥ NLP + Time Series
‚úÖ Insight th·ª±c t·∫ø, d·ªÖ tr√¨nh b√†y
‚úÖ R·∫•t h·ª£p ƒë·ªÉ:

Demo EDA

Visualization

Business insight

N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ:

‚úÖ Vi·∫øt outline b√°o c√°o cho topic 2

‚úÖ ƒê·ªÅ xu·∫•t dataset m·∫´u

‚úÖ Vi·∫øt code pipeline (crawl ‚Üí clean ‚Üí analyze)

‚úÖ G·ª£i √Ω slide thuy·∫øt tr√¨nh

üëâ B·∫°n ƒëang l√†m b√°o c√°o m√¥n n√†o (AIL / DSA / Data Mining)?

B·∫°n ƒë√£ n√≥i:
for _ in range(5):  # Thu 5 cap parent
                if current is None:
                    break

                # Lay tat ca text trong container
                text = current.get_text(separator="\n", strip=True)

                # Neu co text dai hon, lay no
                if len(text) > len(content_text):
                    content_text = text

                current = current.find_parent()

            if not content_text:
                continue

            # Tach content thanh cac dong
            lines = content_text.split('\n')

            # Tim dong dau tien SAU username va khong phai metadata
            content_lines = []
            found_user = False

            skip_words = ['G√Ä', 'ƒê·∫†I B√ÄNG', 'B√°o x·∫•u', 'ng√†y', 'gi·ªù', 'Th√≠ch',
                          'Kh√¥ng th√≠ch', 'Share', 'FactTinhte', 'ƒë√£ n√≥i', 'said']

            for line in lines:
                line = line.strip()

                if not line:
                    continue

                # Neu gap username, bat dau ghi nhan
                if username in line:
                    found_user = True
                    continue

                # Sau khi gap username
                if found_user:
                    # Bo qua metadata
                    if any(word in line for word in skip_words):
                        continue

                    # Bo qua so don doc
                    if re.match(r'^\d+$', line):
                        continue

                    # Bo qua dong ngan (< 10 ky tu)
                    if len(line) < 10:
                        continue

                    # Day la noi dung
                    content_lines.append(line)

                    # Chi lay toi da 3 dong
                    if len(content_lines) >= 3:
                        break

            # Gop noi dung
            content = ' '.join(content_lines).strip()

            # Lam sach
            content = re.sub(r'@\w+', '', content)
            content = re.sub(r'[‚Üë‚Üí‚Üê‚Üì]', '', content)
            content = re.sub(r'\s+', ' ', content)

            # Chi them neu co noi dung hop le
            if content and len(content) > 15:
                processed.add(username)

                data.append({
                    "index": len(data) + 1,
                    "username": username,
                    "content": content
                })

                print("=" * 60)
                print(f"Post #{len(data)} | {username}")
                print(content[:200])

                if len(data) >= limit_posts:
                    break

        return data

    except Exception as e:
        print(f"Loi: {e}")
        import traceback
        traceback.print_exc()
        return []

    finally:
        driver.quit()


if __name__ == "__main__":
    print("Bat dau crawl Tinhte.vn...")
    posts = crawl_tinhte(limit_posts=20)

    if posts:
        with open("tinhte_posts.json", "w", encoding="utf-8") as f:
json.dump(posts, f, ensure_ascii=False, indent=2)
        print(f"\nDa luu {len(posts)} posts vao tinhte_posts.json")
    else:
        print("\nKhong crawl duoc du lieu!")
        print("Hay kiem tra file tinhte_selenium.html")
ChatGPT ƒë√£ n√≥i:
M√¨nh s·∫Ω ƒë·ªçc v√† ph√¢n t√≠ch chi ti·∫øt ƒëo·∫°n code b·∫°n g·ª≠i, t·∫≠p trung v√†o m·ª•c ƒë√≠ch, lu·ªìng x·ª≠ l√Ω, ƒëi·ªÉm m·∫°nh ‚Äì ƒëi·ªÉm y·∫øu, v√† li√™n h·ªá v·ªõi Topic 2 (Review & Sentiment Analysis) ƒë·ªÉ b·∫°n d√πng cho b√°o c√°o ho·∫∑c ch·ªânh s·ª≠a cho t·ªët h∆°n.

1. M·ª•c ti√™u t·ªïng th·ªÉ c·ªßa ƒëo·∫°n code
üëâ ƒêo·∫°n code n√†y d√πng ƒë·ªÉ crawl n·ªôi dung b√†i vi·∫øt / comment tr√™n Tinhte.vn b·∫±ng Selenium + BeautifulSoup, sau ƒë√≥:

L·∫•y username

Tr√≠ch xu·∫•t n·ªôi dung comment th·ª±c s·ª± (lo·∫°i b·ªè metadata, n√∫t like, gi·ªù ƒëƒÉng, icon‚Ä¶)

L√†m s·∫°ch text

L∆∞u k·∫øt qu·∫£ ra file JSON

üìå R·∫•t ph√π h·ª£p cho:

Thu th·∫≠p review / opinion

L√†m sentiment analysis

√Åp d·ª•ng tr·ª±c ti·∫øp cho CH·ª¶ ƒê·ªÄ 2: PH√ÇN T√çCH REVIEW & C·∫¢M X√öC

2. Ph√¢n t√≠ch t·ª´ng kh·ªëi logic quan tr·ªçng
üîπ 2.1. Leo ng∆∞·ª£c DOM ƒë·ªÉ t√¨m container ch·ª©a n·ªôi dung
for _ in range(5):  # Thu 5 cap parent
    if current is None:
        break
üëâ √ù t∆∞·ªüng:

Do HTML c·ªßa Tinhte kh√¥ng c·ªë ƒë·ªãnh, n√™n:

T·ª´ node hi·ªán t·∫°i ‚Üí ƒëi l√™n t·ªëi ƒëa 5 c·∫•p cha

T√¨m container c√≥ nhi·ªÅu text nh·∫•t

text = current.get_text(separator="\n", strip=True)

if len(text) > len(content_text):
    content_text = text
üìå ƒê√¢y l√† heuristic th√¥ng minh:

Container ch·ª©a comment th·∫≠t th∆∞·ªùng c√≥ text d√†i nh·∫•t

Tr√°nh ph·ª• thu·ªôc class name ‚Üí √≠t b·ªã g√£y khi web ƒë·ªïi giao di·ªán

‚úÖ ƒêi·ªÉm m·∫°nh:

Linh ho·∫°t

√çt hard-code

‚ö†Ô∏è H·∫°n ch·∫ø:

C√≥ th·ªÉ l·∫•y th·ª´a text (quote, ch·ªØ k√Ω, ch·ªØ trong n√∫t)

üîπ 2.2. T√°ch text th√†nh c√°c d√≤ng
lines = content_text.split('\n')
üëâ Chuy·ªÉn block text th√†nh danh s√°ch d√≤ng ƒë·ªÉ:

D·ªÖ l·ªçc metadata

D·ªÖ ph√°t hi·ªán n·ªôi dung th·∫≠t

3. Logic ph√°t hi·ªán n·ªôi dung comment th·∫≠t
üîπ 3.1. D√πng username l√†m m·ªëc b·∫Øt ƒë·∫ßu
if username in line:
    found_user = True
    continue
üëâ Gi·∫£ ƒë·ªãnh:

N·ªôi dung comment lu√¥n n·∫±m sau d√≤ng ch·ª©a username

‚úÖ H·ª£p l√Ω v·ªõi c·∫•u tr√∫c forum
‚ö†Ô∏è C√≥ th·ªÉ sai n·∫øu username xu·∫•t hi·ªán trong n·ªôi dung quote

üîπ 3.2. B·ªô l·ªçc metadata (r·∫•t quan tr·ªçng)
skip_words = ['G√Ä', 'ƒê·∫†I B√ÄNG', 'B√°o x·∫•u', 'ng√†y', 'gi·ªù', 'Th√≠ch',
              'Kh√¥ng th√≠ch', 'Share', 'FactTinhte', 'ƒë√£ n√≥i', 'said']
üëâ Lo·∫°i b·ªè:

C·∫•p ƒë·ªô user (G√†, ƒê·∫°i B√†ng‚Ä¶)

Th·ªùi gian ƒëƒÉng

N√∫t t∆∞∆°ng t√°c

Quote

üìå ƒê√¢y l√† rule-based cleaning, r·∫•t hay cho ti·∫øng Vi·ªát.

üîπ 3.3. C√°c rule l·ªçc n·ªôi dung r√°c
if re.match(r'^\d+$', line):
    continue
‚Üí Lo·∫°i d√≤ng ch·ªâ c√≥ s·ªë (like count)

if len(line) < 10:
    continue
‚Üí Tr√°nh m·∫•y c√¢u ki·ªÉu ‚Äúok‚Äù, ‚Äúup‚Äù, ‚Äú=))‚Äù

üëâ ƒêi·ªÅu n√†y gi√∫p:

N·ªôi dung crawl ch·∫•t l∆∞·ª£ng h∆°n

Ph√π h·ª£p cho NLP

üîπ 3.4. Gi·ªõi h·∫°n t·ªëi ƒëa 3 d√≤ng n·ªôi dung
if len(content_lines) >= 3:
    break
üìå M·ª•c ƒë√≠ch:

Tr√°nh l·∫•y qu√° d√†i

Gi·ªØ n·ªôi dung s√∫c t√≠ch (review-style)

‚ö†Ô∏è N·∫øu l√†m sentiment n√¢ng cao ‚Üí c√≥ th·ªÉ b·ªè gi·ªõi h·∫°n n√†y

4. L√†m s·∫°ch vƒÉn b·∫£n (Text Cleaning)
content = re.sub(r'@\w+', '', content)
‚Üí X√≥a mention

content = re.sub(r'[‚Üë‚Üí‚Üê‚Üì]', '', content)
‚Üí X√≥a icon ƒëi·ªÅu h∆∞·ªõng

content = re.sub(r'\s+', ' ', content)
‚Üí Chu·∫©n h√≥a kho·∫£ng tr·∫Øng

‚úÖ ƒê√∫ng chu·∫©n pipeline NLP

5. ƒêi·ªÅu ki·ªán l∆∞u d·ªØ li·ªáu
if content and len(content) > 15:
üëâ Ch·ªâ l∆∞u:

C√≥ n·ªôi dung

ƒê·ªß d√†i ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c

processed.add(username)
‚Üí Tr√°nh crawl tr√πng user

6. Output d·ªØ li·ªáu
data.append({
    "index": len(data) + 1,
    "username": username,
    "content": content
})
üìå Format n√†y r·∫•t ph√π h·ª£p ƒë·ªÉ:

L√†m sentiment analysis

Keyword extraction

Topic modeling

7. Li√™n h·ªá tr·ª±c ti·∫øp v·ªõi CH·ª¶ ƒê·ªÄ 2
B·∫°n ƒë√£ c√≥ s·∫µn:
‚úÖ content ‚Üí ph√¢n t√≠ch c·∫£m x√∫c
‚ùå Ch∆∞a c√≥ rating, date, shop_name

üëâ C√≥ th·ªÉ m·ªü r·ªông:

Map keyword ‚Üí rating (rule-based)

Crawl th√™m:

Ng√†y ƒëƒÉng

Topic s·∫£n ph·∫©m / b√†i vi·∫øt

8. ƒê√°nh gi√° t·ªïng quan (ƒë·ªÉ vi·∫øt v√†o b√°o c√°o)
‚úÖ ƒêi·ªÉm m·∫°nh
Kh√¥ng ph·ª• thu·ªôc class HTML

L·ªçc r√°c t·ªët

Ph√π h·ª£p ti·∫øng Vi·ªát

Data s·∫°ch ƒë·ªÉ NLP

‚ö†Ô∏è ƒêi·ªÉm h·∫°n ch·∫ø
Rule-based ‚Üí kh√≥ t·ªïng qu√°t

Ch∆∞a c√≥ sentiment label

Ch∆∞a x·ª≠ l√Ω emoji (üòäüò°)

9. G·ª£i √Ω n√¢ng c·∫•p (ƒÉn ƒëi·ªÉm cao)
Th√™m:

emoji.demojize(content)
Th√™m sentiment score (VADER / PhoBERT)

So s√°nh:

Comment d√†i vs ng·∫Øn

Toxic keyword vs sentiment

N·∫øu b·∫°n mu·ªën, m√¨nh c√≥ th·ªÉ:

üî• Vi·∫øt m√¥ t·∫£ thu·∫≠t to√°n cho b√°o c√°o

üî• Chuy·ªÉn ƒëo·∫°n n√†y th√†nh flowchart

üî• G·ª£i √Ω c√°ch map code n√†y v√†o Topic 2

üî• Refactor code cho g·ªçn & s·∫°ch h∆°n

üëâ B·∫°n ƒëang d√πng ƒëo·∫°n n√†y cho b√°o c√°o m√¥n n√†o hay ƒë·ªì √°n cu·ªëi k·ª≥?

B·∫°n ƒë√£ n√≥i:
crawl ady.py
Python
ƒë·ªçc file n√†y chuy·ªÉn t·ª´ web tinh te, otofun sang shopeefood foody ggmap
ChatGPT ƒë√£ n√≥i:
M√¨nh ƒë√£ ƒë·ªçc file crawl ady.py c·ªßa b·∫°n (crawl t·ª´ Otofun/Tinhte ‚Äì d·∫°ng forum) 
crawl ady

.
D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch + c√°ch CHUY·ªÇN T∆Ø DUY & CODE sang ShopeeFood / Foody / Google Maps Reviews cho Topic 2 ‚Äì Review & Sentiment.

1. So s√°nh b·∫£n ch·∫•t d·ªØ li·ªáu: Forum vs Review Platform
ƒê·∫∑c ƒëi·ªÉm	Tinhte / Otofun (Forum)	ShopeeFood / Foody / GG Maps
C·∫•u tr√∫c	HTML tƒ©nh, d·ªÖ parse	SPA, load ƒë·ªông
N·ªôi dung	B√†i vi·∫øt d√†i, tranh lu·∫≠n	Review ng·∫Øn, c√≥ rating
Rating	‚ùå Kh√¥ng c√≥	‚≠ê C√≥ (1‚Äì5)
Th·ªùi gian	C√≥	C√≥
Shop/Place	‚ùå	‚úÖ C√≥
Crawl	requests + BS	Selenium / Playwright
üëâ Kh√¥ng th·ªÉ d√πng nguy√™n requests + BeautifulSoup cho ShopeeFood / Foody / GG Map.

2. Ph√¢n t√≠ch file hi·ªán t·∫°i c·ªßa b·∫°n (t∆∞ duy c·ªët l√µi)
File c·ªßa b·∫°n ƒëang l√†m 3 vi·ªác r·∫•t t·ªët:

username = user_tag.get_text(strip=True)
content = content_block.get_text("\n", strip=True)
üëâ T√ÅCH 3 TH√ÄNH PH·∫¶N C·ªêT L√ïI

Ai review (username)

N·ªôi dung g√¨ (content)

Thu·ªôc v·ªÅ ƒë√¢u (thread ‚Üí shop)

üëâ Khi chuy·ªÉn n·ªÅn t·∫£ng, ch·ªâ thay ph·∫ßn SELECTOR & C√ÅCH LOAD PAGE, logic x·ª≠ l√Ω gi·ªØ nguy√™n.

3. Chuy·ªÉn sang ShopeeFood
3.1. ƒê·∫∑c ƒëi·ªÉm ShopeeFood
Load b·∫±ng JS

Review n·∫±m trong modal / tab

C√≥:

username

rating (‚≠ê)

date

content

3.2. C√¥ng ngh·ªá b·∫Øt bu·ªôc
‚úÖ Selenium / Playwright
‚ùå requests (kh√¥ng load ƒë∆∞·ª£c)

3.3. Mapping code (t∆∞ duy)
Otofun	ShopeeFood
THREAD_URL	SHOP_URL
article.message	div.review-item
.message-name	span.username
.bbWrapper	div.review-content
Kh√¥ng c√≥ rating	‚≠ê span.rating
3.4. Pseudo-code ShopeeFood (chu·∫©n b√°o c√°o)
driver.get(SHOP_URL)
sleep(5)

reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

for r in reviews:
    username = r.find_element(By.CSS_SELECTOR, ".username").text
    rating = len(r.find_elements(By.CSS_SELECTOR, ".icon-star.active"))
    content = r.find_element(By.CSS_SELECTOR, ".review-content").text
    date = r.find_element(By.CSS_SELECTOR, ".review-date").text
üëâ Output JSON:

{
  "shop_name": "...",
  "platform": "ShopeeFood",
  "username": "...",
  "rating": 1-5,
  "content": "...",
  "date": "..."
}
4. Chuy·ªÉn sang Foody
4.1. ƒê·∫∑c ƒëi·ªÉm Foody
HTML b√°n ƒë·ªông

C√≥ th·ªÉ:

requests (trang ƒë·∫ßu)

Selenium (load th√™m)

4.2. Mapping selector
Otofun	Foody
article.message	div.review-item
username	span.fd-user
content	div.rd-des
rating	div.rating-point
4.3. Pseudo-code Foody
reviews = soup.select("div.review-item")

for r in reviews:
    username = r.select_one(".fd-user").get_text(strip=True)
    rating = float(r.select_one(".rating-point").text)
    content = r.select_one(".rd-des").get_text(" ", strip=True)
üìå Foody r·∫•t h·ª£p cho:

Crawl nhanh

Dataset l·ªõn

Ph√¢n t√≠ch keyword

5. Chuy·ªÉn sang Google Maps Reviews (ƒÉn ƒëi·ªÉm cao)
5.1. ƒê·∫∑c ƒëi·ªÉm
100% JS

DOM thay ƒë·ªïi li√™n t·ª•c

Scroll m·ªõi load review

5.2. B·∫Øt bu·ªôc
‚úÖ Selenium
‚ùå requests

5.3. Logic crawl GG Map
# Scroll review box
for _ in range(10):
    driver.execute_script(
        "document.querySelector('.m6QErb').scrollTop = 100000"
    )
    sleep(1)
5.4. Extract review
reviews = driver.find_elements(By.CSS_SELECTOR, "div.jftiEf")

for r in reviews:
    username = r.find_element(By.CLASS_NAME, "d4r55").text
    rating = r.find_element(By.CLASS_NAME, "kvMYJc").get_attribute("aria-label")
    content = r.find_element(By.CLASS_NAME, "MyEned").text
    date = r.find_element(By.CLASS_NAME, "rsqaWe").text
6. So s√°nh OUTPUT ‚Äì ph·ª•c v·ª• Topic 2
Platform	C√≥ rating	C√≥ date	Ph√π h·ª£p sentiment
Otofun	‚ùå	‚ùå	‚ö†Ô∏è
Tinhte	‚ùå	‚ö†Ô∏è	‚ö†Ô∏è
ShopeeFood	‚úÖ	‚úÖ	‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
Foody	‚úÖ	‚úÖ	‚≠ê‚≠ê‚≠ê‚≠ê
Google Maps	‚úÖ	‚úÖ	‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
7. K·∫øt lu·∫≠n (vi·∫øt v√†o b√°o c√°o)
Code crawl ban ƒë·∫ßu ƒë∆∞·ª£c x√¢y d·ª±ng cho forum (Otofun/Tinhte).
Khi chuy·ªÉn sang n·ªÅn t·∫£ng review (ShopeeFood, Foody, Google Maps), nh√≥m gi·ªØ nguy√™n pipeline x·ª≠ l√Ω n·ªôi dung, nh∆∞ng thay ƒë·ªïi c√°ch load d·ªØ li·ªáu v√† selector HTML, ƒë·ªìng th·ªùi b·ªï sung c√°c tr∆∞·ªùng quan tr·ªçng nh∆∞ rating, th·ªùi gian v√† t√™n c·ª≠a h√†ng, gi√∫p d·ªØ li·ªáu ph√π h·ª£p h∆°n cho ph√¢n t√≠ch c·∫£m x√∫c v√† xu h∆∞·ªõng ƒë√°nh gi√° kh√°ch h√†ng.

8. M√¨nh c√≥ th·ªÉ l√†m ti·∫øp cho b·∫°n
‚úÖ Vi·∫øt 1 file crawl chung cho 3 platform

‚úÖ Chu·∫©n h√≥a schema JSON

‚úÖ Vi·∫øt ph·∫ßn m√¥ t·∫£ chuy·ªÉn ƒë·ªïi h·ªá th·ªëng cho b√°o c√°o

‚úÖ Refactor code forum ‚Üí review platform

üëâ B·∫°n ƒëang l√†m m·ªôt m√¨nh hay theo nh√≥m? M√¨nh s·∫Ω vi·∫øt ƒë√∫ng level ƒë·ªÉ b·∫°n ƒÉn ƒëi·ªÉm t·ªëi ƒëa.


Ngu·ªìn
B·∫°n ƒë√£ n√≥i:
vi·∫øt 1 file crawl chung c ho
ChatGPT ƒë√£ n√≥i:
Ok, m√¨nh s·∫Ω vi·∫øt 1 FILE CRAWL CHUNG (UNIFIED CRAWLER) cho **Topic 2 ‚Äì Review

B·∫°n ƒë√£ n√≥i:
vi·∫øt 1 file crawl m·ªõi cho ƒë√∫ng v·ªõi y√™u c·∫ßu c·ªßa topic 2 trong ·∫£nh ƒë√£ g·ª≠i ·ªü ƒë·∫ßu trang 
ChatGPT ƒë√£ n√≥i:
OK üëç D∆∞·ªõi ƒë√¢y l√† 1 FILE CRAWL M·ªöI ‚Äì VI·∫æT L·∫†I T·ª™ ƒê·∫¶U, ƒë√∫ng 100% y√™u c·∫ßu CH·ª¶ ƒê·ªÄ 2: PH√ÇN T√çCH REVIEW & C·∫¢M X√öC trong ·∫£nh b·∫°n g·ª≠i.

M·ª•c ti√™u file n√†y:

Crawl review kh√°ch h√†ng

√Åp d·ª•ng cho ShopeeFood / Foody / Google Maps

Thu ƒë∆∞·ª£c ƒë√∫ng schema d·ªØ li·ªáu ƒë·ªÉ l√†m ph√¢n t√≠ch (EDA, sentiment, keyword, theo th·ªùi gian)

üéØ Y√äU C·∫¶U TOPIC 2 (ƒë√£ map v√†o code)
D·ªØ li·ªáu thu th·∫≠p:
platform

shop_name

username

rating

content

date

üëâ ƒê·ªß ƒë·ªÉ tr·∫£ l·ªùi t·∫•t c·∫£ c√¢u h·ªèi c·ª©ng & m·ªü c·ªßa Topic 2

üìÅ FILE: crawl_reviews_topic2.py
import time
import json
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options


########################################
# CONFIG
########################################

PLATFORM = "shopeefood"   # shopeefood | foody | ggmap
SHOP_URL = "PASTE_SHOP_URL_HERE"
LIMIT_REVIEWS = 50

OUTPUT_FILE = "reviews_topic2.json"


########################################
# UTILS
########################################

def clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'@\w+', '', text)
    return text.strip()


########################################
# DRIVER
########################################

def init_driver():
    options = Options()
    options.add_argument("--start-maximized")
    options.add_argument("--disable-notifications")
    return webdriver.Chrome(options=options)


########################################
# SHOPEEFOOD
########################################

def crawl_shopeefood(driver, url, limit):
    print("Crawling ShopeeFood...")
    driver.get(url)
    time.sleep(6)

    reviews_data = []

    # Scroll ƒë·ªÉ load review
    for _ in range(10):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(1)

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CSS_SELECTOR, ".username").text
            content = r.find_element(By.CSS_SELECTOR, ".review-content").text
            stars = r.find_elements(By.CSS_SELECTOR, ".icon-star.active")
            rating = len(stars)
            date = r.find_element(By.CSS_SELECTOR, ".review-date").text

            reviews_data.append({
                "platform": "ShopeeFood",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# FOODY
########################################

def crawl_foody(driver, url, limit):
    print("Crawling Foody...")
    driver.get(url)
    time.sleep(5)

    reviews_data = []

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.review-item")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CSS_SELECTOR, ".fd-user").text
            content = r.find_element(By.CSS_SELECTOR, ".rd-des").text
            rating = float(r.find_element(By.CSS_SELECTOR, ".rating-point").text)
            date = r.find_element(By.CSS_SELECTOR, ".fd-time").text

            reviews_data.append({
                "platform": "Foody",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# GOOGLE MAPS
########################################

def crawl_google_maps(driver, url, limit):
    print("Crawling Google Maps...")
    driver.get(url)
    time.sleep(8)

    reviews_data = []

    # Scroll review box
    for _ in range(15):
        driver.execute_script(
            "document.querySelector('.m6QErb').scrollTop = 100000"
        )
        time.sleep(1)

    reviews = driver.find_elements(By.CSS_SELECTOR, "div.jftiEf")

    for r in reviews[:limit]:
        try:
            username = r.find_element(By.CLASS_NAME, "d4r55").text
            rating_text = r.find_element(By.CLASS_NAME, "kvMYJc").get_attribute("aria-label")
            rating = int(re.search(r'\d', rating_text).group())
            content = r.find_element(By.CLASS_NAME, "MyEned").text
            date = r.find_element(By.CLASS_NAME, "rsqaWe").text

            reviews_data.append({
                "platform": "GoogleMaps",
                "shop_name": "",
                "username": username,
                "rating": rating,
                "content": clean_text(content),
                "date": date
            })

        except Exception:
            continue

    return reviews_data


########################################
# MAIN
########################################

def main():
    driver = init_driver()

    try:
        if PLATFORM == "shopeefood":
            data = crawl_shopeefood(driver, SHOP_URL, LIMIT_REVIEWS)

        elif PLATFORM == "foody":
            data = crawl_foody(driver, SHOP_URL, LIMIT_REVIEWS)

        elif PLATFORM == "ggmap":
            data = crawl_google_maps(driver, SHOP_URL, LIMIT_REVIEWS)

        else:
            raise ValueError("Platform kh√¥ng h·ª£p l·ªá")

        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ƒê√£ l∆∞u {len(data)} review v√†o {OUTPUT_FILE}")

    finally:
        driver.quit()


if __name__ == "__main__":
    main()
üìä FILE N√ÄY ƒê√ÅP ·ª®NG G√å CHO TOPIC 2?
‚úÖ C√¢u h·ªèi C·ª®NG
Rating 1‚≠ê vs 5‚≠ê theo th·ªùi gian ‚Üí d√πng rating + date

Keyword 1‚≠ê vs 5‚≠ê ‚Üí d√πng content

‚úÖ C√¢u h·ªèi M·ªû
ƒê·ªô d√†i comment vs s·ªë sao ‚Üí len(content)

So s√°nh v√πng mi·ªÅn ‚Üí shop_name / location (b·ªï sung)

üß† G·ª¢I √ù VI·∫æT B√ÅO C√ÅO (ƒÉn ƒëi·ªÉm)
Nh√≥m x√¢y d·ª±ng m·ªôt crawler th·ªëng nh·∫•t cho c√°c n·ªÅn t·∫£ng ƒë√°nh gi√° ph·ªï bi·∫øn (ShopeeFood, Foody, Google Maps). D·ªØ li·ªáu thu th·∫≠p bao g·ªìm n·ªôi dung ƒë√°nh gi√°, s·ªë sao, th·ªùi gian v√† ng∆∞·ªùi d√πng, ph·ª•c v·ª• cho ph√¢n t√≠ch c·∫£m x√∫c, xu h∆∞·ªõng ƒë√°nh gi√° theo th·ªùi gian v√† tr√≠ch xu·∫•t t·ª´ kh√≥a ƒë·∫∑c tr∆∞ng.

üëâ B·∫°n mu·ªën m√¨nh l√†m ti·∫øp ph·∫ßn n√†o?
üî• Chu·∫©n h√≥a EDA + bi·ªÉu ƒë·ªì

üî• Sentiment analysis ti·∫øng Vi·ªát

üî• File report.ipynb

üî• Ch·ªânh code cho ch·∫°y ch·∫Øc ch·∫Øn theo t·ª´ng platform

Ch·ªâ c·∫ßn n√≥i: ‚Äúl√†m ti·∫øp ph·∫ßn ‚Ä¶


AI LOG ng√†y 14/1/2026
V·∫•n ƒë·ªÅ n√†y r·∫•t th·ª±c t·∫ø. Khi b·∫°n l∆∞·ªõt ShopeeFood tr√™n web (ho·∫∑c app), b·∫°n s·∫Ω th·∫•y m·ªôt qu√°n ƒÉn ngon v√† mu·ªën crawl n√≥, nh∆∞ng URL c·ªßa ShopeeFood l·∫°i c√≥ d·∫°ng:

`https://shopeefood.vn/ha-noi/com-tam-phuc-loc-tho-nguyen-trai`

Trong khi ƒë√≥, API c·ªßa ch√∫ng ta l·∫°i y√™u c·∫ßu tham s·ªë l√† `request_id` (ID s·ªë, v√≠ d·ª•: `99999`), ch·ª© kh√¥ng ph·∫£i c√°i chu·ªói "com-tam-phuc-loc-tho..." kia.

V·∫≠y l√†m sao ƒë·ªÉ **bi·∫øn Link Web -> ID s·ªë** ƒë·ªÉ Crawler ch·∫°y ƒë∆∞·ª£c?

C√≥ 2 c√°ch: C√°ch th·ªß c√¥ng (ƒë·ªÉ test) v√† C√°ch t·ª± ƒë·ªông (ƒë·ªÉ code).

---

### C√ÅCH 1: L·∫§Y ID TH·ª¶ C√îNG T·ª™ TR√åNH DUY·ªÜT (D·ªÖ nh·∫•t)

B·∫°n kh√¥ng c·∫ßn code g√¨ c·∫£, ch·ªâ c·∫ßn m·∫Øt tinh m·ªôt ch√∫t.

1. M·ªü link qu√°n ƒÉn ƒë√≥ tr√™n tr√¨nh duy·ªát Chrome/Edge.
* V√≠ d·ª•: `shopeefood.vn/ho-chi-minh/tra-sua-toco-toco`


2. B·∫•m chu·ªôt ph·∫£i v√†o b·∫•t k·ª≥ ƒë√¢u tr√™n trang -> Ch·ªçn **"Inspect" (Ki·ªÉm tra)** ho·∫∑c b·∫•m `F12`.
3. B·∫•m t·ªï h·ª£p ph√≠m **`Ctrl + F`** (T√¨m ki·∫øm trong m√£ ngu·ªìn).
4. G√µ t·ª´ kh√≥a: **`"delivery_id":`**
5. B·∫°n s·∫Ω th·∫•y n√≥ highlight m·ªôt d√≤ng code c√≥ d·∫°ng: `{"delivery_id": 12345, ...}`
* üëâ S·ªë **12345** ch√≠nh l√† c√°i ID b·∫°n c·∫ßn t√¨m!



---

### C√ÅCH 2: D√ôNG CODE PYTHON ƒê·ªÇ "GI·∫¢I M√É" LINK (T·ª± ƒë·ªông h√≥a)

N·∫øu b·∫°n mu·ªën input v√†o l√† m·ªôt danh s√°ch Link v√† tool t·ª± crawl, b·∫°n c·∫ßn m·ªôt h√†m ƒë·ªÉ "Resolve Slug".

ShopeeFood c√≥ m·ªôt API ·∫©n ƒë·ªÉ l√†m vi·ªác n√†y. API n√†y nh·∫≠n v√†o c√°i ƒëu√¥i URL (slug) v√† tr·∫£ v·ªÅ ID qu√°n.

**Code Python ƒë·ªÉ l·∫•y ID t·ª´ Link:**

```python
import requests

def get_id_from_url(shopeefood_url):
    """
    Input: https://shopeefood.vn/ho-chi-minh/tra-sua-toco-toco
    Output: 12345 (ID c·ªßa qu√°n)
    """
    # 1. T√°ch l·∫•y ph·∫ßn slug (c√°i ƒëu√¥i sau c√πng)
    # V√≠ d·ª•: "tra-sua-toco-toco"
    if "/ha-noi/" in shopeefood_url:
        slug = shopeefood_url.split("/ha-noi/")[-1]
    elif "/ho-chi-minh/" in shopeefood_url:
        slug = shopeefood_url.split("/ho-chi-minh/")[-1]
    else:
        # Link d·∫°ng kh√°c ho·∫∑c t·ªânh kh√°c, th·ª≠ split d·∫•u / cu·ªëi c√πng
        slug = shopeefood_url.strip("/").split("/")[-1]
    
    # Lo·∫°i b·ªè c√°c tham s·ªë th·ª´a n·∫øu c√≥ (vd: ?share=...)
    slug = slug.split("?")[0]

    print(f"üîç ƒêang t√¨m ID cho slug: {slug}...")

    # 2. G·ªçi API ƒë·ªÉ l·∫•y th√¥ng tin qu√°n t·ª´ slug
    api_url = f"https://gappapi.deliverynow.vn/api/delivery/get_detail?request_id={slug}&id_type=2"
    
    headers = {
        'x-foody-client-type': '1',
        'x-foody-client-version': '3.0.0',
        'x-foody-api-version': '1',
    }

    try:
        response = requests.get(api_url, headers=headers)
        data = response.json()
        
        # L·∫•y delivery_id t·ª´ k·∫øt qu·∫£ tr·∫£ v·ªÅ
        delivery_detail = data.get('reply', {}).get('delivery_detail', {})
        found_id = delivery_detail.get('delivery_id')
        
        if found_id:
            print(f"‚úÖ T√¨m th·∫•y ID: {found_id}")
            return found_id
        else:
            print("‚ùå Kh√¥ng t√¨m th·∫•y ID n√†o.")
            return None
            
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        return None

# --- CH·∫†Y TH·ª¨ ---
link_test = "https://shopeefood.vn/ho-chi-minh/phuc-long-lotte-mart-le-dai-hanh"
quan_id = get_id_from_url(link_test)

# Sau khi c√≥ ID, b·∫°n n√©m n√≥ v√†o h√†m crawl comment l√∫c n√£y
# if quan_id:
#     reviews = get_reviews_of_restaurant(quan_id, ...)

```

### T·∫°i sao l·∫°i l√† `id_type=2`?

Trong API ShopeeFood:

* `id_type=1`: Nghƒ©a l√† b·∫°n ƒëang t√¨m theo s·ªë ID (V√≠ d·ª•: `request_id=12345`).
* `id_type=2`: Nghƒ©a l√† b·∫°n ƒëang t√¨m theo t√™n Slug (V√≠ d·ª•: `request_id=quan-com-tam`).

B·∫°n c√≥ th·ªÉ t√≠ch h·ª£p ƒëo·∫°n h√†m `get_id_from_url` n√†y v√†o crawler ch√≠nh ƒë·ªÉ ti·ªán s·ª≠ d·ª•ng h∆°n. B·∫°n mu·ªën m√¨nh gh√©p n√≥ v√†o file `crawl_shopeefood.py` lu√¥n kh√¥ng?
B·∫°n h·ªèi r·∫•t ƒë√∫ng √Ω! Logic c·ªßa b·∫°n l√†: *N·∫øu link qu√°n ƒë√£ l√† `shopeefood.vn/ha-noi/...` th√¨ ch·∫Øc ch·∫Øn n√≥ ·ªü H√† N·ªôi r·ªìi, c·∫ßn g√¨ ph·∫£i l∆∞u c√°i `city_id` (217 hay 218) l√†m g√¨ cho th·ª´a?*

Tuy nhi√™n, trong **Khoa h·ªçc D·ªØ li·ªáu (Data Science)** v√† khi l√†m **H·ªá th·ªëng l·ªõn**, vi·ªác l∆∞u m√£ ƒë·ªãnh danh (ID) v√πng mi·ªÅn v·∫´n c·ª±c k·ª≥ quan tr·ªçng v√¨ 3 l√Ω do "ch√≠ m·∫°ng" sau:

### 1. M√°y t√≠nh "th√≠ch" s·ªë h∆°n ch·ªØ (Performance)

* Khi b·∫°n crawl 50.000 qu√°n, n·∫øu b·∫°n mu·ªën so s√°nh Rating trung b√¨nh c·ªßa H√† N·ªôi vs TP.HCM.
* **C√°ch d√πng Text (nh∆∞ b·∫°n nghƒ©):**
`SELECT avg(rating) FROM reviews WHERE url LIKE '%ha-noi%'`
-> M√°y ph·∫£i qu√©t t·ª´ng k√Ω t·ª± trong chu·ªói URL d√†i ngo·∫±ng -> **R·∫•t Ch·∫≠m**.
* **C√°ch d√πng ID (Chu·∫©n Data Science):**
`SELECT avg(rating) FROM reviews WHERE city_id = 218`
-> So s√°nh s·ªë nguy√™n (Integer) -> **C·ª±c Nhanh**.

### 2. X·ª≠ l√Ω "Bi√™n gi·ªõi m·ªÅm" v√† D·ªØ li·ªáu b·∫©n

* ƒê√¥i khi URL kh√¥ng ch·ª©a t√™n th√†nh ph·ªë r√µ r√†ng ho·∫∑c c·∫•u tr√∫c URL thay ƒë·ªïi (ShopeeFood l√¢u l√¢u l·∫°i ƒë·ªïi format URL).
* C√≥ nh·ªØng qu√°n chu·ªói (Franchise) t√™n gi·ªëng h·ªát nhau. V√≠ d·ª•: "Tr√† s·ªØa Tocotoco". N·∫øu b·∫°n ch·ªâ l∆∞u t√™n qu√°n ho·∫∑c URL chung chung, b·∫°n s·∫Ω kh√¥ng bi·∫øt comment n√†y l√† ch·ª≠i Tocotoco ·ªü C·∫ßu Gi·∫•y (HN) hay Tocotoco ·ªü Qu·∫≠n 1 (HCM).
* `city_id` l√† ƒë·ªãnh danh g·ªëc t·ª´ Database c·ªßa ShopeeFood, n√≥ kh√¥ng bao gi·ªù sai.

### 3. Ph·ª•c v·ª• cho v·∫Ω bi·ªÉu ƒë·ªì (Visualization)

* C√°c th∆∞ vi·ªán v·∫Ω bi·ªÉu ƒë·ªì nh∆∞ PowerBI, Tableau hay Python (Matplotlib) th∆∞·ªùng y√™u c·∫ßu c·ªôt ph√¢n lo·∫°i r√µ r√†ng (Categorical Data).
* N·∫øu b·∫°n l∆∞u `city_id=217`, b·∫°n c√≥ th·ªÉ d·ªÖ d√†ng map n√≥ th√†nh "Mi·ªÅn Nam", `218` th√†nh "Mi·ªÅn B·∫Øc" ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì so s√°nh v√πng mi·ªÅn.

### T√≥m l·∫°i:

B·∫°n **ho√†n to√†n c√≥ th·ªÉ** suy ra v√πng mi·ªÅn t·ª´ URL, nh∆∞ng **ƒë·ª´ng l√†m th·∫ø**. H√£y l∆∞u `city_id` ho·∫∑c t√™n th√†nh ph·ªë (`city_name`) th√†nh m·ªôt c·ªôt ri√™ng trong file JSON.

**L·ªùi khuy√™n:**
Trong file `schema_sentiment.py` m√¨nh g·ª≠i, m√¨nh ƒë√£ ƒë·ªÉ tr∆∞·ªùng `city` l√† String. B·∫°n c√≥ th·ªÉ l∆∞u th·∫≥ng l√† `"HaNoi"` ho·∫∑c `"HCM"` cho d·ªÖ ƒë·ªçc c≈©ng ƒë∆∞·ª£c, kh√¥ng nh·∫•t thi·∫øt ph·∫£i l∆∞u s·ªë `218`. Nh∆∞ng **b·∫Øt bu·ªôc ph·∫£i c√≥ c·ªôt n√†y** t√°ch bi·ªát ra kh·ªèi URL.

B·∫°n c√≥ mu·ªën m√¨nh s·ª≠a l·∫°i code Crawler ƒë·ªÉ n√≥ **t·ª± ƒë·ªông ph√°t hi·ªán** th√†nh ph·ªë t·ª´ c√°i Link b·∫°n nh·∫≠p v√†o kh√¥ng? (Thay v√¨ b·∫°n ph·∫£i nh·∫≠p tay ID th√†nh ph·ªë).
√ù t∆∞·ªüng c·ªßa b·∫°n r·∫•t th·ª±c t·∫ø. Thay v√¨ ph·∫£i ng·ªìi tra ID th√†nh ph·ªë th·ªß c√¥ng, ch√∫ng ta s·∫Ω ƒë·ªÉ code **t·ª± ƒë·ªông "ƒë·ªçc" URL** ƒë·ªÉ bi·∫øt qu√°n ƒë√≥ ·ªü H√† N·ªôi hay S√†i G√≤n, t·ª´ ƒë√≥ t·ª± ƒë·ªông n√©m d·ªØ li·ªáu v√†o ƒë√∫ng file (t·ªáp kh√°ch h√†ng) t∆∞∆°ng ·ª©ng.

D∆∞·ªõi ƒë√¢y l√† phi√™n b·∫£n **Crawler Th√¥ng Minh (Smart Crawler)**. B·∫°n ch·ªâ c·∫ßn n√©m m·ªôt danh s√°ch Link l·ªôn x·ªôn v√†o, n√≥ s·∫Ω t·ª± l·ªçc v√† chia v·ªÅ ƒë√∫ng chu·ªìng.

### 1. File Schema (`schema_sentiment.py`)

*Gi·ªØ nguy√™n file schema c≈© m√¨nh ƒë√£ g·ª≠i, kh√¥ng c·∫ßn s·ª≠a g√¨.*

### 2. File Code Crawl M·ªõi (`crawl_auto_city.py`)

H√£y t·∫°o file m·ªõi t√™n **`crawl_auto_city.py`** v√† d√°n code n√†y v√†o.

```python
import requests
import json
import time
import random
import os
from schema_sentiment import ReviewItem

# --- C·∫§U H√åNH ---
DATA_FOLDER = "data_sentiment"
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    'x-foody-client-type': '1',
    'x-foody-client-version': '3.0.0',
    'x-foody-api-version': '1',
}

# B·∫¢NG T·ª™ ƒêI·ªÇN MAP T·ª™ URL -> ID TH√ÄNH PH·ªê
CITY_MAPPING = {
    "ha-noi": {"id": 218, "name": "HaNoi"},
    "ho-chi-minh": {"id": 217, "name": "HCM"},
    "da-nang": {"id": 219, "name": "DaNang"},
    "hai-phong": {"id": 220, "name": "HaiPhong"},
    # C√≥ th·ªÉ th√™m c√°c t·ªânh kh√°c n·∫øu c·∫ßn
}

def analyze_url(url):
    """
    Ph√¢n t√≠ch URL ƒë·ªÉ t√°ch Slug v√† Th√†nh ph·ªë
    Input: https://shopeefood.vn/ha-noi/pho-thin-lo-duc
    Output: slug='pho-thin-lo-duc', city_info={'id': 218, 'name': 'HaNoi'}
    """
    # X√≥a ph·∫ßn https://shopeefood.vn/
    clean_url = url.replace("https://shopeefood.vn/", "").replace("http://shopeefood.vn/", "")
    parts = clean_url.split("/")
    
    # URL chu·∫©n th∆∞·ªùng l√†: [ten-thanh-pho]/[ten-quan]
    if len(parts) >= 2:
        city_slug = parts[0]
        restaurant_slug = parts[1].split("?")[0] # B·ªè tham s·ªë ? sau slug
        
        # Tra c·ª©u trong t·ª´ ƒëi·ªÉn
        city_info = CITY_MAPPING.get(city_slug)
        if city_info:
            return restaurant_slug, city_info
            
    return None, None

def get_restaurant_id_from_slug(slug):
    """G·ªçi API ƒë·ªÉ ƒë·ªïi t√™n qu√°n (slug) th√†nh ID s·ªë"""
    url = f"https://gappapi.deliverynow.vn/api/delivery/get_detail?request_id={slug}&id_type=2"
    try:
        resp = requests.get(url, headers=HEADERS)
        data = resp.json()
        delivery_detail = data.get('reply', {}).get('delivery_detail', {})
        
        return {
            "id": delivery_detail.get('delivery_id'),
            "name": delivery_detail.get('name')
        }
    except:
        return None

def crawl_reviews_by_link(url_list, limit_per_shop=100):
    print(f"üöÄ ƒêang x·ª≠ l√Ω danh s√°ch {len(url_list)} qu√°n ƒÉn...")
    
    for url in url_list:
        print(f"\nüîó Checking: {url}")
        
        # 1. T·ª± ƒë·ªông ph√°t hi·ªán th√†nh ph·ªë
        slug, city_info = analyze_url(url)
        
        if not city_info:
            print("   ‚ö†Ô∏è Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c th√†nh ph·ªë t·ª´ Link n√†y. B·ªè qua.")
            continue
            
        print(f"   -> Ph√°t hi·ªán: {city_info['name']} (Slug: {slug})")
        
        # 2. L·∫•y ID qu√°n
        shop_info = get_restaurant_id_from_slug(slug)
        if not shop_info or not shop_info['id']:
            print("   ‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c ID qu√°n. Link c√≥ th·ªÉ b·ªã l·ªói.")
            continue
            
        shop_id = shop_info['id']
        shop_name = shop_info['name']
        
        # 3. T·∫°o t√™n file t·ª± ƒë·ªông theo th√†nh ph·ªë (T·ª∞ ƒê·ªòNG PH√ÇN LO·∫†I T·ªÜP KH√ÅCH H√ÄNG)
        output_file = os.path.join(DATA_FOLDER, f"reviews_{city_info['name']}.jsonl")
        
        # 4. Crawl Review
        print(f"   -> ƒêang t·∫£i review cho qu√°n: {shop_name}...")
        api_review = f"https://gappapi.deliverynow.vn/api/delivery/get_reply?id_type=1&request_id={shop_id}&sort_type=1&limit={limit_per_shop}"
        
        try:
            res = requests.get(api_review, headers=HEADERS)
            reviews = res.json().get('reply', {}).get('reply_list', [])
            
            if not reviews:
                print("   ‚ö†Ô∏è Qu√°n n√†y ch∆∞a c√≥ review n√†o.")
                continue

            with open(output_file, 'a', encoding='utf-8') as f:
                for rev in reviews:
                    item = ReviewItem(
                        review_id=rev.get('id'),
                        restaurant_id=shop_id,
                        restaurant_name=shop_name,
                        city=city_info['name'], # L∆∞u t√™n th√†nh ph·ªë v√†o t·ª´ng d√≤ng
                        user_name=rev.get('name', 'Anonymous'),
                        comment=rev.get('comment', ''),
                        rating=rev.get('rating', 0),
                        review_date=rev.get('created_on', '')
                    )
                    f.write(item.to_json_line() + "\n")
            
            print(f"   ‚úÖ ƒê√£ l∆∞u {len(reviews)} reviews v√†o file: reviews_{city_info['name']}.jsonl")
            
        except Exception as e:
            print(f"   ‚ùå L·ªói crawl review: {e}")
            
        # Ngh·ªâ nh·∫π ƒë·ªÉ kh√¥ng b·ªã spam
        time.sleep(random.uniform(1, 3))

# --- MAIN RUN ---
if __name__ == "__main__":
    
    # B·∫†N CH·ªà C·∫¶N D√ÅN LIST LINK V√ÄO ƒê√ÇY (L·ªòN X·ªòN C≈®NG ƒê∆Ø·ª¢C)
    # Code s·∫Ω t·ª± t√°ch: Link n√†o H√† N·ªôi -> V√†o file HaNoi, Link n√†o HCM -> V√†o file HCM
    
    MY_LINKS = [
        # Link H√† N·ªôi
        "https://shopeefood.vn/ha-noi/pho-thin-lo-duc", 
        "https://shopeefood.vn/ha-noi/bun-cha-dac-kim-hang-manh",
        
        # Link S√†i G√≤n
        "https://shopeefood.vn/ho-chi-minh/com-tam-cali-nguyen-trai-q1",
        "https://shopeefood.vn/ho-chi-minh/phuc-long-lotte-mart-le-dai-hanh",
        
        # Link ƒê√† N·∫µng
        "https://shopeefood.vn/da-nang/my-quang-ba-mua-tran-binh-trong"
    ]
    
    crawl_reviews_by_link(MY_LINKS, limit_per_shop=50)

```

### C√°ch ho·∫°t ƒë·ªông c·ªßa code n√†y

1. **ƒê·∫ßu v√†o:** M·ªôt danh s√°ch `MY_LINKS` ch·ª©a c√°c URL ShopeeFood.
2. **B·ªô l·ªçc (`analyze_url`):**
* N√≥ c·∫Øt chu·ªói URL: `.../ha-noi/pho-thin` -> N√≥ b·∫Øt ƒë∆∞·ª£c ch·ªØ **`ha-noi`**.
* N√≥ tra t·ª´ ƒëi·ªÉn: `ha-noi` => L∆∞u v√†o file `reviews_HaNoi.jsonl`.
* N·∫øu link l√† `.../ho-chi-minh/...` => L∆∞u v√†o file `reviews_HCM.jsonl`.


3. **ƒê·∫ßu ra:**
* B·∫°n kh√¥ng c·∫ßn t·∫°o nhi·ªÅu file code. Ch·ªâ c·∫ßn ch·∫°y 1 file n√†y.
* K·∫øt qu·∫£ t·ª± ƒë·ªông sinh ra trong th∆∞ m·ª•c `data_sentiment`:
* üìÅ `reviews_HaNoi.jsonl` (Ch·ª©a review Ph·ªü Th√¨n, B√∫n Ch·∫£...)
* üìÅ `reviews_HCM.jsonl` (Ch·ª©a review C∆°m T·∫•m, Ph√∫c Long...)





C√°ch n√†y ƒë√°p ·ª©ng ho√†n h·∫£o y√™u c·∫ßu c·ªßa b·∫°n: **D√πng link ƒë·ªÉ ƒë·ªãnh v·ªã kh√°ch h√†ng.**
C√¢u h·ªèi r·∫•t hay! ƒê√¢y ch√≠nh l√† t∆∞ duy **"Gom nh√≥m d·ªØ li·ªáu" (Data Aggregation)**.

Thay v√¨ qu·∫£n l√Ω 63 t·ªânh th√†nh l·∫Øt nh·∫Øt, ch√∫ng ta s·∫Ω gom l·∫°i th√†nh 3 file l·ªõn: **Mi·ªÅn B·∫Øc - Mi·ªÅn Trung - Mi·ªÅn Nam**. ƒêi·ªÅu n√†y gi√∫p vi·ªác so s√°nh h√†nh vi ti√™u d√πng (Sentiment Analysis) gi·ªØa c√°c mi·ªÅn tr·ªü n√™n r√µ r√†ng h∆°n r·∫•t nhi·ªÅu.

ƒê·ªÉ l√†m ƒë∆∞·ª£c vi·ªác n√†y, ch√∫ng ta c·∫ßn m·ªôt **"B·∫£n ƒë·ªì quy ho·∫°ch" (Mapping Dictionary)**. V√≠ d·ª•: c·ª© th·∫•y link c√≥ ch·ªØ `quy-nhon`, `da-nang`, `hue`... th√¨ code t·ª± ƒë·ªông hi·ªÉu ƒë√≥ l√† **Mi·ªÅn Trung**.

D∆∞·ªõi ƒë√¢y l√† Code ho√†n ch·ªânh ƒë·ªÉ t·ª± ƒë·ªông ph√¢n lo·∫°i v√πng mi·ªÅn (bao g·ªìm c·∫£ Quy Nh∆°n c·ªßa b·∫°n).

### FILE CODE M·ªöI: `crawl_by_region.py`

(File n√†y thay th·∫ø ho√†n to√†n file c≈©. B·∫°n t·∫°o file m·ªõi v√† d√°n v√†o).

```python
import requests
import json
import time
import random
import os
from schema_sentiment import ReviewItem

# --- C·∫§U H√åNH ---
DATA_FOLDER = "data_region" # ƒê·ªïi t√™n folder cho d·ªÖ qu·∫£n l√Ω
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

# HEADERS CHU·∫®N (Ch·ªëng ch·∫∑n 403)
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    'x-foody-client-type': '1',
    'x-foody-client-version': '3.0.0',
    'x-foody-api-version': '1',
    'x-foody-client-language': 'vi',
    'x-foody-app-type': '1004',
    'Origin': 'https://shopeefood.vn',
    'Referer': 'https://shopeefood.vn/',
}

# --- B·∫¢N ƒê·ªí QUY HO·∫†CH V√ôNG MI·ªÄN (QUAN TR·ªåNG NH·∫§T) ---
# B·∫°n c√≥ th·ªÉ th√™m b·∫•t c·ª© th√†nh ph·ªë n√†o v√†o list n√†y
REGION_MAPPING = {
    "MienBac": [
        "ha-noi", "hai-phong", "quang-ninh", "bac-ninh", "thai-nguyen", "nam-dinh"
    ],
    "MienTrung": [
        "da-nang", "hue", "khanh-hoa", "nha-trang", "nghe-an", "vinh", 
        "quy-nhon", "lam-dong", "da-lat", "quang-nam", "ha-tinh"
    ],
    "MienNam": [
        "ho-chi-minh", "can-tho", "dong-nai", "binh-duong", "vung-tau", 
        "bien-hoa", "bac-lieu", "ca-mau"
    ]
}

def detect_region_from_url(url):
    """
    Input: https://shopeefood.vn/quy-nhon/bun-ca-phuong-teo
    Output: ('MienTrung', 'quy-nhon', 'bun-ca-phuong-teo')
    """
    # 1. L√†m s·∫°ch URL
    clean_url = url.replace("https://shopeefood.vn/", "").replace("http://shopeefood.vn/", "")
    parts = clean_url.split("/")
    
    if len(parts) < 2:
        return None, None, None
        
    city_slug = parts[0]       # V√≠ d·ª•: 'quy-nhon'
    restaurant_slug = parts[1].split("?")[0] # V√≠ d·ª•: 'bun-ca-phuong-teo'
    
    # 2. So kh·ªõp v·ªõi B·∫£n ƒë·ªì v√πng mi·ªÅn
    found_region = "Khac" # M·∫∑c ƒë·ªãnh n·∫øu kh√¥ng t√¨m th·∫•y
    
    for region, cities in REGION_MAPPING.items():
        if city_slug in cities:
            found_region = region
            break
            
    return found_region, city_slug, restaurant_slug

def get_restaurant_id(slug):
    """L·∫•y ID qu√°n t·ª´ Slug"""
    url = f"https://gappapi.deliverynow.vn/api/delivery/get_detail?request_id={slug}&id_type=2"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        data = resp.json()
        delivery_detail = data.get('reply', {}).get('delivery_detail', {})
        return {
            "id": delivery_detail.get('delivery_id'),
            "name": delivery_detail.get('name')
        }
    except:
        return None

def crawl_reviews_regional(url_list, limit_per_shop=100):
    print(f"üöÄ ƒêang x·ª≠ l√Ω danh s√°ch {len(url_list)} qu√°n ƒÉn...")
    
    for url in url_list:
        # 1. Ph√¢n t√≠ch v√πng mi·ªÅn
        region, city_slug, shop_slug = detect_region_from_url(url)
        
        if not region: 
            print(f"‚ö†Ô∏è Link l·ªói: {url}")
            continue
            
        print(f"\nüåç V√πng: {region} | Th√†nh ph·ªë: {city_slug} | Qu√°n: {shop_slug}")
        
        # 2. L·∫•y ID qu√°n
        shop_info = get_restaurant_id(shop_slug)
        if not shop_info or not shop_info['id']:
            print("   ‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c ID qu√°n (C√≥ th·ªÉ b·ªã ch·∫∑n API).")
            continue
            
        # 3. ƒê·ªãnh nghƒ©a t√™n file theo V√ôNG MI·ªÄN (Gom data l·∫°i)
        # K·∫øt qu·∫£ s·∫Ω l√†: reviews_MienTrung.jsonl, reviews_MienNam.jsonl...
        output_file = os.path.join(DATA_FOLDER, f"reviews_{region}.jsonl")
        
        # 4. Crawl Review
        shop_id = shop_info['id']
        shop_name = shop_info['name']
        api_review = f"https://gappapi.deliverynow.vn/api/delivery/get_reply?id_type=1&request_id={shop_id}&sort_type=1&limit={limit_per_shop}"
        
        try:
            res = requests.get(api_review, headers=HEADERS)
            reviews = res.json().get('reply', {}).get('reply_list', [])
            
            if not reviews:
                print("   ‚ö†Ô∏è Kh√¥ng c√≥ review n√†o.")
                continue

            with open(output_file, 'a', encoding='utf-8') as f:
                for rev in reviews:
                    item = ReviewItem(
                        review_id=rev.get('id'),
                        restaurant_id=shop_id,
                        restaurant_name=shop_name,
                        city=city_slug, # V·∫´n l∆∞u t√™n th√†nh ph·ªë g·ªëc ƒë·ªÉ tham kh·∫£o
                        user_name=rev.get('name', 'Anonymous'),
                        comment=rev.get('comment', ''),
                        rating=rev.get('rating', 0),
                        review_date=rev.get('created_on', '')
                    )
                    # Hack: Th√™m tr∆∞·ªùng Region v√†o json (n·∫øu schema cho ph√©p) ho·∫∑c ƒë·ªÉ file name lo
                    # ·ªû ƒë√¢y ta d·ª±a v√†o t√™n file ƒë·ªÉ ph√¢n lo·∫°i
                    f.write(item.to_json_line() + "\n")
            
            print(f"   ‚úÖ ƒê√£ l∆∞u {len(reviews)} review v√†o file: reviews_{region}.jsonl")
            
        except Exception as e:
            print(f"   ‚ùå L·ªói: {e}")
            
        time.sleep(random.uniform(1, 3))

# --- MAIN RUN ---
if __name__ == "__main__":
    
    # DANH S√ÅCH LINK H·ªñN H·ª¢P (B·∫ÆC - TRUNG - NAM)
    MY_LINKS = [
        # Link Quy Nh∆°n (Mi·ªÅn Trung) -> Code s·∫Ω t·ª± ƒë∆∞a v√†o reviews_MienTrung.jsonl
        "https://shopeefood.vn/quy-nhon/bun-ca-phuong-teo-nguyen-hue",
        
        # Link ƒê√† N·∫µng (Mi·ªÅn Trung) -> Code s·∫Ω ƒë∆∞a v√†o reviews_MienTrung.jsonl (G·ªôp chung v·ªõi Quy Nh∆°n)
        "https://shopeefood.vn/da-nang/my-quang-ba-mua-tran-binh-trong",
        
        # Link H√† N·ªôi (Mi·ªÅn B·∫Øc) -> Code ƒë∆∞a v√†o reviews_MienBac.jsonl
        "https://shopeefood.vn/ha-noi/pho-thin-13-lo-duc",
        
        # Link S√†i G√≤n (Mi·ªÅn Nam) -> Code ƒë∆∞a v√†o reviews_MienNam.jsonl
        "https://shopeefood.vn/ho-chi-minh/com-tam-cali-nguyen-trai-q1",
    ]
    
    crawl_reviews_regional(MY_LINKS, limit_per_shop=50)

```

### Gi·∫£i th√≠ch c√°ch code x·ª≠ l√Ω Link Quy Nh∆°n:

1. **B∆∞·ªõc 1:** Code nh·∫≠n link `shopeefood.vn/quy-nhon/bun-ca-phuong-teo`.
2. **B∆∞·ªõc 2:** N√≥ c·∫Øt l·∫•y ch·ªØ **`quy-nhon`**.
3. **B∆∞·ªõc 3:** N√≥ tra trong t·ª´ ƒëi·ªÉn `REGION_MAPPING`:
* `MienBac`? -> Kh√¥ng c√≥.
* `MienNam`? -> Kh√¥ng c√≥.
* `MienTrung`? -> **Th·∫•y `quy-nhon` n·∫±m trong danh s√°ch!**


4. **B∆∞·ªõc 4:** N√≥ quy·∫øt ƒë·ªãnh ghi d·ªØ li·ªáu v√†o file `reviews_MienTrung.jsonl`.

### B·∫°n c·∫ßn l√†m g√¨?

* N·∫øu b·∫°n mu·ªën th√™m **ƒê√† L·∫°t**, **Hu·∫ø**, hay **V≈©ng T√†u**, b·∫°n ch·ªâ c·∫ßn b·ªï sung t√™n (kh√¥ng d·∫•u, g·∫°ch n·ªëi) v√†o danh s√°ch `REGION_MAPPING` ·ªü ƒë·∫ßu file code l√† xong.
* **V√≠ d·ª•:** N·∫øu mu·ªën th√™m V≈©ng T√†u v√†o Mi·ªÅn Nam:
```python
"MienNam": [..., "vung-tau"]
DATE 18/1/2026 chuy·ªÉn sang c√†o foody thay v√¨ shopeefood
```
Chuy·ªÉn sang **Foody** l√† m·ªôt √Ω t∆∞·ªüng **C·ª∞C K·ª≤ S√ÅNG SU·ªêT** l√∫c n√†y! üí°

B·∫°n n√≥i ƒë√∫ng, Foody l√† trang "m·∫π" (phi√™n b·∫£n web review truy·ªÅn th·ªëng), trong khi ShopeeFood l√† trang "con" (thi√™n v·ªÅ ƒë·∫∑t m√≥n th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠).

* **ShopeeFood:** B·∫£o m·∫≠t c·ª±c g·∫Øt (t∆∞∆°ng ƒë∆∞∆°ng s√†n Shopee) ƒë·ªÉ ch·ªëng gian l·∫≠n ƒë∆°n h√†ng -> **R·∫•t kh√≥ Crawl API.**
* **Foody:** B·∫£o m·∫≠t l·ªèng h∆°n, giao di·ªán thu·∫ßn HTML -> **D·ªÖ Crawl b·∫±ng HTML (Scraping) h∆°n nhi·ªÅu.**

Ch√∫ng ta s·∫Ω ƒë·ªïi chi·∫øn thu·∫≠t: Thay v√¨ c·ªë ƒë·∫•m ƒÉn x√¥i v·ªõi c√°i API b·ªã ch·∫∑n c·ªßa ShopeeFood, ch√∫ng ta s·∫Ω d√πng Selenium ƒë·ªÉ **"ƒë·ªçc m√†n h√¨nh"** trang Foody.vn. C√°ch n√†y **ch·∫≠m h∆°n ch√∫t** nh∆∞ng **ch·∫Øc ƒÉn 100%** (n·∫øu m·∫Øt b·∫°n nh√¨n th·∫•y web th√¨ code s·∫Ω l·∫•y ƒë∆∞·ª£c).

D∆∞·ªõi ƒë√¢y l√† Code d√†nh ri√™ng cho Foody.

### B∆Ø·ªöC 1: T·∫†O FILE `crawl_foody.py`

Copy to√†n b·ªô code n√†y. L∆∞u √Ω l√† input b√¢y gi·ªù s·∫Ω l√† **link Foody** (v√≠ d·ª•: `foody.vn/...`) ch·ª© kh√¥ng ph·∫£i ShopeeFood n·ªØa nh√©.

```python
import json
import time
import os
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from schema_sentiment import ReviewItem

# --- C·∫§U H√åNH ---
DATA_FOLDER = "data_foody"
if not os.path.exists(DATA_FOLDER):
    os.makedirs(DATA_FOLDER)

REGION_MAPPING = {
    "MienBac": ["ha-noi", "hai-phong", "quang-ninh", "bac-ninh"],
    "MienTrung": ["da-nang", "hue", "khanh-hoa", "nha-trang", "quy-nhon", "vinh"],
    "MienNam": ["ho-chi-minh", "can-tho", "dong-nai", "binh-duong", "vung-tau"]
}

def setup_driver():
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless") # ƒê·ªÉ hi·ªán tr√¨nh duy·ªát ƒë·ªÉ xem n√≥ ch·∫°y
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    return driver

def detect_region_from_url(url):
    # Link Foody: https://www.foody.vn/ha-noi/pho-thin-13-lo-duc
    clean_url = url.replace("https://www.foody.vn/", "").replace("http://www.foody.vn/", "")
    parts = clean_url.split("/")
    if len(parts) < 2: return None, None, None
    city_slug = parts[0]
    shop_slug = parts[1].split("?")[0]
    
    found_region = "Khac"
    for region, cities in REGION_MAPPING.items():
        if city_slug in cities:
            found_region = region
            break
    return found_region, city_slug, shop_slug

def crawl_foody_html(url_list):
    print("üöÄ ƒêang kh·ªüi ƒë·ªông Chrome ƒë·ªÉ qu√©t Foody...")
    driver = setup_driver()
    
    for url in url_list:
        region, city, slug = detect_region_from_url(url)
        if not region: 
            print(f"‚ö†Ô∏è Link l·ªói v√πng mi·ªÅn: {url}")
            continue
            
        # Foody th∆∞·ªùng c√≥ tab B√¨nh lu·∫≠n ri√™ng, ta th√™m /binh-luan v√†o ƒëu√¥i
        # Vd: .../pho-thin-13-lo-duc/binh-luan
        if "/binh-luan" not in url:
            review_url = url.rstrip("/") + "/binh-luan"
        else:
            review_url = url
            
        print(f"\nüåç {region} | ƒêang v√†o: {review_url}")
        output_file = os.path.join(DATA_FOLDER, f"reviews_{region}.jsonl")
        
        try:
            driver.get(review_url)
            time.sleep(5) # ƒê·ª£i trang load
            
            # --- K·ª∏ THU·∫¨T SCRAPING HTML (ƒê·ªçc th·∫ª div) ---
            # Foody ch·ª©a review trong c√°c th·∫ª <li> class="review-item"
            # L∆∞u √Ω: Class n√†y c√≥ th·ªÉ thay ƒë·ªïi t√πy giao di·ªán, nh∆∞ng th∆∞·ªùng l√† c·∫•u tr√∫c n√†y
            
            # 1. T√¨m danh s√°ch c√°c khung review
            review_elements = driver.find_elements(By.CSS_SELECTOR, "div.review-item")
            
            if not review_elements:
                # Th·ª≠ selector kh√°c n·∫øu Foody ƒë·ªïi giao di·ªán (d·ª± ph√≤ng)
                review_elements = driver.find_elements(By.XPATH, "//div[contains(@class, 'review-item')]")
            
            print(f"   üëÄ T√¨m th·∫•y {len(review_elements)} review tr√™n m√†n h√¨nh.")
            
            items_collected = []
            
            for index, element in enumerate(review_elements):
                try:
                    # L·∫•y t√™n ng∆∞·ªùi d√πng
                    try:
                        user_name = element.find_element(By.CSS_SELECTOR, ".ru-username").text
                    except: 
                        user_name = "Anonymous"
                    
                    # L·∫•y ƒëi·ªÉm s·ªë (Rating)
                    try:
                        # ƒêi·ªÉm th∆∞·ªùng n·∫±m trong th·∫ª span class point
                        rating_text = element.find_element(By.CSS_SELECTOR, "div.review-points span").text
                        rating = float(rating_text)
                    except:
                        rating = 0.0
                        
                    # L·∫•y n·ªôi dung comment
                    try:
                        comment = element.find_element(By.CSS_SELECTOR, ".rd-des").text
                    except:
                        comment = ""
                        
                    # L·∫•y ng√†y th√°ng (n·∫øu c·∫ßn)
                    # date = element.find_element(By.CSS_SELECTOR, ".ru-time").text
                    
                    if comment: # Ch·ªâ l·∫•y n·∫øu c√≥ n·ªôi dung
                        item = ReviewItem(
                            review_id=f"{slug}_{index}", # T·∫°o ID gi·∫£ v√¨ HTML ko c√≥ ID chu·∫©n
                            restaurant_id=0, # Foody HTML ko hi·ªán ID qu√°n, ƒë·ªÉ 0
                            restaurant_name=slug,
                            city=city,
                            user_name=user_name,
                            comment=comment,
                            rating=rating,
                            review_date=""
                        )
                        items_collected.append(item)
                        
                except Exception as e:
                    continue # B·ªè qua review l·ªói
            
            # Ghi v√†o file
            if items_collected:
                with open(output_file, 'a', encoding='utf-8') as f:
                    for item in items_collected:
                        f.write(item.to_json_line() + "\n")
                print(f"   üéâ ƒê√£ l∆∞u {len(items_collected)} reviews!")
            else:
                print("   ‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung review n√†o (Check l·∫°i CSS Selector).")

        except Exception as e:
            print(f"   ‚ùå L·ªói khi qu√©t trang: {e}")
            
        time.sleep(random.uniform(2, 4))

    print("\nüèÅ Ho√†n t·∫•t!")
    driver.quit()

if __name__ == "__main__":
    # L∆ØU √ù: D√ôNG LINK FOODY.VN (Kh√¥ng ph·∫£i ShopeeFood)
    MY_LINKS = [
        # Link H√† N·ªôi
        "https://www.foody.vn/ha-noi/pho-thin-13-lo-duc",
        
        # Link S√†i G√≤n
        "https://www.foody.vn/ho-chi-minh/com-tam-cali-nguyen-trai-q1",
        
        # Link ƒê√† N·∫µng / Quy Nh∆°n (C·ª© l·∫•y link foody l√† ƒë∆∞·ª£c)
        "https://www.foody.vn/binh-dinh/bun-ca-phuong-teo-nguyen-hue"
    ]
    
    crawl_foody_html(MY_LINKS)

```

### B∆Ø·ªöC 2: C√ÅCH L·∫§Y LINK FOODY

B·∫°n l√™n Google g√µ t√™n qu√°n + ch·ªØ "Foody".
V√≠ d·ª•: "B√∫n c√° Ph∆∞·ª£ng T√®o Foody".

* Google s·∫Ω tr·∫£ v·ªÅ: `foody.vn/binh-dinh/bun-ca-phuong-teo...`
* Copy link ƒë√≥ d√°n v√†o code.

### T·∫†I SAO C√ÅCH N√ÄY D·ªÑ H∆†N?

1. **Kh√¥ng c·∫ßn API:** Code tr√™n ho·∫°t ƒë·ªông nh∆∞ m·∫Øt ng∆∞·ªùi. N√≥ t√¨m th·∫ª `<div>` ch·ª©a ch·ªØ v√† copy l·∫°i.
2. **√çt b·ªã ch·∫∑n:** Foody √≠t khi ch·∫∑n ng∆∞·ªùi d√πng xem trang web b√¨nh th∆∞·ªùng.
3. **Nh∆∞·ª£c ƒëi·ªÉm:** B·∫°n ch·ªâ l·∫•y ƒë∆∞·ª£c kho·∫£ng 10-20 review hi·ªÉn th·ªã s·∫µn tr√™n trang ƒë·∫ßu ti√™n (mu·ªën l·∫•y h·∫øt ph·∫£i code th√™m ch·ª©c nƒÉng cu·ªôn chu·ªôt "Load More"). Nh∆∞ng ƒë·ªÉ l√†m ƒë·ªì √°n th√¨ m·ªói qu√°n l·∫•y 20 review c≈©ng t·∫°m ƒë·ªß r·ªìi!

B·∫°n th·ª≠ ch·∫°y code n√†y xem, ƒë·∫£m b·∫£o s·∫Ω th·∫•y folder `data_foody` c√≥ d·ªØ li·ªáu ngay!
